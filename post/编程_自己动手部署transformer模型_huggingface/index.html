<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage data-theme=light><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>自己动手部署transformer模型 by huggingface - Weiming Hu
</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=color-scheme content="light dark"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=generator content="Hugo 0.140.2"><link rel=canonical href=https://huweim.github.io/post/%E7%BC%96%E7%A8%8B_%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E9%83%A8%E7%BD%B2transformer%E6%A8%A1%E5%9E%8B_huggingface/><meta name=author content="Weiming Hu"><meta name=description content="0. 前言 这部分内容还是很重要的，预计会设计常见的 pytorch 模型部署方法，理解框架中，每个模块在做什么。另外，这也是工程上必备的技能。
0.1 模型及下载地址 Model Repo Paper ResNet (18: 12M; 50: 26M; 152: 60M) https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py BERT (110 / 330M) https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT https://arxiv.org/abs/1810.04805 GPT-2 (1.5B) https://github.com/openai/gpt-2 OPT (125 / 350M; 1.3 / 2.7 / 6.7 / 13 / 30 / 66 / 175B) https://github.com/facebookresearch/metaseq https://arxiv.org/pdf/2205.01068.pdf BLOOM (560M; 1.1 / 1.7 / 3 / 7.1 / 176B) https://huggingface.co/docs/transformers/model_doc/bloom T5 (60 / 220 / 770M; 3 / 11B) https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints https://jmlr.org/papers/volume21/20-074/20-074.pdf 0.2 模型下载方法 2023-07-08 20:10:12，重新回顾 22 年关于大模型研究的工作。从 chatgpt 爆火之后，大模型应用的框架变得火热，语言模型的社区也变得火热起来。准备在 ant_ext 工作中加入一些新的模型的评估，但是期智连接 huggingface 的网络老是抽风，导致试图从 huggingface 下载模型时出现错误。现在总结一些其他的下载方法。
"><meta name=keywords content="huggingface"><meta property="og:url" content="https://huweim.github.io/post/%E7%BC%96%E7%A8%8B_%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E9%83%A8%E7%BD%B2transformer%E6%A8%A1%E5%9E%8B_huggingface/"><meta property="og:site_name" content="Weiming Hu"><meta property="og:title" content="自己动手部署transformer模型 by huggingface"><meta property="og:description" content="0. 前言 这部分内容还是很重要的，预计会设计常见的 pytorch 模型部署方法，理解框架中，每个模块在做什么。另外，这也是工程上必备的技能。
0.1 模型及下载地址 Model Repo Paper ResNet (18: 12M; 50: 26M; 152: 60M) https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py BERT (110 / 330M) https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT https://arxiv.org/abs/1810.04805 GPT-2 (1.5B) https://github.com/openai/gpt-2 OPT (125 / 350M; 1.3 / 2.7 / 6.7 / 13 / 30 / 66 / 175B) https://github.com/facebookresearch/metaseq https://arxiv.org/pdf/2205.01068.pdf BLOOM (560M; 1.1 / 1.7 / 3 / 7.1 / 176B) https://huggingface.co/docs/transformers/model_doc/bloom T5 (60 / 220 / 770M; 3 / 11B) https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints https://jmlr.org/papers/volume21/20-074/20-074.pdf 0.2 模型下载方法 2023-07-08 20:10:12，重新回顾 22 年关于大模型研究的工作。从 chatgpt 爆火之后，大模型应用的框架变得火热，语言模型的社区也变得火热起来。准备在 ant_ext 工作中加入一些新的模型的评估，但是期智连接 huggingface 的网络老是抽风，导致试图从 huggingface 下载模型时出现错误。现在总结一些其他的下载方法。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-10-23T11:00:17+08:00"><meta property="article:modified_time" content="2022-11-26T16:17:41+08:00"><meta property="article:tag" content="Huggingface"><meta itemprop=name content="自己动手部署transformer模型 by huggingface"><meta itemprop=description content="0. 前言 这部分内容还是很重要的，预计会设计常见的 pytorch 模型部署方法，理解框架中，每个模块在做什么。另外，这也是工程上必备的技能。
0.1 模型及下载地址 Model Repo Paper ResNet (18: 12M; 50: 26M; 152: 60M) https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py BERT (110 / 330M) https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT https://arxiv.org/abs/1810.04805 GPT-2 (1.5B) https://github.com/openai/gpt-2 OPT (125 / 350M; 1.3 / 2.7 / 6.7 / 13 / 30 / 66 / 175B) https://github.com/facebookresearch/metaseq https://arxiv.org/pdf/2205.01068.pdf BLOOM (560M; 1.1 / 1.7 / 3 / 7.1 / 176B) https://huggingface.co/docs/transformers/model_doc/bloom T5 (60 / 220 / 770M; 3 / 11B) https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints https://jmlr.org/papers/volume21/20-074/20-074.pdf 0.2 模型下载方法 2023-07-08 20:10:12，重新回顾 22 年关于大模型研究的工作。从 chatgpt 爆火之后，大模型应用的框架变得火热，语言模型的社区也变得火热起来。准备在 ant_ext 工作中加入一些新的模型的评估，但是期智连接 huggingface 的网络老是抽风，导致试图从 huggingface 下载模型时出现错误。现在总结一些其他的下载方法。"><meta itemprop=datePublished content="2022-10-23T11:00:17+08:00"><meta itemprop=dateModified content="2022-11-26T16:17:41+08:00"><meta itemprop=wordCount content="6767"><meta itemprop=keywords content="Huggingface"><meta name=twitter:card content="summary"><meta name=twitter:title content="自己动手部署transformer模型 by huggingface"><meta name=twitter:description content="0. 前言 这部分内容还是很重要的，预计会设计常见的 pytorch 模型部署方法，理解框架中，每个模块在做什么。另外，这也是工程上必备的技能。
0.1 模型及下载地址 Model Repo Paper ResNet (18: 12M; 50: 26M; 152: 60M) https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py BERT (110 / 330M) https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT https://arxiv.org/abs/1810.04805 GPT-2 (1.5B) https://github.com/openai/gpt-2 OPT (125 / 350M; 1.3 / 2.7 / 6.7 / 13 / 30 / 66 / 175B) https://github.com/facebookresearch/metaseq https://arxiv.org/pdf/2205.01068.pdf BLOOM (560M; 1.1 / 1.7 / 3 / 7.1 / 176B) https://huggingface.co/docs/transformers/model_doc/bloom T5 (60 / 220 / 770M; 3 / 11B) https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints https://jmlr.org/papers/volume21/20-074/20-074.pdf 0.2 模型下载方法 2023-07-08 20:10:12，重新回顾 22 年关于大模型研究的工作。从 chatgpt 爆火之后，大模型应用的框架变得火热，语言模型的社区也变得火热起来。准备在 ant_ext 工作中加入一些新的模型的评估，但是期智连接 huggingface 的网络老是抽风，导致试图从 huggingface 下载模型时出现错误。现在总结一些其他的下载方法。"><link rel=icon href=/favicon.ico><link rel=stylesheet href=/css/style.min.1acfe3c0bf85aa6c451ba764236bb3ab12c22cd38eee8ecc8fac33defcc7156d.css integrity="sha256-Gs/jwL+FqmxFG6dkI2uzqxLCLNOO7o7Mj6wz3vzHFW0=" media=screen crossorigin=anonymous><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--><script>(function(){var e=localStorage.getItem("theme")||"light";document.documentElement.setAttribute("data-theme",e)})()</script></head><body><div id=back-to-top></div><header class=site-header><div class=desktop-header><div class=desktop-header-logo><a href=/ class=logo>Weiming Hu</a></div><nav class=desktop-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/>Home</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/post>All posts</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/archives>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/tags>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/about/>About</a></li><li class=menu-item><a class="theme-toggle menu-item-link" href=javascript:void(0);><svg aria-hidden="true" class="lucide lucide-sun hi-svg-inline theme-icon-light" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><circle cx="12" cy="12" r="4"/><path d="M12 2v2"/><path d="M12 20v2"/><path d="m4.93 4.93 1.41 1.41"/><path d="m17.66 17.66 1.41 1.41"/><path d="M2 12h2"/><path d="M20 12h2"/><path d="m6.34 17.66-1.41 1.41"/><path d="m19.07 4.93-1.41 1.41"/></svg><svg aria-hidden="true" class="lucide lucide-moon hi-svg-inline theme-icon-dark" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M12 3a6 6 0 009 9 9 9 0 11-9-9z"/></svg></a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/index.xml rel="noopener alternate" type=application/rss+xml title=rss target=_blank><svg aria-hidden="true" class="lucide lucide-rss hi-svg-inline icon--rss" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></li></ul></nav></div><div class=mobile-header><div id=mobile-navbar class=mobile-navbar><div id=mobile-navbar-icon class=mobile-navbar-icon><svg aria-hidden="true" class="lucide lucide-menu hi-svg-inline icon--menu" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><line x1="4" x2="20" y1="12" y2="12"/><line x1="4" x2="20" y1="6" y2="6"/><line x1="4" x2="20" y1="18" y2="18"/></svg></div><div class=mobile-navbar-logo><a href=/ class=logo>Weiming Hu</a></div></div><div id=mobile-menu-close-modal class=mobile-menu-close-modal></div><nav id=mobile-menu class=mobile-menu><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/post>All posts</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/archives>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/tags>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/about/>About</a></li><li class=mobile-menu-item><a class="theme-toggle menu-item-link" href=javascript:void(0);><svg aria-hidden="true" class="lucide lucide-sun hi-svg-inline theme-icon-light" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><circle cx="12" cy="12" r="4"/><path d="M12 2v2"/><path d="M12 20v2"/><path d="m4.93 4.93 1.41 1.41"/><path d="m17.66 17.66 1.41 1.41"/><path d="M2 12h2"/><path d="M20 12h2"/><path d="m6.34 17.66-1.41 1.41"/><path d="m19.07 4.93-1.41 1.41"/></svg><svg aria-hidden="true" class="lucide lucide-moon hi-svg-inline theme-icon-dark" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M12 3a6 6 0 009 9 9 9 0 11-9-9z"/></svg></a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/index.xml rel="noopener alternate" type=application/rss+xml title=rss target=_blank><svg aria-hidden="true" class="lucide lucide-rss hi-svg-inline icon--rss" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></li></ul></nav></div></header><main id=main class="main pico container"><div class=content-wrapper><aside class=sidebar></aside><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>自己动手部署transformer模型 by huggingface</h1><div class=post-meta-list><div class="post-meta-item post-meta-author"><svg aria-hidden="true" class="lucide lucide-user-round-pen hi-svg-inline" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M2 21a8 8 0 0110.821-7.487"/><path d="M21.378 16.626a1 1 0 00-3.004-3.004l-4.01 4.012a2 2 0 00-.506.854l-.837 2.87a.5.5.0 00.62.62l2.87-.837a2 2 0 00.854-.506z"/><circle cx="10" cy="8" r="5"/></svg>
Weiming Hu</div><div class="post-meta-item post-meta-time"><svg aria-hidden="true" class="lucide lucide-calendar-days hi-svg-inline" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M8 2v4"/><path d="M16 2v4"/><rect width="18" height="18" x="3" y="4" rx="2"/><path d="M3 10h18"/><path d="M8 14h.01"/><path d="M12 14h.01"/><path d="M16 14h.01"/><path d="M8 18h.01"/><path d="M12 18h.01"/><path d="M16 18h.01"/></svg>
<time datetime=2022-10-23>2022-10-23
</time><span class="post-meta-item post-meta-lastmod">(LastMod:
2022-11-26)</span></div><div class=post-meta__right><span class=post-meta-more>6767 words -
14 min read</span><div class="post-meta-item post-meta-category"><a href=https://huweim.github.io/categories/%E7%BC%96%E7%A8%8B/>编程</a></div></div></div></header><div class=post-content><h1 id=0-前言>0. 前言</h1><p>这部分内容还是很重要的，预计会设计常见的 pytorch 模型部署方法，理解框架中，每个模块在做什么。另外，这也是工程上必备的技能。</p><h2 id=01-模型及下载地址>0.1 模型及下载地址</h2><table><thead><tr><th>Model</th><th>Repo</th><th>Paper</th></tr></thead><tbody><tr><td>ResNet (18: 12M; 50: 26M; 152: 60M)</td><td><a href=https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py</a></td><td></td></tr><tr><td>BERT (110 / 330M)</td><td><a href=https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT>https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT</a></td><td><a href=https://arxiv.org/abs/1810.04805>https://arxiv.org/abs/1810.04805</a></td></tr><tr><td>GPT-2 (1.5B)</td><td><a href=https://github.com/openai/gpt-2>https://github.com/openai/gpt-2</a></td><td></td></tr><tr><td>OPT (125 / 350M; 1.3 / 2.7 / 6.7 / 13 / 30 / 66 / 175B)</td><td><a href=https://github.com/facebookresearch/metaseq>https://github.com/facebookresearch/metaseq</a></td><td><a href=https://arxiv.org/pdf/2205.01068.pdf>https://arxiv.org/pdf/2205.01068.pdf</a></td></tr><tr><td>BLOOM (560M; 1.1 / 1.7 / 3 / 7.1 / 176B)</td><td><a href=https://huggingface.co/docs/transformers/model_doc/bloom>https://huggingface.co/docs/transformers/model_doc/bloom</a></td><td></td></tr><tr><td>T5 (60 / 220 / 770M; 3 / 11B)</td><td><a href=https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints>https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints</a></td><td><a href=https://jmlr.org/papers/volume21/20-074/20-074.pdf>https://jmlr.org/papers/volume21/20-074/20-074.pdf</a></td></tr></tbody></table><h2 id=02-模型下载方法>0.2 模型下载方法</h2><p>2023-07-08 20:10:12，重新回顾 22 年关于大模型研究的工作。从 chatgpt 爆火之后，大模型应用的框架变得火热，语言模型的社区也变得火热起来。准备在 ant_ext 工作中加入一些新的模型的评估，但是期智连接 huggingface 的网络老是抽风，导致试图从 huggingface 下载模型时出现错误。现在总结一些其他的下载方法。</p><p>Ref: <a href=https://zhuanlan.zhihu.com/p/475260268>https://zhuanlan.zhihu.com/p/475260268</a></p><h3 id=021-git-lfs>0.2.1 git lfs</h3><p>这个方法会下载所有框架的模型文件，flax_model.msgpack、tf_model.h5和pytorch_model.bin，可以看到有 3 种框架的模型文件，比较冗余。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>git lfs install
</span></span><span style=display:flex><span>git clone https://huggingface.co/bert-base-chinese</span></span></code></pre></div></div><h3 id=022-hungging-face-hub>0.2.2 hungging face hub</h3><p>这种方法其实仍然是通过访问 huggingface 网站来下载</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>python
</span></span><span style=display:flex><span>&gt;&gt;&gt; from huggingface_hub import snapshot_download
</span></span><span style=display:flex><span>&gt;&gt;&gt; snapshot_download<span style=color:#f92672>(</span>repo_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;facebook/opt-13b&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 通过参数可以指定下载的文件，屏蔽不需要的文件即可</span>
</span></span><span style=display:flex><span>&gt;&gt;&gt; snapshot_download<span style=color:#f92672>(</span>repo_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;facebook/opt-13b&#34;</span>, ignore_patterns<span style=color:#f92672>=[</span><span style=color:#e6db74>&#34;*.h5&#34;</span>, <span style=color:#e6db74>&#34;*.ot&#34;</span>, <span style=color:#e6db74>&#34;*.msgpack&#34;</span><span style=color:#f92672>]</span>, local_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/localdata_ssd/model&#34;</span>, local_dir_use_symlinks<span style=color:#f92672>=</span>False<span style=color:#f92672>)</span></span></span></code></pre></div></div><h1 id=1-hugging-face>1. Hugging Face</h1><p>简介：Hugging face 是一家总部位于纽约的聊天机器人初创服务商，开发的应用在青少年中颇受欢迎，相比于其他公司，Hugging Face更加注重产品带来的情感以及环境因素。但更令它广为人知的是Hugging Face专注于NLP技术，拥有大型的开源社区。尤其是在github上开源的自然语言处理，预训练模型库 Transformers，已被下载超过一百万次，github上超过24000个star。Transformers 提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。</p><p>BERT 模型中，一个 token-wise 就是 768 维的向量。</p><blockquote><p>总的来说，提供了各种易用 transformer 模型，并且有很好的社区和文档。</p></blockquote><h2 id=11-简介>1.1 简介</h2><h3 id=111-用途>1.1.1 用途</h3><p>理解了一下，感觉是提供预训练好的模型，自己可以用来解决一些任务。</p><h3 id=112-重要概念>1.1.2 重要概念</h3><h4 id=1121-pipeline>1.1.2.1 Pipeline</h4><p>pipeline 函数：包含 pro-processing, model, post-processing</p><p>如果只是想使用训练好的模型的功能，这是最简单的方法。pipeline 函数中应该完成了一套 workflow。下列代码下载了大概 300MB 的预训练模型，给出了对我输入的句子的情感分析。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline
</span></span><span style=display:flex><span>classifier <span style=color:#f92672>=</span> pipeline(<span style=color:#e6db74>&#34;sentiment-analysis&#34;</span>)
</span></span><span style=display:flex><span>print(classifier(<span style=color:#e6db74>&#34;You like a pig&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output: [{<span style=color:#e6db74>&#39;label&#39;</span>: <span style=color:#e6db74>&#39;NEGATIVE&#39;</span>, <span style=color:#e6db74>&#39;score&#39;</span>: <span style=color:#ae81ff>0.9961085915565491</span>}]</span></span></code></pre></div></div><h4 id=1122-autoclass>1.1.2.2 AutoClass</h4><p><code>AutoModelForSequenceClassification</code> 以及 <code>AutoTokenizer</code> 用于支持 <code>pipeline()</code>。AutoClass 是一种快捷方式，可以自动从预训练模型的名称或路径中检索模型架构。 您只需要为您的任务选择适当的 AutoClass 及其关联的预处理类。</p><blockquote><p>在自己使用时，选择 GPT2 模型，需要 <code>from transformers import GPT2Model</code>，如果要引入其他模型也是类似的做法。<code>from transformers import AutoModel</code>，应该可以根据模型名称自己去检索下载。</p></blockquote><p>2022-10-25 21:13:01，当我有可能通过参数载入多种模型时，我意识到了 AutoClass 的作用。</p><h4 id=1123-custom-model>1.1.2.3 Custom Model</h4><p>主要途径是获取模型的 config，然后手动修改即可。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoConfig
</span></span><span style=display:flex><span>my_config <span style=color:#f92672>=</span> AutoConfig<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;distilbert-base-uncased&#34;</span>, n_heads<span style=color:#f92672>=</span><span style=color:#ae81ff>12</span>)
</span></span><span style=display:flex><span>my_model <span style=color:#f92672>=</span> AutoModel<span style=color:#f92672>.</span>from_config(my_config)</span></span></code></pre></div></div><h4 id=1124-trainer---a-pytorch-optimized-training-loop>1.1.2.4 Trainer - a PyTorch optimized training loop</h4><p>目前的很多框架中都有一套训练的 loop，hugging face 也提供了这套流程。设置好几个必备的参数和输入即可。</p><ol><li>A PreTrainedModel or a <code>torch.nn.Module</code>；自己定制一个模型应该也是可以的。</li></ol><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForSequenceClassification
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;distilbert-base-uncased&#34;</span>)</span></span></code></pre></div></div><ol start=2><li><code>TrainingArguments</code>：超参数，比如 learning rate, batch size, epoch</li></ol><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TrainingArguments
</span></span><span style=display:flex><span>training_args <span style=color:#f92672>=</span> TrainingArguments(
</span></span><span style=display:flex><span>    output_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;path/to/save/folder/&#34;</span>,
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>2e-5</span>,
</span></span><span style=display:flex><span>    per_device_train_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>    per_device_eval_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>    num_train_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>)</span></span></code></pre></div></div><ol start=3><li>设置预处理过程：比如 tokenizer, feature extractor, processor</li></ol><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;distilbert-base-uncased&#34;</span>)</span></span></code></pre></div></div><ol start=4><li>定义预处理的训练数据集和测试数据集；不过这段代码之前得先把 dataset load 进来吧。</li></ol><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;train&#34;</span>]  <span style=color:#75715e># doctest: +SKIP</span>
</span></span><span style=display:flex><span>eval_dataset <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;eval&#34;</span>]  <span style=color:#75715e># doctest: +SKIP</span></span></span></code></pre></div></div><ol start=5><li><code>DataCollator()</code> to create a batch of examples from your dataset</li></ol><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> DefaultDataCollator
</span></span><span style=display:flex><span>data_collator <span style=color:#f92672>=</span> DefaultDataCollator()</span></span></code></pre></div></div><blockquote><p>2022-10-24 09:43:20，这个 example 比较陌生</p></blockquote><ol start=6><li>Put it all together</li></ol><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> Trainer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> Trainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>=</span>training_args,
</span></span><span style=display:flex><span>    train_dataset<span style=color:#f92672>=</span>dataset[<span style=color:#e6db74>&#34;train&#34;</span>],
</span></span><span style=display:flex><span>    eval_dataset<span style=color:#f92672>=</span>dataset[<span style=color:#e6db74>&#34;test&#34;</span>],
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>tokenizer,
</span></span><span style=display:flex><span>    data_collator<span style=color:#f92672>=</span>data_collator,
</span></span><span style=display:flex><span>)  <span style=color:#75715e># doctest: +SKIP</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># start training</span>
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>train()</span></span></code></pre></div></div><h4 id=1125-预处理>1.1.2.5 预处理</h4><p>对于 NLP 模型，一般就是用 tokenizer 来做。经过 tokenizer 的字典有三个重要的 items</p><ul><li>input_ids, the indices corresponding to each token in the sentence.</li><li>attention_mask, indicates whether a token should be attended to or not.</li><li>token_type_ids, identifies which sequence a token belongs to when there is more than one sequence.</li></ul><p><strong>padding</strong></p><p>每个 sentence 长度不一致，用 padding 来 tensor 对齐。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>batch_sentences <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;But what about second breakfast?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Don&#39;t think he knows about second breakfast, Pip.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;What about elevensies?&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>encoded_input <span style=color:#f92672>=</span> tokenizer(batch_sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(encoded_input)
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;input_ids&#39;</span>: [[<span style=color:#ae81ff>101</span>, <span style=color:#ae81ff>1252</span>, <span style=color:#ae81ff>1184</span>, <span style=color:#ae81ff>1164</span>, <span style=color:#ae81ff>1248</span>, <span style=color:#ae81ff>6462</span>, <span style=color:#ae81ff>136</span>, <span style=color:#ae81ff>102</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>               [<span style=color:#ae81ff>101</span>, <span style=color:#ae81ff>1790</span>, <span style=color:#ae81ff>112</span>, <span style=color:#ae81ff>189</span>, <span style=color:#ae81ff>1341</span>, <span style=color:#ae81ff>1119</span>, <span style=color:#ae81ff>3520</span>, <span style=color:#ae81ff>1164</span>, <span style=color:#ae81ff>1248</span>, <span style=color:#ae81ff>6462</span>, <span style=color:#ae81ff>117</span>, <span style=color:#ae81ff>21902</span>, <span style=color:#ae81ff>1643</span>, <span style=color:#ae81ff>119</span>, <span style=color:#ae81ff>102</span>], 
</span></span><span style=display:flex><span>               [<span style=color:#ae81ff>101</span>, <span style=color:#ae81ff>1327</span>, <span style=color:#ae81ff>1164</span>, <span style=color:#ae81ff>5450</span>, <span style=color:#ae81ff>23434</span>, <span style=color:#ae81ff>136</span>, <span style=color:#ae81ff>102</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]], 
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;token_type_ids&#39;</span>: [[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]], 
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;attention_mask&#39;</span>: [[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>], 
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]]}</span></span></code></pre></div></div><p><strong>Truncation</strong></p><p>也是一种对齐的方式</p><h3 id=113-模型-model>1.1.3 模型 Model</h3><p>支持的模型很多，比较热门的预训练模型基本都有。学术界的很多模型也有，比如 I-BERT，RoBERTa</p><h2 id=12-api>1.2 API</h2><h3 id=121-from_pretrained>1.2.1 from_pretrained</h3><p>功能：<code>from_pretrained</code> 提供了模型类别判断，模型文件列表映射，模型文件下载及缓存，网络下载稳定性容错等功能。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Download from huggingface.co and cache</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> BertModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>save_pretrained(<span style=color:#e6db74>&#39;./local_model_directory/&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load from local</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> BertModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;./local_model_directory/&#39;</span>)</span></span></code></pre></div></div><p>实例：</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span>(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(<span style=color:#e6db74>&#39;./model/&#39;</span><span style=color:#f92672>+</span>args<span style=color:#f92672>.</span>gpt2_model<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;pytorch_model.bin&#39;</span>)):
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>GPT2ForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;./model/&#39;</span><span style=color:#f92672>+</span>args<span style=color:#f92672>.</span>gpt2_model)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> GPT2ForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(args<span style=color:#f92672>.</span>gpt2_model)
</span></span><span style=display:flex><span>    num_labels <span style=color:#f92672>=</span> len(model<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>id2label)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> GPT2ForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(args<span style=color:#f92672>.</span>gpt2_model, num_labels<span style=color:#f92672>=</span>num_labels)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>save_pretrained(<span style=color:#e6db74>&#39;./model/&#39;</span><span style=color:#f92672>+</span>args<span style=color:#f92672>.</span>gpt2_model)</span></span></code></pre></div></div><h2 id=13-fine-tune-a-pretrained-model>1.3 Fine-tune a pretrained model</h2><p>可以用 1.1.2.4 中的 <code>Trainer()</code> 来实现微调和训练，也可以在 pytorch 框架中实现。下面是步骤。</p><h3 id=131-prepare-a-dataset>1.3.1 Prepare a dataset</h3><p>比如，从 <code>Yelp Reviews</code> 中 load 数据，然后进行预处理。下面的代码用到了 <code>map()</code> 函数，写法比较陌生，不过这个过程是通用的。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># load</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;yelp_review_full&#34;</span>)
</span></span><span style=display:flex><span>dataset[<span style=color:#e6db74>&#34;train&#34;</span>][<span style=color:#ae81ff>100</span>]
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;label&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;text&#39;</span>: <span style=color:#e6db74>&#39;My expectations for McDonalds are t rarely high....&#39;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># tokenizer</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-cased&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tokenize_function</span>(examples):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> tokenizer(examples[<span style=color:#e6db74>&#34;text&#34;</span>], padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;max_length&#34;</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenized_datasets <span style=color:#f92672>=</span> dataset<span style=color:#f92672>.</span>map(tokenize_function, batched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)</span></span></code></pre></div></div><h3 id=132-train>1.3.2 Train</h3><p>用 <code>Trainer()</code> 训练在 1.1.2.4 中讲过了，讲一下用 native pytorch 训练，<strong>Train in native PyTorch</strong></p><h4 id=1321-dataloader>1.3.2.1 DataLoader</h4><p>为训练数据集和测试数据集创建一个 <code>DataLoader</code>，以便可以遍历数据的 batch。给出 BERT 框架中的示例代码。变量 <code>batch</code> 中就是一个 batch，经过 tokenizer 之后的数据。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> (DataLoader, RandomSampler, SequentialSampler, TensorDataset)
</span></span><span style=display:flex><span>train_features <span style=color:#f92672>=</span> get_train_features(
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>data_dir,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>gpt2_model,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>max_seq_length,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>do_lower_case,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>local_rank,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>train_batch_size,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>gradient_accumulation_steps,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>num_train_epochs,
</span></span><span style=display:flex><span>    tokenizer,
</span></span><span style=display:flex><span>    processor,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>train_data <span style=color:#f92672>=</span> gen_tensor_dataset(train_features)
</span></span><span style=display:flex><span>        train_dataloader <span style=color:#f92672>=</span> DataLoader(
</span></span><span style=display:flex><span>            train_data,
</span></span><span style=display:flex><span>            sampler<span style=color:#f92672>=</span>train_sampler,
</span></span><span style=display:flex><span>            batch_size<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>train_batch_size,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> e <span style=color:#f92672>in</span> range(args<span style=color:#f92672>.</span>num_train_epochs):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> step, batch <span style=color:#f92672>in</span> enumerate(train_dataloader):
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span></span></span></code></pre></div></div><h4 id=1322-optimizer-and-learning-rate-scheduler>1.3.2.2 Optimizer and learning rate scheduler</h4><p>优化器根据模型来定义，要传入 learning rate 吗，代码示例：</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model, optimizer, scheduler <span style=color:#f92672>=</span> init_optimizer_and_amp(
</span></span><span style=display:flex><span>    model,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>learning_rate,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>loss_scale,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>warmup_proportion,
</span></span><span style=display:flex><span>    num_train_optimization_steps,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>fp16,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_optimizer_and_amp</span>(model, learning_rate, loss_scale, warmup_proportion,
</span></span><span style=display:flex><span>                           num_train_optimization_steps, use_fp16):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    optimizer, scheduler <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>, <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;using fp32&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> num_train_optimization_steps <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        optimizer <span style=color:#f92672>=</span> BertAdam(
</span></span><span style=display:flex><span>            optimizer_grouped_parameters,
</span></span><span style=display:flex><span>            lr<span style=color:#f92672>=</span>learning_rate,
</span></span><span style=display:flex><span>            warmup<span style=color:#f92672>=</span>warmup_proportion,
</span></span><span style=display:flex><span>            t_total<span style=color:#f92672>=</span>num_train_optimization_steps,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model, optimizer, scheduler</span></span></code></pre></div></div><h4 id=1323-training-loop>1.3.2.3 Training Loop</h4><p>这个无需多说。设置好 epoch，每个 epoch 会过完训练数据集中的所有数据。（当然，也有随机抽样的形式）</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> e <span style=color:#f92672>in</span> range(args<span style=color:#f92672>.</span>num_train_epochs):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> step, batch <span style=color:#f92672>in</span> enumerate(train_dataloader):
</span></span><span style=display:flex><span>        batch <span style=color:#f92672>=</span> tuple(t<span style=color:#f92672>.</span>to(device) <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> batch)
</span></span><span style=display:flex><span>        input_ids, input_mask, segment_ids, label_ids <span style=color:#f92672>=</span> batch
</span></span><span style=display:flex><span>        <span style=color:#75715e># outputs = model(**batch)</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> model(input_ids<span style=color:#f92672>=</span>input_ids, token_type_ids<span style=color:#f92672>=</span>segment_ids, attention_mask<span style=color:#f92672>=</span>input_mask, labels<span style=color:#f92672>=</span>label_ids)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> outputs<span style=color:#f92672>.</span>loss
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        lr_scheduler<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        progress_bar<span style=color:#f92672>.</span>update(<span style=color:#ae81ff>1</span>)</span></span></code></pre></div></div><h4 id=1324-evaluate>1.3.2.4 Evaluate</h4><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, (input_ids, input_mask, segment_ids, label_ids) <span style=color:#f92672>in</span> enumerate(eval_dataloader):
</span></span><span style=display:flex><span>    input_ids <span style=color:#f92672>=</span> input_ids<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    input_mask <span style=color:#f92672>=</span> input_mask<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    segment_ids <span style=color:#f92672>=</span> segment_ids<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    label_ids <span style=color:#f92672>=</span> label_ids<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        <span style=color:#75715e># label_ids = label_ids.to(torch.float)</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> model(input_ids<span style=color:#f92672>=</span>input_ids, token_type_ids<span style=color:#f92672>=</span>segment_ids, attention_mask<span style=color:#f92672>=</span>input_mask, labels<span style=color:#f92672>=</span>label_ids)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> outputs<span style=color:#f92672>.</span>logits
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span></span></span></code></pre></div></div><h2 id=14-accelerate>1.4 Accelerate</h2><p>Easily rain and use PyTorch models with multi-GPU, TPU, mixed-precision</p><h3 id=140-术语>1.4.0 术语</h3><p>关于分布式的一些概念和术语。</p><p>参考：</p><p><a href=https://zhuanlan.zhihu.com/p/544273093>https://zhuanlan.zhihu.com/p/544273093</a></p><p><a href=https://www.zhihu.com/question/453920336/answer/1828326535>https://www.zhihu.com/question/453920336/answer/1828326535</a></p><ul><li>Scatter: 把数据分发到其他线程 or 进程</li><li>Gather: 把数据从其他进程 or 线程中收集到一个主进程中</li><li>Reduce: 计算在各个进程 or 线程中进行，然后归约到一个主进程中</li><li>All-Reduce: 同 Reduce 一样，计算再各个进程 or 线程中进行，但是每个节点都一起归约，保持每个节点的结果一致</li><li>Broadcast: 把数据复制到各个进程 or 线程</li><li>All-Gather: 与 Gather 不同，每个进程 or 线程中完成数据的手机，保证每个节点的结果一致</li><li>CLI (Command Line Interface)：一种通过命令行来交互的工具或者应用，比如 mkdir, cd, scp, npm 等</li><li>Rank: 表示进程的编号/序号。（自己观察到一个 GPU 似乎对应一个 rank，不过 rank 和 GPU 没有严格的对应关系。如果是多进程共享 GPU，那么一个 GPU 可以为多个 rank 服务）</li><li>Node: 物理节点，可以是一台机器或者一个容器，节点内可以有多个 GPU</li><li>Rank 和 Local Rank: rank是指在整个分布式任务中进程的序号；local_rank是指在一个node上进程的相对序号，local_rank在node之间相互独立。</li><li>nnodes、node_rank 与 nproc_per_node： nnodes是指物理节点数量，node_rank是物理节点的序号；nproc_per_node是指每个物理节点上面进程的数量。</li><li>word size ： 全局（一个分布式任务）中，rank的数量。</li><li>csrc: 应该是 C source code</li><li>NVIDIA Megatron-LM: 针对Transformer类的模型提供半自动的分布式部署。</li><li>DeepSpeed: 微软提供的一个开源深度学习训练优化库。基于英伟达 Megatron-LM 进行了张量切分式的模型并行。</li></ul><p>同步 SGD：每个 Worker 都是同步计算一个批量。</p><h3 id=141-分布式概念-distributed>1.4.1 分布式概念 Distributed</h3><p>2022-10-24 10:43:08，日后填坑。</p><p>2022-11-01 09:04:30，开始学习。</p><p><code>nn.parallel.scatter(data, devices)</code>，可以将一组数据 split 到多个 devices 上</p><h4 id=1411-dataparallel>1.4.1.1 DataParallel</h4><p><code>nn.DataParallel(net, devices_ids=devices)</code>，DP 模型。单进程，多线程，适于一台机器的情况。</p><p>做法是将 model 复制到不同的 GPU 上，各个 GPU 计算不同 batch 的数据。</p><h4 id=1412-distributeddataparallel>1.4.1.2 DistributedDataParallel</h4><p><code>torch.nn.parallel.DistributedDataParallel</code>，DDP 模型（PyTorch 官网更推荐）。</p><p>需要启动 <code>init_process_group</code></p><p>如果想在一个有 N 个 GPU 的设备上面使用 DistributedDataParallel，则需要 spawn up N 个进程，每个进程对应0-N-1 的一个 GPU。这可以通过下面的语句实现</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>set_device(i)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># i from 0-N-1，每个进程中都需要：</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>init_process_group(
</span></span><span style=display:flex><span>    backend<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nccl&#39;</span>, world_size<span style=color:#f92672>=</span>N, init_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;...&#39;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> DistributedDataParallel(model, device_ids<span style=color:#f92672>=</span>[i], output_device<span style=color:#f92672>=</span>i)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 实践</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> args<span style=color:#f92672>.</span>local_rank <span style=color:#f92672>==</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> args<span style=color:#f92672>.</span>no_cuda:
</span></span><span style=display:flex><span>    device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;cuda&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> args<span style=color:#f92672>.</span>no_cuda <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>)
</span></span><span style=display:flex><span>    n_gpu <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>device_count()
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>set_device(args<span style=color:#f92672>.</span>local_rank)
</span></span><span style=display:flex><span>    device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda&#34;</span>, args<span style=color:#f92672>.</span>local_rank)
</span></span><span style=display:flex><span>    n_gpu <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initializes the distributed backend which will take care of</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># sychronizing nodes/GPUs.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> torch<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>is_initialized():
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>init_process_group(backend<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nccl&#39;</span>)</span></span></code></pre></div></div><p>多进程，适于多台机器的情况，可以结合模型并行的方法。</p><p>使用时建议</p><ul><li>使用一个 big dataset</li><li>好的 CPU-GPU 和机器-机器带宽</li><li>高效的 data load 以及 preprocess</li><li>模型需要有好的计算（FLOP）通讯（model size）比<ul><li>Inception > ResNet > AlexNet</li></ul></li><li>使用足够大的 batch size 来获取好的系统性能</li><li>使用高效的优化算法对应大批量大小</li></ul><h3 id=142-hugging-face-文档>1.4.2 Hugging face 文档</h3><h4 id=1421-步骤>1.4.2.1 步骤</h4><ul><li>import Accelerator 并且实例化</li></ul><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> accelerator <span style=color:#f92672>import</span> Accelerator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>accelerator <span style=color:#f92672>=</span> Accelerator()</span></span></code></pre></div></div><ul><li>消去 model 以及 input data 的 <code>.to(device)</code> 或者 <code>.cuda()</code> 操作。<code>accelerator</code> 会帮你完成这件事</li><li>把相关的对象（optimizer, model, train_dataloader, lr_scheduler）传递给 <code>prepare()</code> 方法</li></ul><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model, optimizer, train_dataloader, lr_scheduler <span style=color:#f92672>=</span> accelerator<span style=color:#f92672>.</span>prepare(
</span></span><span style=display:flex><span>    model, optimizer, train_dataloader, lr_scheduler
</span></span><span style=display:flex><span>)</span></span></code></pre></div></div><blockquote><p>每个 GPU 会加载 training dataset 中的不同部分（batch），这看起来像是数据并行。</p></blockquote><ul><li>Replace the line <code>loss.backward()</code> by <code>accelerator.backward(loss)</code>.</li></ul><h4 id=1422-distributed-evaluation>1.4.2.2 Distributed evaluation</h4><p>如果需要分布式评估，同理，把 validation dataloader 发送给 <code>prepare()</code> 方法。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>validation_dataloader <span style=color:#f92672>=</span> accelerator<span style=color:#f92672>.</span>prepare(validation_dataloader)</span></span></code></pre></div></div><p>这样的话每个 devices 只能看见 evaluation data 中的一部分，所以最后需要把结果 group 到一起，通过 <code>gather_for_metrics()</code> 方法。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> inputs, targets <span style=color:#f92672>in</span> validation_dataloader:
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> model(inputs)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Gather all predictions and targets</span>
</span></span><span style=display:flex><span>    all_predictions, all_targets <span style=color:#f92672>=</span> accelerator<span style=color:#f92672>.</span>gather_for_metrics((predictions, targets))
</span></span><span style=display:flex><span>    <span style=color:#75715e># Example of use with a *Datasets.Metric*</span>
</span></span><span style=display:flex><span>    metric<span style=color:#f92672>.</span>add_batch(all_predictions, all_targets)</span></span></code></pre></div></div><h4 id=1423-launch>1.4.2.3 Launch</h4><p><code>accelerate</code> 命令整合了在不同平台上发射脚本的命令，你无需自己记住所有命令。如果你自己熟悉 PyTorch 提供的发射脚本，也可以不使用 <code>accelerate</code>。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># 单 GPU 执行</span>
</span></span><span style=display:flex><span>CUDA_VISIBLE_DEVICES<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;0&#34;</span> accelerate launch <span style=color:#f92672>{</span>script_name.py<span style=color:#f92672>}</span> --arg1 --arg2 ...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 默认参数，使用所有的 GPU，不启动混合精度</span>
</span></span><span style=display:flex><span>accelerate launch --multi_gpu <span style=color:#f92672>{</span>script_name.py<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--arg1<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--arg2<span style=color:#f92672>}</span> ...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 当然，也可以指定参数</span>
</span></span><span style=display:flex><span>accelerate launch --multi_gpu --mixed_precision<span style=color:#f92672>=</span>fp16 --num_processes<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>{</span>script_name.py<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--arg1<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--arg2<span style=color:#f92672>}</span> ...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看所有参数</span>
</span></span><span style=display:flex><span>accelerate launch -h</span></span></code></pre></div></div><p><strong>accelerate config</strong></p><p>当然，更好的方式是配置一个 config 文件。使用了 <code>accelerate config</code> 之后，会在 <code>~/.cache/huggingface/accelerate</code> 目录下生成 <code>default_config.yaml</code> 文件。也可以自己定制 config 文件然后读取。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>accelerate launch --config_file ~/.cache/huggingface/accelerate <span style=color:#f92672>{</span>script_name.py<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--arg1<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--arg2<span style=color:#f92672>}</span> ...</span></span></code></pre></div></div><h2 id=15-dataset>1.5 Dataset</h2><h3 id=151-load-from-hugging-face>1.5.1 load from hugging face</h3><p>hugging face 下载文件存储的位置：<code>~/.cache/huggingface</code></p><p>找到 dataset 之后如何下载：</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 搜索到数据集 oscar，里面有很多 subset，比如：unshuffled_deduplicated_en, unshuffled_deduplicated_br</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 第一个参数是数据集的名称，第二个参数用于索引子集，一般还有 split 参数：train, test, validation </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;oscar&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;unshuffled_deduplicated_en&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 420G，赶紧中断</span>
</span></span><span style=display:flex><span>dataset<span style=color:#f92672>.</span>save_to_disk(<span style=color:#e6db74>&#34;./glue/&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 尝试下载 wikitext</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;wikitext&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;wikitext-103-v1&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 2022-10-24 14:07:31，是可以的</span>
</span></span><span style=display:flex><span>print(dataset[:<span style=color:#ae81ff>3</span>])
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;text&#39;</span>: [<span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#e6db74>&#39; = Valkyria Chronicles III = </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>]}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 保存</span>
</span></span><span style=display:flex><span>dataset<span style=color:#f92672>.</span>save_to_disk(<span style=color:#e6db74>&#34;./&#34;</span>)</span></span></code></pre></div></div><h3 id=152-数据集格式>1.5.2 数据集格式</h3><p>1.5.1 的例子中，默认保存为 <code>dataset.arrow</code> 加上 <code>.json</code> 文件。</p><p>现在我们想将其保存为 <code>.csv</code></p><h3 id=153-split-作用>1.5.3 split 作用</h3><p>下面是带有 feature 的 dataset 一个示例</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span><span style=color:#75715e># 如果用了 split </span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;rotten_tomatoes&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)
</span></span><span style=display:flex><span>print(dataset)
</span></span><span style=display:flex><span>Dataset({
</span></span><span style=display:flex><span>    features: [<span style=color:#e6db74>&#39;text&#39;</span>, <span style=color:#e6db74>&#39;label&#39;</span>],
</span></span><span style=display:flex><span>    num_rows: <span style=color:#ae81ff>8530</span>
</span></span><span style=display:flex><span>})
</span></span><span style=display:flex><span><span style=color:#75715e># 如果不用 split，</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;rotten_tomatoes&#34;</span>)
</span></span><span style=display:flex><span>DatasetDict({
</span></span><span style=display:flex><span>    train: Dataset({
</span></span><span style=display:flex><span>        features: [<span style=color:#e6db74>&#39;text&#39;</span>, <span style=color:#e6db74>&#39;label&#39;</span>],
</span></span><span style=display:flex><span>        num_rows: <span style=color:#ae81ff>8530</span>
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>    validation: Dataset({
</span></span><span style=display:flex><span>        features: [<span style=color:#e6db74>&#39;text&#39;</span>, <span style=color:#e6db74>&#39;label&#39;</span>],
</span></span><span style=display:flex><span>        num_rows: <span style=color:#ae81ff>1066</span>
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>    test: Dataset({
</span></span><span style=display:flex><span>        features: [<span style=color:#e6db74>&#39;text&#39;</span>, <span style=color:#e6db74>&#39;label&#39;</span>],
</span></span><span style=display:flex><span>        num_rows: <span style=color:#ae81ff>1066</span>
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>})</span></span></code></pre></div></div><h2 id=16-github-文档>1.6 Github 文档</h2><p>所有任务的 example 存放在 <a href=https://github.com/huggingface/transformers/tree/main/examples/pytorch>https://github.com/huggingface/transformers/tree/main/examples/pytorch</a> 里面。</p><h1 id=2-知识补充>2. 知识补充</h1><p>补充一些相关的，之前不熟悉的概念。</p><h2 id=22-data-set>2.2 Data Set</h2><h3 id=221-glue>2.2.1 GLUE</h3><p>数据集是之前屏蔽的一个部分，但是如果想了解模型是在做什么，并且要动手做实验，必须了解数据集。</p><p>GLUE 是学术界非常常用的数据集。</p><table><thead><tr><th>数据集</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>MNLI</td><td>句子对分类任务</td><td>给定一个前提 (Premise) ，根据这个前提去推断假设 (Hypothesis) 与前提的关系。该任务的关系分为三种，蕴含关系 (Entailment)、矛盾关系 (Contradiction) 以及中立关系 (Neutral)。</td></tr><tr><td>QQP</td><td>句子对分类任务</td><td>判断 Quora 上的两个问题句是否表示的是一样的意思</td></tr><tr><td>SST-2</td><td>分类任务</td><td>情感分析</td></tr><tr><td>CoLA</td><td>单句分类任务</td><td>句子语义判断，是否是可接受的（Acceptable）</td></tr><tr><td>QNLI</td><td></td><td>用于判断文本是否包含问题的答案</td></tr><tr><td>STS-B</td><td></td><td>预测两个句子的相似性，包括5个级别。</td></tr><tr><td>MRPC</td><td>句子对分类任务</td><td>也是判断两个句子是否是等价的。</td></tr><tr><td>RTE</td><td>分类</td><td>类似于MNLI，但是只是对蕴含关系的二分类判断，而且数据集更小。</td></tr><tr><td>SWAG</td><td></td><td>从四个句子中选择为可能为前句下文的那个</td></tr></tbody></table><h1 id=3-编程实例>3. 编程实例</h1><h2 id=31-通用知识>3.1 通用知识</h2><h3 id=311-主要的组件>3.1.1 主要的组件</h3><ul><li>Configuration: 编程模型中的参数或是变量。用于配置词表大小，隐藏层维数，Dropout rate 等等。给出一个 BERT_base 配置的示例。</li></ul><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;attention_probs_dropout_prob&#34;</span>: 0.1,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;hidden_act&#34;</span>: <span style=color:#e6db74>&#34;gelu&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;hidden_dropout_prob&#34;</span>: 0.1,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;hidden_size&#34;</span>: 768,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;initializer_range&#34;</span>: 0.02,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;intermediate_size&#34;</span>: 3072,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;max_position_embeddings&#34;</span>: 512,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;num_attention_heads&#34;</span>: 12,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;num_hidden_layers&#34;</span>: 12,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;type_vocab_size&#34;</span>: 2,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;vocab_size&#34;</span>: <span style=color:#ae81ff>30522</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div></div><ul><li>Tokenizer: 每个模型都有对应的分词器。存储 token 到 index 的映射。</li><li>Model: 实现模型的计算图和编码过程，实现前向传播。对于 output 层，不同的模型有不一样的封装。比如今天（2022-10-23）debug 观察的 <code>BaseModelOutputWithPoolingAndCrossAttentions</code>，<code>类型：SequenceClassifierOutput</code> 等等。</li></ul><h3 id=312-任务分类>3.1.2 任务分类</h3><p>语言生成任务，对应模型：GPT，GPT-2，XLNet</p><p>语言理解任务，对应模型：BERT，RoBERTa，XLM</p><h2 id=32-在-gpt-2-上运行-wikitext>3.2 在 GPT-2 上运行 wikitext</h2><p>README：</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pip install transformers
</span></span><span style=display:flex><span>ln -s ../BERT/glue/ ./glue</span></span></code></pre></div></div><h3 id=301-坑1>3.0.1 坑1</h3><p><strong>坑，和 BERT 区分</strong></p><p>BERT forward 和 GPT-2 forward 不一样。之前实现的 <code>model(inputs_id, token_type_ids, attention_mask)</code> 按位置传参，但是 GPT-2 forward 第二个参数是 <code>past_key_values</code>，所以报错，按关键字传参即可解决这个问题。</p><p>他们经过前向之后，output 也不一样。</p><h3 id=302-大坑2>3.0.2 大坑2</h3><p>2022-10-26 22:31:46，复现 GPT 的结果出了大问题，好在今晚解决了。问题来源于框架中会自动启动分布式 train/eval，但是没有很好地支持分布式（单 GPU 运行 30s，4 GPU 运行 6min），而且 loss 的处理也有问题，导致最终的 metric - PPL 出了问题。用单 GPU 可以解决这个问题。</p><h3 id=303-坑3>3.0.3 坑3</h3><p>2022-10-28 16:03:02，框架K 是不支持分布式的，这导致了 local_rank 索引不到，因此执行 evaluate()，进入 quant 模块时报错。</p><h3 id=321-prepare-dataset>3.2.1 Prepare Dataset</h3><p>读取 datasets 并保存到本地</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>test_dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;json&#34;</span>, data_files<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;test.json&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)
</span></span><span style=display:flex><span>test_dataset<span style=color:#f92672>.</span>save_to_disk(<span style=color:#e6db74>&#34;test.hf&#34;</span>)</span></span></code></pre></div></div><h3 id=322-lm-预处理>3.2.2 LM 预处理</h3><p>针对生成类模型的预处理。如何从原始的数据集生成 tokenizer 之后的数据。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>row_datasets <span style=color:#f92672>=</span> load_dataset(data_args<span style=color:#f92672>.</span>dataset_name, data_args<span style=color:#f92672>.</span>dataset_config_name, cache_dir<span style=color:#f92672>=</span>model_args<span style=color:#f92672>.</span>cache_dir)
</span></span><span style=display:flex><span><span style=color:#75715e># tokenizer</span>
</span></span><span style=display:flex><span>tokenized_datasets <span style=color:#f92672>=</span> raw_datasets<span style=color:#f92672>.</span>map(
</span></span><span style=display:flex><span>    tokenize_function,
</span></span><span style=display:flex><span>    batched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    num_proc<span style=color:#f92672>=</span>data_args<span style=color:#f92672>.</span>preprocessing_num_workers,
</span></span><span style=display:flex><span>    remove_columns<span style=color:#f92672>=</span>column_names,
</span></span><span style=display:flex><span>    load_from_cache_file<span style=color:#f92672>=</span><span style=color:#f92672>not</span> data_args<span style=color:#f92672>.</span>overwrite_cache,
</span></span><span style=display:flex><span>    desc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Running tokenizer on dataset&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> training_args<span style=color:#f92672>.</span>main_process_first(desc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;grouping texts together&#34;</span>):
</span></span><span style=display:flex><span>    lm_datasets <span style=color:#f92672>=</span> tokenized_datasets<span style=color:#f92672>.</span>map(
</span></span><span style=display:flex><span>        group_texts,
</span></span><span style=display:flex><span>        batched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        num_proc<span style=color:#f92672>=</span>data_args<span style=color:#f92672>.</span>preprocessing_num_workers,
</span></span><span style=display:flex><span>        load_from_cache_file<span style=color:#f92672>=</span><span style=color:#f92672>not</span> data_args<span style=color:#f92672>.</span>overwrite_cache,
</span></span><span style=display:flex><span>        desc<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Grouping texts in chunks of </span><span style=color:#e6db74>{</span>block_size<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span><span style=color:#75715e># 计算 token 长度</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> split, data <span style=color:#f92672>in</span> lm_datasets<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>total_eval_tokens <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>        
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> data[<span style=color:#e6db74>&#39;labels&#39;</span>]:
</span></span><span style=display:flex><span>    total_eval_tokens <span style=color:#f92672>+=</span> len([x <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> chunk[<span style=color:#ae81ff>1</span>:] <span style=color:#66d9ef>if</span> x <span style=color:#f92672>!=</span> padding_index])
</span></span><span style=display:flex><span>logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;[</span><span style=color:#e6db74>{</span>split<span style=color:#e6db74>}</span><span style=color:#e6db74>] Total eval tokens: </span><span style=color:#e6db74>{</span>total_eval_tokens<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 得到 eval dataset 以及 train dataset</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> training_args<span style=color:#f92672>.</span>do_train:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;train&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> tokenized_datasets:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;--do_train requires a train dataset&#34;</span>)
</span></span><span style=display:flex><span>    train_dataset <span style=color:#f92672>=</span> lm_datasets[<span style=color:#e6db74>&#34;train&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> data_args<span style=color:#f92672>.</span>max_train_samples <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        train_dataset <span style=color:#f92672>=</span> train_dataset<span style=color:#f92672>.</span>select(range(data_args<span style=color:#f92672>.</span>max_train_samples))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> training_args<span style=color:#f92672>.</span>do_eval:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;validation&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> tokenized_datasets:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;--do_eval requires a validation dataset&#34;</span>)
</span></span><span style=display:flex><span>    eval_dataset <span style=color:#f92672>=</span> lm_datasets[data_args<span style=color:#f92672>.</span>eval_subset]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> data_args<span style=color:#f92672>.</span>max_eval_samples <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        eval_dataset <span style=color:#f92672>=</span> eval_dataset<span style=color:#f92672>.</span>select(range(data_args<span style=color:#f92672>.</span>max_eval_samples))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 这个时候 eval_dataset size (480, 3)，3 个维度分别是 input_ids, attention_mask, labels</span></span></span></code></pre></div></div><h2 id=33-在-t5-上运行-wmt>3.3 在 T5 上运行 WMT</h2><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>MODEL<span style=color:#f92672>=</span>t5-small
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>CUDA_VISIBLE_DEVICES<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> python -u run_translation.py  <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --model_name_or_path <span style=color:#e6db74>${</span>MODEL<span style=color:#e6db74>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --dataset_name wmt16 --dataset_config_name ro-en <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --per_device_eval_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --output_dir checkpoints-translation/<span style=color:#e6db74>${</span>MODEL<span style=color:#e6db74>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --source_lang en --target_lang ro <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --do_eval <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --predict_with_generate <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --source_prefix <span style=color:#e6db74>&#34;translate English to Romanian: &#34;</span></span></span></code></pre></div></div><h1 id=4-编程-flow>4. 编程 flow</h1><p>利用 debug 以及 hugging face 文档，了解整个流程。在 Section 1 介绍 hugging face 指导文档，这部分结合实际编程来看。</p><h2 id=41-数据读取及预处理>4.1 数据读取及预处理</h2><h3 id=421-数据读取>4.2.1 数据读取</h3><p>通过参数 <code>--data_dir</code>，把本地的数据加载进来，应该包含 <code>.tsv</code> 文件。hugging face 的预训练模型可以下载到本地并保存，那么 Dataset 应该也可以。</p><h3 id=422-dataloader>4.2.2 DataLoader</h3><p>经过处理后的 <code>train_features</code>，长度是 8551 examples，装的是经过 tokenizer 后的数据。</p><p>原始的 <code>train_example</code>，从 <code>./glue/CoLA/train.tsv</code> 读入，长度是 8551 examples，每个 example 装的是：</p><ul><li><code>guid</code>:<code>train-1</code> or <code>train=2</code>&mldr;</li><li><code>label</code>:<code>1</code> or <code>0</code></li><li><code>text_a</code>:<code>Our friends won't buy this analysis...</code></li><li><code>text_b</code>:None</li></ul><p><code>train_data</code> 就是通过 <code>gen_tensor_dataset()</code> 把 <code>train_features</code> 转换为 tensor；经过 <code>DataLoader(train_data, sampler, batch_size)</code> 方法之后，得到 <code>train_dataloader</code>，<code>sampler</code> 就是 8551。</p><p>在一个 epoch 中，需要遍历一遍整个 train_dataloader，一个 iteration 处理其中一个 batch，train_loader 遍历完就是一个 epoch。</p><blockquote><p>记得数据需要转移到 device</p></blockquote><h3 id=42x-tokenizer>4.2.X Tokenizer</h3><p><code>input_ids</code> 存放的就是 tokenizer 后的数据本身，是一个 (64, 128) 的 tensor。<code>mask</code>, <code>segment_id</code> 也是 (64, 128)，label 是 (64)，这应该就是最后的分类，因为 batch size 是 64，所以有 64 个输出。</p><p>对于 hugging face BERT，经过 tokenizer 打印出来就是一整个 mapping，也就是对应 <code>run_glue.py</code> 中遍历 <code>train_dataloader</code> 的变量 <code>batch</code>。不过 <code>batch</code> 的类型是 list，而下面的 <code>inputs</code> 是 dict。而 hugging face BERT 的 <code>output</code> 也是一个 <code>BaseModelOutputWithPoolingAndCrossAttentions</code> 类型。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer, BertModel
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> BertModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#34;Hello, my dog is cute&#34;</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>print(inputs)
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;input_ids&#39;</span>: tensor([[  <span style=color:#ae81ff>101</span>,  <span style=color:#ae81ff>7592</span>,  <span style=color:#ae81ff>1010</span>,  <span style=color:#ae81ff>2026</span>,  <span style=color:#ae81ff>3899</span>,  <span style=color:#ae81ff>2003</span>, <span style=color:#ae81ff>10140</span>,   <span style=color:#ae81ff>102</span>]]), <span style=color:#e6db74>&#39;token_type_ids&#39;</span>: tensor([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]]), <span style=color:#e6db74>&#39;attention_mask&#39;</span>: tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>]])}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>inputs)
</span></span><span style=display:flex><span><span style=color:#75715e># output 输出包含 last_hidden_state, pooler_output，其他属性 hidden_states, past_key_values, attentions, cross_attentions 为 None</span></span></span></code></pre></div></div><p>对于 hugging face BertForSequenceClassification，</p><ul><li><code>output</code> 类型：SequenceClassifierOutput(loss=None, logits=tensor([[-2.2095, 2.5760]]), hidden_states=None, attentions=None)</li></ul><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python></code></pre></div></div><h2 id=43-计算-loss>4.3 计算 loss</h2><p>hugging face 有一个普通 trainer 的实例，关于如何计算 loss。看一下能不能套用到原先 BERT 的框架中。</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> Trainer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CustomTrainer</span>(Trainer):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(self, model, inputs, return_outputs<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>        labels <span style=color:#f92672>=</span> inputs<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;labels&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># forward pass</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>inputs)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> outputs<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;logits&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># compute custom loss (suppose one has 3 labels with different weights)</span>
</span></span><span style=display:flex><span>        loss_fct <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss(weight<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]))
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> loss_fct(logits<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>num_labels), labels<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (loss, outputs) <span style=color:#66d9ef>if</span> return_outputs <span style=color:#66d9ef>else</span> loss</span></span></code></pre></div></div><p>2022-10-23 18:32:34，直接用 forward 函数中计算的 loss</p><div class=highlight-container><button class="copy-code-btn outline">Copy</button><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>label_ids <span style=color:#f92672>=</span> label_ids<span style=color:#f92672>.</span>to(torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> model(input_ids<span style=color:#f92672>=</span>input_ids, token_type_ids<span style=color:#f92672>=</span>segment_ids, attention_mask<span style=color:#f92672>=</span>input_mask, labels<span style=color:#f92672>=</span>label_ids)
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> outputs<span style=color:#f92672>.</span>logits
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> outputs<span style=color:#f92672>.</span>loss</span></span></code></pre></div></div><p>注意，对于 <code>single_label_classification</code> 分类任务，不要将 <code>label_ids</code> 转化成 int。</p><h1 id=5-理解>5. 理解</h1><h2 id=51-深度学习研究者在做什么>5.1 深度学习研究者在做什么</h2><p>转载自 <a href=https://www.zhihu.com/question/433274875/answer/2240764095>https://www.zhihu.com/question/433274875/answer/2240764095</a></p><p>以下阶段层层递进</p><ul><li>把已有的开源模型下载下来，换成自己的数据集。这个时候应该是偏向于应用型研究，把动物图像数据集换成自己领域的，医学图像数据集，并且跑出 SOTA 的准确率。</li></ul><blockquote><p>这个过程要求看懂开源的代码架构，loss, optimizer, 基本的前向反向传播应该无需修改，但是需要了解数据的预处理，输入的数据是什么样的，如何进行数据转换，数据集从哪里下载，用哪个函数 load data。</p></blockquote><ul><li>从理论上理解模型的一些算法和思想，知道一些重要的超参数背后的思想。有信心调整它，并符合自己预期的效果</li></ul><blockquote><p>最显然的，learning rate, batch size，又比如量化时的 scale factor 搜索范围</p></blockquote><ul><li>深入了解模型的代码实现和细节，能够为模型添加一些主流的，提升性能的 module 或者 trick；能够分析修改前后，模型的差异；对模型的推理和训练过程了如指掌</li></ul><blockquote><p>比如分析推理时某个 epoch 的激活分布情况，分布出现了什么问题，如何解决，loss 变化有什么问题等等</p></blockquote><ul><li>能够根据自己的业务需求，拓展模型的功能，让模型能做更多的事情</li></ul><blockquote><p>这个可能更多体现在业务需求层面，如何去部署公司需要的模型。不过我认为做了量化框架的拓展，也达到这个水平了。</p></blockquote><h1 id=reference>Reference</h1><p><a href=https://zhuanlan.zhihu.com/p/120315111>https://zhuanlan.zhihu.com/p/120315111</a></p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Weiming Hu</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-11-26</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=https://huweim.github.io/tags/huggingface/>huggingface</a></div><nav class=post-nav><a class=prev href=/post/%E7%BC%96%E7%A8%8B_makefile%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%AE%9E%E8%B7%B5/><i class=iconfont><svg aria-hidden="true" class="lucide lucide-chevron-left hi-svg-inline" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="m15 18-6-6 6-6"/></svg>
</i><span class="prev-text nav-default">Makefile学习和实践</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/post/%E6%80%BB%E7%BB%93_having-effective-meetings-between-advisors-and-students/><span class="next-text nav-default">（转载）Having Effective Meetings Between Advisors and Students</span>
<span class="prev-text nav-mobile">Next</span>
<i class=iconfont><svg aria-hidden="true" class="lucide lucide-chevron-right hi-svg-inline" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="m9 18 6-6-6-6"/></svg></i></a></nav></footer></article></div><nav class=toc id=toc><div class=toc-title>Table of Contents</div><div class="toc-content custom-scrollbar"><nav id=TableOfContents><ul><li><a href=#0-前言>0. 前言</a><ul><li><a href=#01-模型及下载地址>0.1 模型及下载地址</a></li><li><a href=#02-模型下载方法>0.2 模型下载方法</a><ul><li><a href=#021-git-lfs>0.2.1 git lfs</a></li><li><a href=#022-hungging-face-hub>0.2.2 hungging face hub</a></li></ul></li></ul></li><li><a href=#1-hugging-face>1. Hugging Face</a><ul><li><a href=#11-简介>1.1 简介</a><ul><li><a href=#111-用途>1.1.1 用途</a></li><li><a href=#112-重要概念>1.1.2 重要概念</a><ul><li><a href=#1121-pipeline>1.1.2.1 Pipeline</a></li><li><a href=#1122-autoclass>1.1.2.2 AutoClass</a></li><li><a href=#1123-custom-model>1.1.2.3 Custom Model</a></li><li><a href=#1124-trainer---a-pytorch-optimized-training-loop>1.1.2.4 Trainer - a PyTorch optimized training loop</a></li><li><a href=#1125-预处理>1.1.2.5 预处理</a></li></ul></li><li><a href=#113-模型-model>1.1.3 模型 Model</a></li></ul></li><li><a href=#12-api>1.2 API</a><ul><li><a href=#121-from_pretrained>1.2.1 from_pretrained</a></li></ul></li><li><a href=#13-fine-tune-a-pretrained-model>1.3 Fine-tune a pretrained model</a><ul><li><a href=#131-prepare-a-dataset>1.3.1 Prepare a dataset</a></li><li><a href=#132-train>1.3.2 Train</a><ul><li><a href=#1321-dataloader>1.3.2.1 DataLoader</a></li><li><a href=#1322-optimizer-and-learning-rate-scheduler>1.3.2.2 Optimizer and learning rate scheduler</a></li><li><a href=#1323-training-loop>1.3.2.3 Training Loop</a></li><li><a href=#1324-evaluate>1.3.2.4 Evaluate</a></li></ul></li></ul></li><li><a href=#14-accelerate>1.4 Accelerate</a><ul><li><a href=#140-术语>1.4.0 术语</a></li><li><a href=#141-分布式概念-distributed>1.4.1 分布式概念 Distributed</a><ul><li><a href=#1411-dataparallel>1.4.1.1 DataParallel</a></li><li><a href=#1412-distributeddataparallel>1.4.1.2 DistributedDataParallel</a></li></ul></li><li><a href=#142-hugging-face-文档>1.4.2 Hugging face 文档</a><ul><li><a href=#1421-步骤>1.4.2.1 步骤</a></li><li><a href=#1422-distributed-evaluation>1.4.2.2 Distributed evaluation</a></li><li><a href=#1423-launch>1.4.2.3 Launch</a></li></ul></li></ul></li><li><a href=#15-dataset>1.5 Dataset</a><ul><li><a href=#151-load-from-hugging-face>1.5.1 load from hugging face</a></li><li><a href=#152-数据集格式>1.5.2 数据集格式</a></li><li><a href=#153-split-作用>1.5.3 split 作用</a></li></ul></li><li><a href=#16-github-文档>1.6 Github 文档</a></li></ul></li><li><a href=#2-知识补充>2. 知识补充</a><ul><li><a href=#22-data-set>2.2 Data Set</a><ul><li><a href=#221-glue>2.2.1 GLUE</a></li></ul></li></ul></li><li><a href=#3-编程实例>3. 编程实例</a><ul><li><a href=#31-通用知识>3.1 通用知识</a><ul><li><a href=#311-主要的组件>3.1.1 主要的组件</a></li><li><a href=#312-任务分类>3.1.2 任务分类</a></li></ul></li><li><a href=#32-在-gpt-2-上运行-wikitext>3.2 在 GPT-2 上运行 wikitext</a><ul><li><a href=#301-坑1>3.0.1 坑1</a></li><li><a href=#302-大坑2>3.0.2 大坑2</a></li><li><a href=#303-坑3>3.0.3 坑3</a></li><li><a href=#321-prepare-dataset>3.2.1 Prepare Dataset</a></li><li><a href=#322-lm-预处理>3.2.2 LM 预处理</a></li></ul></li><li><a href=#33-在-t5-上运行-wmt>3.3 在 T5 上运行 WMT</a></li></ul></li><li><a href=#4-编程-flow>4. 编程 flow</a><ul><li><a href=#41-数据读取及预处理>4.1 数据读取及预处理</a><ul><li><a href=#421-数据读取>4.2.1 数据读取</a></li><li><a href=#422-dataloader>4.2.2 DataLoader</a></li><li><a href=#42x-tokenizer>4.2.X Tokenizer</a></li></ul></li><li><a href=#43-计算-loss>4.3 计算 loss</a></li></ul></li><li><a href=#5-理解>5. 理解</a><ul><li><a href=#51-深度学习研究者在做什么>5.1 深度学习研究者在做什么</a></li></ul></li><li><a href=#reference>Reference</a></li></ul></nav></div></nav></div></main><footer id=footer class=site-footer><div class=social-icon-links><a href=mailto:huwm1@shanghaitech.edu.cn rel="me noopener" class=social-icon-link title=email><svg aria-hidden="true" class="icon hi-svg-inline" fill="currentcolor" height="1em" viewBox="0 0 1451 1024" width="1em" xlink="http://www.w3.org/1999/xlink"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408h399.992405s71.046998 3.997201 71.046998 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zm53.281707 130.131124C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523S0 1024 83.726336 1024H682.532949 753.579947h595.368192C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955c-39.771477 34.470494-74.671786 43.295855-98.955861 43.868928z"/></svg>
</a><a href=http://localhost:1313 rel="me noopener" class=social-icon-link title=linkedin target=_blank><svg aria-hidden="true" class="icon hi-svg-inline" fill="currentcolor" height="1em" viewBox="0 0 1024 1024" width="1em" xlink="http://www.w3.org/1999/xlink"><path d="M872.405333 872.618667H720.768V635.008c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333.0-91.136 61.653333-91.136 125.397334v241.792H398.976V384H544.64v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667.0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667.0 01-88.021333-88.106666A88.064 88.064.0 11227.712 317.141333zm76.032 555.477334H151.68V384h152.064v488.618667zM948.266667.0h-872.704C33.792.0.0 33.024.0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667.0 948.138667.0h.128z"/></svg>
</a><a href=https://github.com/huweim rel="me noopener" class=social-icon-link title=github target=_blank><svg aria-hidden="true" class="icon hi-svg-inline" fill="currentcolor" height="1em" viewBox="0 0 1024 1024" width="1em" xlink="http://www.w3.org/1999/xlink"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04C242.005334 929.664 211.968 830.08 211.968 830.08 188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg>
</a><a href=https://www.zhihu.com/people/hu-wei-ming-31-86 rel="me noopener" class=social-icon-link title=zhihu target=_blank><svg aria-hidden="true" class="icon hi-svg-inline" fill="currentcolor" height="1em" viewBox="0 0 1024 1024" width="1em" xlink="http://www.w3.org/1999/xlink"><path d="M351.791182 562.469462h192.945407c0-45.367257-21.3871-71.939449-21.3871-71.939449L355.897709 490.530013c3.977591-82.182744 7.541767-187.659007 8.816806-226.835262h159.282726s-.86367-67.402109-18.578124-67.402109-279.979646.0-279.979646.0 16.850783-88.141456 39.318494-127.053698c0 0-83.60514-4.510734-112.121614 106.962104S81.344656 355.077018 76.80834 367.390461s24.62791 5.832845 36.941354.0c12.313443-5.832845 68.050885-25.924439 84.252893-103.69571h86.570681c1.165546 49.28652 4.596691 200.335724 3.515057 226.835262H109.86113c-25.275663 18.147312-33.701566 71.939449-33.701566 71.939449H279.868105c-8.497535 56.255235-23.417339 128.763642-44.275389 167.210279-33.05279 60.921511-50.55235 116.65793-169.802314 212.576513.0.0-19.442818 14.257725 40.829917 9.073656 60.273758-5.185093 117.305683-20.739347 156.840094-99.807147 20.553105-41.107233 41.805128-93.250824 58.386782-146.138358l-.055259.185218 167.855986 193.263655s22.035876-51.847855 5.832845-108.880803L371.045711 650.610918l-42.1244 31.157627-.045025.151449c11.69946-41.020252 20.11206-81.5749 22.726607-116.858498C351.665315 564.212152 351.72876 563.345412 351.791182 562.469462z"/><path d="M584.918753 182.033893v668.840094h70.318532l28.807093 80.512708 121.875768-80.512708h153.600307L959.520453 182.033893h-374.6017zM887.150192 778.934538h-79.837326l-99.578949 65.782216-23.537066-65.782216h-24.855084L659.341766 256.673847h227.807403V778.934538z"/></svg>
</a><a href=https://huweim.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=social-icon-link title=rss target=_blank><svg aria-hidden="true" class="lucide lucide-rss hi-svg-inline" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a>
</span><span class=copyright-year>&copy;
2020 -
2025
<span class=heart><i class=iconfont><svg aria-hidden="true" class="lucide lucide-heart hi-svg-inline" fill="none" height="1em" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="1em"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5.0 0016.5 3c-1.76.0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5.0 002 8.5c0 2.3 1.5 4.05 3 5.5l7 7z"/></svg>
</i></span><span class=author>Weiming Hu</span></span></div></footer><script type=text/javascript src=/js/main.eb94e793601239645bc98e36c443aef1b210646ccb43e2217ea949a0212e0ed1.js integrity="sha256-65Tnk2ASOWRbyY42xEOu8bIQZGzLQ+IhfqlJoCEuDtE=" crossorigin=anonymous></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>