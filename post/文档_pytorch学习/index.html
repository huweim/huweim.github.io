<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>PyTorch 学习 - Cory</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="Cory"><meta name=description content="1.2 Tensor 张量，多维数组。 数据类型需要注意一下 关于 dtype，PyTorch 提供了 9 种数据类型，共分为 3 大类：float (16-bit, 32-bit, 64-bit)、int"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.110.0"><link rel=canonical href=https://huweim.github.io/post/%E6%96%87%E6%A1%A3_pytorch%E5%AD%A6%E4%B9%A0/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css integrity="sha256-+ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media=screen crossorigin=anonymous><meta property="og:title" content="PyTorch 学习"><meta property="og:description" content="1.2 Tensor 张量，多维数组。 数据类型需要注意一下 关于 dtype，PyTorch 提供了 9 种数据类型，共分为 3 大类：float (16-bit, 32-bit, 64-bit)、int"><meta property="og:type" content="article"><meta property="og:url" content="https://huweim.github.io/post/%E6%96%87%E6%A1%A3_pytorch%E5%AD%A6%E4%B9%A0/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-07-28T16:05:35+08:00"><meta property="article:modified_time" content="2022-08-11T16:05:35+08:00"><meta itemprop=name content="PyTorch 学习"><meta itemprop=description content="1.2 Tensor 张量，多维数组。 数据类型需要注意一下 关于 dtype，PyTorch 提供了 9 种数据类型，共分为 3 大类：float (16-bit, 32-bit, 64-bit)、int"><meta itemprop=datePublished content="2022-07-28T16:05:35+08:00"><meta itemprop=dateModified content="2022-08-11T16:05:35+08:00"><meta itemprop=wordCount content="5507"><meta itemprop=keywords content="PyTorch,"><meta name=twitter:card content="summary"><meta name=twitter:title content="PyTorch 学习"><meta name=twitter:description content="1.2 Tensor 张量，多维数组。 数据类型需要注意一下 关于 dtype，PyTorch 提供了 9 种数据类型，共分为 3 大类：float (16-bit, 32-bit, 64-bit)、int"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Cory</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/post>All posts</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/archives>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/tags>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://huweim.github.io/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>Cory</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/>Home</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/post>All posts</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/archives>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/tags>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://huweim.github.io/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>PyTorch 学习</h1><div class=post-meta><time datetime=2022-07-28 class=post-time>2022-07-28</time><div class=post-category><a href=https://huweim.github.io/categories/%E5%B7%A5%E5%85%B7/>工具</a></div><span class=more-meta>5507 words</span>
<span class=more-meta>11 min read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Table of Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#12-tensor>1.2 Tensor</a></li></ul><ul><li><a href=#11-parser-模块>1.1 parser 模块</a><ul><li><a href=#111-parseradd_argument>1.1.1 parser.add_argument()</a></li><li><a href=#112-parserparse_args>1.1.2 parser.parse_args()</a></li></ul></li></ul><ul><li><a href=#21-nn>2.1 nn</a><ul><li><a href=#211-nnmodule>2.1.1 nn.Module</a></li><li><a href=#212-nnlayer>2.1.2 nn.Layer</a></li><li><a href=#213-model-的创建>2.1.3 model 的创建</a></li><li><a href=#214-crossentropyloss>2.1.4 CrossEntropyLoss</a></li></ul></li><li><a href=#22-tensor>2.2 Tensor</a></li><li><a href=#23-autograd>2.3 autograd</a></li><li><a href=#24-data-模块>2.4 data 模块</a><ul><li><a href=#241-dataloader>2.4.1 DataLoader</a></li><li><a href=#242-dataset>2.4.2 DataSet</a></li><li><a href=#242-torchvision>2.4.2 torchvision</a></li><li><a href=#243-batch-size>2.4.3 Batch Size</a></li></ul></li><li><a href=#25-模型训练>2.5 模型训练</a><ul><li><a href=#251-损失函数>2.5.1 损失函数</a></li><li><a href=#252-optimizer>2.5.2 optimizer</a></li></ul></li><li><a href=#26-regularization-正则化>2.6 Regularization 正则化</a><ul><li><a href=#261-weight-decay>2.6.1 weight decay</a></li><li><a href=#262-dropout>2.6.2 Dropout</a></li><li><a href=#263-normalization>2.6.3 Normalization</a></li></ul></li><li><a href=#27-model-相关的操作>2.7 Model 相关的操作</a><ul><li><a href=#271-torchsave>2.7.1 torch.save</a></li><li><a href=#272-torchload>2.7.2 torch.load</a></li><li><a href=#273-fine-tuning>2.7.3 Fine-tuning</a></li></ul></li></ul><ul><li><a href=#31-debug-分布式训练的模型>3.1 debug 分布式训练的模型</a></li></ul><ul><li><a href=#41-register_hook>4.1 register_hook</a><ul><li><a href=#411-register_forward_hook>4.1.1 register_forward_hook</a></li><li><a href=#412-register_backward_hook>4.1.2 register_backward_hook()</a></li></ul></li></ul></nav></div></div><div class=post-content><h2 id=12-tensor>1.2 Tensor</h2><p>张量，多维数组。</p><p>数据类型需要注意一下</p><blockquote><p>关于 dtype，PyTorch 提供了 9 种数据类型，共分为 3 大类：float (16-bit, 32-bit, 64-bit)、integer (unsigned-8-bit ,8-bit, 16-bit, 32-bit, 64-bit)、Boolean。模型参数和数据用的最多的类型是 float-32-bit。label 常用的类型是 integer-64-bit。</p></blockquote><h1 id=1-python-模块>1. Python 模块</h1><h2 id=11-parser-模块>1.1 parser 模块</h2><h3 id=111-parseradd_argument>1.1.1 parser.add_argument()</h3><p>在命令行给代码赋值，不需要反复在 python 中修改代码。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--file-dir&#39;</span>,type<span style=color:#f92672>=</span>str, required<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,help<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Input file directory&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 实例</span>
</span></span><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--dataset&#39;</span>, default<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cifar10&#39;</span>, type<span style=color:#f92672>=</span>str, 
</span></span><span style=display:flex><span>                    help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dataset name&#39;</span>)
</span></span><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--dataset_path&#39;</span>, default<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;/state/partition/imagenet-raw-data&#39;</span>, type<span style=color:#f92672>=</span>str, 
</span></span><span style=display:flex><span>                    help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dataset path&#39;</span>)
</span></span><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--model&#39;</span>, default<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;resnet18&#39;</span>, type<span style=color:#f92672>=</span>str, 
</span></span><span style=display:flex><span>                    help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model name&#39;</span>)
</span></span><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--train&#39;</span>, default<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, action<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;store_true&#39;</span>, 
</span></span><span style=display:flex><span>                    help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;train&#39;</span>)
</span></span></code></pre></div><p><code>action</code>: <code>-train</code> 设置成一个开关，</p><ul><li>如果使用了 <code>python -u -m --train ...</code>，就会把参数 <code>--train</code> 设置为 True</li><li><code>python -u -m ...</code>，没有这个开关，则参数存储为 False</li></ul><h3 id=112-parserparse_args>1.1.2 parser.parse_args()</h3><h1 id=2-torch>2. torch</h1><p>有很多方便的数学操作，同理，先了解有这个东西，需要用到时看具体的用法。包括 torch.rand(), torch.range(), torch.chunk(), torch.normal(), torch.add()</p><p>pytorch 主要分为五大模块</p><ul><li>dataset</li><li>model</li><li>loss funtion</li><li>optimizer</li><li>迭代训练</li></ul><h2 id=21-nn>2.1 nn</h2><p><code>torch.nn</code> 主要包含 4 个模块</p><ul><li>nn.Parameter, Tensor 子类，表示可学习的参数，如 weights, bias</li><li>nn.Modules, 所有模型的基类，用于管理网络的属性</li><li>nn.functional, 函数具体实现，如 conv, pool, 激活函数</li><li>nn,init, 网络参数初始化方法</li></ul><h3 id=211-nnmodule>2.1.1 nn.Module</h3><p>class torch.nn.Module 是所有网络的基类（Base class for all neural network modules），每个模型都应该继承这个类，参考lab1的网络模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Net</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(Net, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>5</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>pool <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>) <span style=color:#75715e># run after each conv (hence the 5x5 FC layer)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>5</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>16</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>120</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>120</span>, <span style=color:#ae81ff>84</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc3 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>10</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x: torch<span style=color:#f92672>.</span>Tensor) <span style=color:#f92672>-&gt;</span> torch<span style=color:#f92672>.</span>Tensor:
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>pool(F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>conv1(x)))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>pool(F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>conv2(x)))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>16</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>5</span>)  
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>fc1(x)) <span style=color:#75715e>#输入是列向量</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>fc2(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc3(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>net <span style=color:#f92672>=</span> Net()<span style=color:#f92672>.</span>to(device)
</span></span></code></pre></div><p>一般在 model.py 文件中定义 NN model，再举一个 ViT 的例子</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ViT</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        name (str): Model name, e.g. &#39;B_16&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        pretrained (bool): Load pretrained weights
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        in_channels (int): Number of channels in input data
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        num_classes (int): Number of classes, default 1000
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    References:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        [1] https://openreview.net/forum?id=YicbFdNTTy
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(
</span></span><span style=display:flex><span>        self, 
</span></span><span style=display:flex><span>        name: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>        pretrained: bool <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>, 
</span></span><span style=display:flex><span>        patches: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>        dim: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>768</span>,
</span></span><span style=display:flex><span>        ff_dim: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>3072</span>,
</span></span><span style=display:flex><span>        num_heads: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>12</span>,
</span></span><span style=display:flex><span>        num_layers: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>12</span>,
</span></span><span style=display:flex><span>        attention_dropout_rate: float <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>,
</span></span><span style=display:flex><span>        dropout_rate: float <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>        representation_size: Optional[int] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        load_repr_layer: bool <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        classifier: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;token&#39;</span>,
</span></span><span style=display:flex><span>        positional_embedding: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;1d&#39;</span>,
</span></span><span style=display:flex><span>        in_channels: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>, 
</span></span><span style=display:flex><span>        image_size: Optional[int] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        num_classes: Optional[int] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span></code></pre></div><h4 id=2111-一些子函数>2.1.1.1 一些子函数</h4><p>2022-08-06 22:31:40</p><p>named_ 系列</p><p><strong>model.named_parameters()</strong>，返回两个变量，比如赋值给 name(e.g. <code>name</code> -> <code>stage_1.0.conv_b.weight</code>) 和 param (e.g. <code>param.requires_grad</code> -> <code>False</code>)</p><h3 id=212-nnlayer>2.1.2 nn.Layer</h3><p><strong>model.named_modules()</strong>，返回所有模块的迭代器。打印的话会输出模型的结构，如同 <code>print(model)</code></p><p><strong>model.named_children</strong>，named_modules 的子集，返回子模块的迭代器</p><p><strong>model.children()</strong>，返回下一级模块的迭代器</p><blockquote><p>所以这个只是访问到一级，如果下一级是一个 Sequential，那么就还得继续迭代，这个时候用 .modules() 可能会更好</p></blockquote><p><strong>model.modules()</strong>，Returns an iterator over all modules in the network</p><p>model.named_modules() 会有冗余的返回，这种情况下需要结合一些函数来过滤。</p><p>nn 还包含了很多 layer，比如 <code>nn.Conv2d</code>, <code>nn.MaxPool1d</code>, <code>nn.ReLU</code></p><h3 id=213-model-的创建>2.1.3 model 的创建</h3><p>主要是 2 个要素，构建子模块和拼接子模块，把子模块理解为 layer，构建子模块就是 <code>__init__</code>，拼接子模块就是 <code>forward()</code></p><ul><li>调用 <code>model = ViT(model_name, pretrained=True)</code> 创建模型时，会调用 <code>__init__()</code> 方法创建模型的子模块</li><li>训练时调用 <code>outputs = net(inputs)</code> 时，会进入 <code>module.py</code> 的 <code>call()</code> 函数中</li><li>在 <code>__call__</code> 中调用 <code>result = self.forward(*input, **kwargs)</code> 函数，进入到模型的 <code>forward()</code> 函数中，进行前向传播</li></ul><h4 id=2131-model-实例>2.1.3.1 model() 实例</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> quantize_model(model)
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> model(inputs)
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> criterion(outputs, targets)
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span></code></pre></div><p>在 <code>outputs = model(inputs)</code> 语句中会进入到 <code>class Conv2dQuantizer(nn.Module)</code> 的 forward 函数，<code>super(Conv2dQuantizer, self).__init__()</code> 是继承父类的构造函数 <code>__init__()</code>，从而使得 Conv2dQuantizer 中包含了父类</p><h4 id=2132-modeleval>2.1.3.2 model.eval()</h4><p>作用：不启动 BatchNormalization 和 Dropout，保证 BN 和 Dropout 不发生变化，pytorch 框架会自动把 BN 和 Dropout 固定住，不会取平均值，而是用训练好的值，不然的话，一旦 test 的 batch_size 过小，很容易就会被 BN 层导致生成图片颜色失真极大。</p><p>Reference: <a href=https://zhuanlan.zhihu.com/p/357075502>https://zhuanlan.zhihu.com/p/357075502</a></p><h4 id=2133-torchno_grad>2.1.3.3 torch.no_grad()</h4><p>tensor 有一个参数是 <code>requires_grad</code>，如果设置为 True，则反向传播时该 tensor 会自动求导，默认为 False，反向传播时不求导，可以极大地节约显存或者内存。</p><p><code>with torch.no_grad</code> 的作用：所有计算得出的 tensor 的 requires_grad 都自动设置为 False</p><h3 id=214-crossentropyloss>2.1.4 CrossEntropyLoss</h3><p>This criterion combines LogSoftmax and NLLLoss in one single class.</p><h2 id=22-tensor>2.2 Tensor</h2><p><code>Tensor</code> is a multi-dimensional matrix containing elements of a single data type</p><p>可以用 list 作为参数来构造 tensor</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1.</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.</span>], [<span style=color:#ae81ff>1.</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.</span>]])
</span></span><span style=display:flex><span>tensor([[ <span style=color:#ae81ff>1.0000</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0000</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>1.0000</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0000</span>]])
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> torch<span style=color:#f92672>.</span>tensor(np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>]]))
</span></span><span style=display:flex><span>tensor([[ <span style=color:#ae81ff>1</span>,  <span style=color:#ae81ff>2</span>,  <span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>4</span>,  <span style=color:#ae81ff>5</span>,  <span style=color:#ae81ff>6</span>]])
</span></span></code></pre></div><h2 id=23-autograd>2.3 autograd</h2><p>weight 更新依赖于梯度的计算，在 pytorch 中搭建好 forward 计算图，利用 <code>torch.autograd</code> 自动求导得到所有 gradient of tensor</p><h2 id=24-data-模块>2.4 data 模块</h2><p>数据模块可以细分为 4 个部分</p><ul><li>数据收集：样本，label</li><li>数据划分：train set, valid set, test set</li><li>数据读取：pytorch dataloader 模块，dataloader 包括 sampler, dataset<ul><li>sampler: 生成索引(index)</li><li>dataset: 根据生成的索引(index)读取样本以及标签(label)</li></ul></li><li>数据预处理：对应于 pytorch transforms</li></ul><h3 id=241-dataloader>2.4.1 DataLoader</h3><p><code>torch.utils.data.DataLoader()</code>, 构建可迭代的数据装载器</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>DataLoader(dataset, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, sampler<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, batch_sampler<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, collate_fn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, pin_memory<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, drop_last<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, worker_init_fn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, multiprocessing_context<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>)
</span></span></code></pre></div><ul><li>dataset: torchvision.datasets 类，决定数据从哪里读取，如何读取，以及是否下载，是否训练，给出一个例子</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cifar100_test <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>CIFAR100(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./data&#39;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, transform<span style=color:#f92672>=</span>transform_test)
</span></span></code></pre></div><ul><li>num_works: 是否多进程读取，指定读取的进程数量</li><li>shuffle: 每个 epoch 是否乱序</li><li>sampler: 指定一个 <code>torch.utils.data.distributed.DistributedSampler</code> 类型</li></ul><p><strong>其他名词</strong></p><ul><li>Epoch: 所有训练样本都已经输入到模型中，称为一个 epoch</li><li>Iteration: a batch of 样本已经输入到模型中</li><li>Batchsize: 批大小，决定一个 iteration 有多少样本，也决定了一个 Epoch 有多少个 Iteration</li></ul><h4 id=2411-nvidiadali>2.4.1.1 NVIDIA.DALI</h4><p>2022-07-28 21:24:17 了解到这玩意儿</p><h3 id=242-dataset>2.4.2 DataSet</h3><p><code>torch.utils.data.Dataset</code>, 抽象类，所有自定义大的 DataSet 都需要继承该类。</p><p>在Dataset 的初始化函数中会调用 <code>get_img_info()</code> 方法。</p><h3 id=242-torchvision>2.4.2 torchvision</h3><p>计算机视觉工具包，有 3 个主要的模块</p><ul><li><code>torchvision.transforms</code>, 包括常用的图像预处理方法</li><li><code>torchvision.datasets</code>, 包括常用的 dataset, e.g. MNIST, CIFAR-10, ImageNet</li><li><code>torchvision.models</code>, 常用的 pre-trained models, e.g. AlexNet, VGG, ResNet, GoogleNet</li></ul><p>data 的数量和分布对模型训练的结果起决定性的作用，需要对 data 进行 pre-process 和数据增强。目的是增加数据的多样性，提高模型的泛化能力。</p><h3 id=243-batch-size>2.4.3 Batch Size</h3><p>2022-08-09 18:24:56，学习一下梯度，训练，和 batch size 的关系。</p><p>通过举例来学习，比如目前使用的网络，batch_size = 128，训练数据行数为 $|x| = 1024$，代表每次网络模型的迭代使用了 128 个样本，128 个样本来自 $x$，可能是无序抽样，也可能是有序抽样。</p><p>每个 epoch 包含 1024/128 iterations</p><blockquote><p>同一个 epoch，我用第一个 batch 完成了一次前向反向，接下来的第二次 iteration，换了一个 batch，但是 weight 已经更新过了
这个就是之前卡住我的点，要理解这一点。训练是为了让 weight 收敛。</p></blockquote><h4 id=2431-epoch-bach-iteration>2.4.3.1 epoch, bach, iteration</h4><p>epoch: 把所有训练数据丢进网络的周期</p><p>batch_size: 一次迭代的数据量；这个从一些说法中，看起来是图片的张数</p><p>iteration: 完成所有训练数据的迭代，所需要的次数。</p><blockquote><p>batch_size = 128，训练数据行数为 $|x| = 1024$，代表每次网络模型的迭代使用了 128 个样本，128 个样本来自 $x$，可能是无序抽样，也可能是有序抽样。
每个 epoch 包含 1024/128 iterations</p></blockquote><p>⭐ 注意，第一个 epoch 结束之后，weight 是没有 reset 的，也就是说第二个 epoch 仍然接着更新 weight。</p><blockquote><p>epoch，背诵词典次数多了，就记牢了。当然，也有可能背傻了（过拟合）</p></blockquote><h2 id=25-模型训练>2.5 模型训练</h2><h3 id=251-损失函数>2.5.1 损失函数</h3><p>Loss Function, 衡量模型输出与真实标签之间的差异，也就是 <strong>一个</strong> 样本的 output 和真实标签（label）的差异</p><p>Cost Function, 计算整个样本集的 output 和真实标签（label）的差异</p><p>pytorch 中的损失函数也是继承于 <code>nn.Module</code></p><h3 id=252-optimizer>2.5.2 optimizer</h3><p>PyTorch 中的优化器是用于管理并更新模型中 <strong>可学习参数的值</strong>，使得模型输出更加接近真实标签。</p><h4 id=2521-属性>2.5.2.1 属性</h4><ul><li>defaults: 优化器的超参数，如 weight_decay, momentum</li><li>state: 参数的缓存，如 momentum 中需要用到前几次的梯度，缓存在这个变量中</li><li>param_groups: 管理的参数组，是一个 list，其中每个元素是 dict，包括 momentum, lr, weight_decay, params</li><li>_step_count: 记录更新次数，在学习率调整中使用</li></ul><h4 id=2522-optimizer-方法>2.5.2.2 optimizer 方法</h4><ul><li>zero_grad(): 清空所管理参数的梯度。因为 pytorch 张量的梯度不会自动清零，因此每次反向传播之后都需要清空梯度</li><li>step(): 执行一步梯度更新</li><li>add_param_group(): 添加参数组</li><li>state_dict(): 获取优化器当前状态</li><li>load_state_dict(): 加载状态信息的 dict</li></ul><h4 id=2523-learning-rate>2.5.2.3 learning rate</h4><p>learning rate, 影响 loss function 收敛的重要因素，控制了梯度下降更新的步伐</p><h2 id=26-regularization-正则化>2.6 Regularization 正则化</h2><p>正则化是一种减少方差的策略</p><h3 id=261-weight-decay>2.6.1 weight decay</h3><p>weight decay 是优化器中的一个参数，在执行 <code>optim_wdecay_step()</code> 时，会计算 weight decay 后的梯度</p><h3 id=262-dropout>2.6.2 Dropout</h3><p>一种抑制过拟合的方法。理解为放缩数据</p><h3 id=263-normalization>2.6.3 Normalization</h3><p>Batch Normalization, 经过 normalization 后的数据服从 $N(0, 1)$ 分布，有如下优点</p><ul><li>可以使用更大的 lr，加速模型收敛</li><li>可以不用精心设计 weight 初始化</li><li>可以不用 dropout 或者较小的 dropout</li><li>可以不用 L2 或者较小的 weight decay</li><li>可以不用 LRN (Local Response Normalization)</li></ul><h2 id=27-model-相关的操作>2.7 Model 相关的操作</h2><h3 id=271-torchsave>2.7.1 torch.save</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>save(obj, f, pickle_module, pickle_protocol<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, _use_new_zipfile_serialization<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p>obj 是保存的对象，f 是输出路径。还有 2 种方式</p><ul><li>保存整个 Module, <code>torch.savev(net, path)</code> 这种方法比较耗时，保存的文件比较大</li><li>只保存模型的参数，<code>torch.savev(state_sict, path)</code>，推荐，保存的文件比较小</li></ul><h3 id=272-torchload>2.7.2 torch.load</h3><p>对应于 save</p><h3 id=273-fine-tuning>2.7.3 Fine-tuning</h3><p>一种迁移学习的方法，比如在人脸识别应用中，ImageNet 作为 source domain，人脸数据作为 target domain。通常 source domain 比 target domain 大很多，可以利用 ImageNet 训练好的网络应用到人脸识别中。</p><p><strong>理解</strong>
对于一个模型，可以分为流程在前面的 feature extractor （conv 层） 和后面的 classifier。fine-tune 通常不改变 feature extractor 的 weight，也就是冻结 conv 层；改变最后一个 fc layer 的输出来适应目标任务，训练后面 classifier 的 weight。</p><p>通常 target domain 的数据比较小，不足以训练全部参数，容易导致过拟合，因此不改变 feature extractor 的 weight。</p><p><strong>Step</strong></p><ul><li>获取 pre-trained model 参数</li><li><code>load_state_dict()</code> 把参数加载到模型中</li><li>修改输出层</li><li>固定 feature extractor 的参数，通常有 2 种做法<ul><li>固定 conv 层的预训练参数。可以设置 <code>requires_grad = False</code> 或者 <code>lr = 0</code></li><li>通过 <code>params_group</code> 给 feature extractor 设置一个较小的 lr</li></ul></li></ul><h1 id=3-pytorch-分布式训练>3. pytorch 分布式训练</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist<span style=color:#f92672>.</span>init_process_group(backend<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nccl&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># backend是后台利用nccl进行通信</span>
</span></span></code></pre></div><p>调试时报错，如何在调试分布式训练的模型</p><h2 id=31-debug-分布式训练的模型>3.1 debug 分布式训练的模型</h2><p>修改 python <code>launch.json</code> 文件，把 program 换成 torch.distribution 的 launch.py</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>//</span> Use IntelliSense to learn about possible attributes<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>//</span> Hover to view descriptions of existing attributes<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>//</span> For more information, visit: https:<span style=color:#f92672>//</span>go<span style=color:#f92672>.</span>microsoft<span style=color:#f92672>.</span>com<span style=color:#f92672>/</span>fwlink<span style=color:#f92672>/</span><span style=color:#960050;background-color:#1e0010>?</span>linkid<span style=color:#f92672>=</span><span style=color:#ae81ff>830387</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;version&#34;</span>: <span style=color:#e6db74>&#34;0.2.0&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;configurations&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;Python: Current File&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;request&#34;</span>: <span style=color:#e6db74>&#34;launch&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;program&#34;</span>: <span style=color:#e6db74>&#34;/nvme/wmhu/anaconda3/envs/ant/lib/python3.8/site-packages/torch/distributed/launch.py&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;console&#34;</span>: <span style=color:#e6db74>&#34;integratedTerminal&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;justMyCode&#34;</span>: true,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;args&#34;</span>: [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--nproc_per_node=1&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;/nvme/wmhu/work/ANT/ImageNet/main.py&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--dataset=imagenet&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--model=vit_b_16&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--dataset_path=/nvme/imagenet&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--epoch=4&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--mode=int&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--wbit=4&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--abit=4&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;env&#34;</span>:{<span style=color:#e6db74>&#34;CUDA_VISIBLE_DIVICES&#34;</span>:<span style=color:#e6db74>&#34;0&#34;</span>},
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>注意 <code>nproc_per_node</code> 是 Python 自带的参数，因此可以写到里面，对于 <code>--dataset=imagenet</code> 会有报错。</p><p><code>--dataset=imagenet</code> 要放到 <code>main.py</code> 的后面，这个方法相当于用调整参数的形式来达到目的。不太有通用性，相当于专门改了一个 <code>launch.json</code> 文件。</p><h1 id=4-hook>4. Hook</h1><p>这个功能被广泛用于可视化神经网络中间层的 feature、gradient，从而诊断神经网络中可能出现的问题，分析网络有效性。</p><p>视频：https://www.youtube.com/watch?v=syLFCVYua6Q</p><p>pytorch 计算图似乎只会保留叶子节点的梯度，舍弃中间的梯度</p><blockquote><p>简而言之，register_hook的作用是，反向传播时，除了完成原有的反传，额外多完成一些任务。你可以定义一个中间变量的hook，将它的grad值打印出来，当然你也可以定义一个全局列表，将每次的grad值添加到里面去。</p></blockquote><p>什么是中间变量：有的博客里有提到，似乎是没有直接指定数值，而是通过计算得到的变量。比如下面的 z 就是中间变量。z 的梯度是不会保存的</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Tensor([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>])<span style=color:#f92672>.</span>requires_grad_()
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Tensor([<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>])<span style=color:#f92672>.</span>requires_grad_()
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> y       <span style=color:#75715e># 中间变量</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>output <span style=color:#f92672>=</span> model(input)
</span></span><span style=display:flex><span><span style=color:#75715e># 此时会做几件事，一个是调用 forward 方法计算结果，一个是判断有没有注册 forward_hook，有的话就将 forward 的输入及结果作为 hook 的实参</span>
</span></span></code></pre></div><h2 id=41-register_hook>4.1 register_hook</h2><p><code>z.register_hook(hook_fn)</code>，这个 hook_fn 是一个用户自定义函数，返回 Tensor （如果需要对 grad 进行修改）或者 None（用于直接打印，不修改），所以直接用 lambda 函数即可，<code>z.register_hook(lambda grad: print(grad))</code></p><blockquote><p>个人理解下来，register_hook 可以实现保留中间变量梯度的功能，而且不像 <code>retain_grad</code> 那样会带来很大的开销</p></blockquote><h3 id=411-register_forward_hook>4.1.1 register_forward_hook</h3><p><strong>作用</strong>：获取中间层的 feature map</p><p>通常，pytorch 只提供了网络整体的输入和输出，对于夹在网络中间的模块，很难获得他的输入输出。除非设计网络时，在 forward 函数的返回值中包含中间 module 的输出。总而言之别的方法都比较麻烦，pytorch 设计好了 register_forward_hook 和 register_backward_hook。</p><p>相比针对 tensor 的 register_hook，这个 forward hook 没有返回值，也就是不能改变输入，只能打印。</p><p>代码实例，讲的非常清晰，来自<a href=https://cloud.tencent.com/developer/article/1475430>博客</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Class Model(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 全局变量，用于存储中间层的 feature</span>
</span></span><span style=display:flex><span>total_feature_out <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>total_feature_in <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Model()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义 forward hook function</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>hook_fn_forward</span>(module, input, output):
</span></span><span style=display:flex><span>    print(module) <span style=color:#75715e># 用于区分模块</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;input&#39;</span>, input) <span style=color:#75715e># 首先打印出来</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;output&#39;</span>, output)
</span></span><span style=display:flex><span>    total_feature_out<span style=color:#f92672>.</span>append(output) <span style=color:#75715e># 然后分别存入全局 list 中</span>
</span></span><span style=display:flex><span>    total_feature_in<span style=color:#f92672>.</span>append(input)
</span></span><span style=display:flex><span><span style=color:#75715e># 给每个 module 都装上 hook</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, module <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>named_children():
</span></span><span style=display:flex><span>    module<span style=color:#f92672>.</span>register_forward_hook(hook_fn_forward)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 前向传播和回传</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Tensor([[<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>]])<span style=color:#f92672>.</span>requires_grad_() 
</span></span><span style=display:flex><span>o <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>o<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;==========Saved inputs and outputs==========&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(len(total_feature_in)):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;input: &#39;</span>, total_feature_out[idx])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;output: &#39;</span>, total_feature_out[idx])
</span></span></code></pre></div><h3 id=412-register_backward_hook>4.1.2 register_backward_hook()</h3><h4 id=4121-使用方法和示例>4.1.2.1 使用方法和示例</h4><p><strong>作用</strong>：用于获取梯度</p><p><strong>使用</strong>：<code>module.register_backward_hook(hook_fn)</code>, <code>hook_fn(module, grad_input, grad_output) -> Tensor or None</code></p><p>如果有多个输入或者输出，grad_input, grad_output 可以是 tuple 类型。比如对于线性模块，grad_input 是一个三元组，分别是 $g_{bias}$, $g_x$, $g_W$，对 bias 的导数，对 x 的导数以及对 weight 的导数。</p><p>直接看代码，这个代码是可以直接跑的，不得不说写的确实很好。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Model</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(Model, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>initialize()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>initialize</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>fc1<span style=color:#f92672>.</span>weight <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(
</span></span><span style=display:flex><span>                torch<span style=color:#f92672>.</span>Tensor([[<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>2.</span>, <span style=color:#ae81ff>3.</span>],
</span></span><span style=display:flex><span>                              [<span style=color:#f92672>-</span><span style=color:#ae81ff>4.</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>5.</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>6.</span>],
</span></span><span style=display:flex><span>                              [<span style=color:#ae81ff>7.</span>, <span style=color:#ae81ff>8.</span>, <span style=color:#ae81ff>9.</span>],
</span></span><span style=display:flex><span>                              [<span style=color:#f92672>-</span><span style=color:#ae81ff>10.</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>11.</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>12.</span>]]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>fc1<span style=color:#f92672>.</span>bias <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>Tensor([<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>, <span style=color:#ae81ff>4.0</span>]))
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>fc2<span style=color:#f92672>.</span>weight <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>Tensor([[<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>, <span style=color:#ae81ff>4.0</span>]]))
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>fc2<span style=color:#f92672>.</span>bias <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>Tensor([<span style=color:#ae81ff>1.0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        o <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc1(x)
</span></span><span style=display:flex><span>        o <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu1(o)
</span></span><span style=display:flex><span>        o <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc2(o)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> o
</span></span><span style=display:flex><span><span style=color:#75715e># 全局变量，用于存储中间层的 feature</span>
</span></span><span style=display:flex><span>total_grad_out <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>total_grad_in <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Model()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义 forward hook function</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>hook_fn_backward</span>(module, grad_input, grad_output):
</span></span><span style=display:flex><span>    print(module) <span style=color:#75715e># 用于区分模块</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 为了符合反向传播顺序，先打印 grad_output</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;grad_output&#39;</span>, grad_output)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;grad_input&#39;</span>, grad_input) 
</span></span><span style=display:flex><span>    total_grad_out<span style=color:#f92672>.</span>append(grad_output) 
</span></span><span style=display:flex><span>    total_grad_in<span style=color:#f92672>.</span>append(grad_input)
</span></span><span style=display:flex><span><span style=color:#75715e># 给每个 module 都装上 hook</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, module <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>named_children():
</span></span><span style=display:flex><span>    module<span style=color:#f92672>.</span>register_backward_hook(hook_fn_backward)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 前向传播和回传</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 这里的 requires_grad 很重要，如果不加，backward hook</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 执行到第一层，对 x 的导数将为 None，某英文博客作者这里疏忽了</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 此外再强调一遍 x 的维度，一定不能写成 torch.Tensor([1.0, 1.0, 1.0]).requires_grad_()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 否则 backward hook 会出问题。</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Tensor([[<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>]])<span style=color:#f92672>.</span>requires_grad_()
</span></span><span style=display:flex><span>o <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>o<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;==========Saved inputs and outputs==========&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(len(total_grad_in)):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;input: &#39;</span>, total_grad_in[idx])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;output: &#39;</span>, total_grad_out[idx])
</span></span></code></pre></div><p><strong>注意</strong>，作者提到“register_backward_hook只能操作简单模块，而不能操作包含多个子模块的复杂模块。如果对复杂模块用了 backward hook，那么我们只能得到该模块最后一次简单操作的梯度信息。”不太确定什么是简单模块，不太确定诸如 resnet18 这样的网络是不是简单模块。</p><p>2022-08-06 23:14:22，这个地方应该是想说，用 for loop 遍历 model.named_children() 是有必要的，否则直接 <code>model = Model()model.register_backward_hook(hook_fn_backward)</code> 会有问题。</p><h4 id=4122-注意事项>4.1.2.2 注意事项</h4><p><strong>形状</strong></p><ul><li><p>在卷积层中，weight 的梯度和 weight 的形状相同</p></li><li><p>在全连接层中，weight 的梯度的形状是 weight 形状的转秩（观察上文中代码的输出可以验证）</p></li></ul><p><strong>grad_input tuple 中各梯度的顺序</strong></p><ul><li><p>在卷积层中，bias 的梯度位于tuple 的末尾：grad_input = (对feature的导数，对权重 W 的导数，对 bias 的导数)</p></li><li><p>在全连接层中，bias 的梯度位于 tuple 的开头：grad_input=(对 bias 的导数，对 feature 的导数，对 W 的导数)</p></li></ul><p><strong>当 batchsize > 1 时，对 bias 的梯度处理不同</strong></p><ul><li><p>在卷积层，对 bias 的梯度为整个 batch 的数据在 bias 上的梯度之和：grad_input = (对feature的导数，对权重 W 的导数，对 bias 的导数)</p></li><li><p>在全连接层，对 bias 的梯度是分开的，bach 中每条数据，对应一个 bias 的梯度：grad_input = ((data1 对 bias 的导数，data2 对 bias 的导数 &mldr;)，对 feature 的导数，对 W 的导数)</p></li></ul><h1 id=misc>Misc</h1><p>@classmethod 用法</p><p>想给初始类再新添功能，不需要改初始类，只要在下一个类内部新写一个方法，方法用@classmethod装饰一下即可。、</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@classmethod</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>convert_sync_batchnorm</span>(cls, module, process_group<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span></code></pre></div><h1 id=问题>问题</h1><ul><li>Debug 的时候怎么看 tensor 变量的参数？</li></ul><h1 id=reference>Reference</h1><p><a href=https://zhuanlan.zhihu.com/p/265394674>https://zhuanlan.zhihu.com/p/265394674</a></p><p><a href=https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/>https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/</a> pytorch 中文文档</p><p><a href=https://cloud.tencent.com/developer/article/1475430#>https://cloud.tencent.com/developer/article/1475430#</a></p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Cory</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-08-11</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=https://huweim.github.io/tags/pytorch/>PyTorch</a></div><nav class=post-nav><a class=prev href=/post/blog_hugo_%E6%96%87%E7%AB%A0%E9%A1%B5%E9%9D%A2%E6%B7%BB%E5%8A%A0%E5%9B%BA%E5%AE%9A%E7%9B%AE%E5%BD%95/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">Hugo 文章页面添加固定目录栏</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/post/%E5%8D%9A%E5%AE%A2_%E5%9B%BD%E5%86%85%E5%8D%9A%E5%A3%AB%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E8%87%AA%E5%B7%B1%E7%9A%84%E8%8B%B1%E8%AF%AD/><span class="next-text nav-default">国内硕博如何提升自己的英语</span>
<span class="prev-text nav-mobile">Next</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:huwm1@shanghaitech.edu.cn rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408h399.992405s71.046998 3.997201 71.046998 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zm53.281707 130.131124C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523S0 1024 83.726336 1024H682.532949 753.579947h595.368192C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955c-39.771477 34.470494-74.671786 43.295855-98.955861 43.868928z"/></svg></a><a href=http://localhost:1313 rel="me noopener" class=iconfont title=linkedin target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="33" height="33"><path d="M872.405333 872.618667H720.768V635.008c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333.0-91.136 61.653333-91.136 125.397334v241.792H398.976V384H544.64v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667.0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667.0 01-88.021333-88.106666A88.064 88.064.0 11227.712 317.141333zm76.032 555.477334H151.68V384h152.064v488.618667zM948.266667.0h-872.704C33.792.0.0 33.024.0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667.0 948.138667.0h.128z"/></svg></a><a href=https://github.com/huweim rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04C242.005334 929.664 211.968 830.08 211.968 830.08 188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://www.zhihu.com/people/hu-wei-ming-31-86 rel="me noopener" class=iconfont title=zhihu target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M351.791182 562.469462h192.945407c0-45.367257-21.3871-71.939449-21.3871-71.939449L355.897709 490.530013c3.977591-82.182744 7.541767-187.659007 8.816806-226.835262h159.282726s-.86367-67.402109-18.578124-67.402109-279.979646.0-279.979646.0 16.850783-88.141456 39.318494-127.053698c0 0-83.60514-4.510734-112.121614 106.962104S81.344656 355.077018 76.80834 367.390461s24.62791 5.832845 36.941354.0c12.313443-5.832845 68.050885-25.924439 84.252893-103.69571h86.570681c1.165546 49.28652 4.596691 200.335724 3.515057 226.835262H109.86113c-25.275663 18.147312-33.701566 71.939449-33.701566 71.939449H279.868105c-8.497535 56.255235-23.417339 128.763642-44.275389 167.210279-33.05279 60.921511-50.55235 116.65793-169.802314 212.576513.0.0-19.442818 14.257725 40.829917 9.073656 60.273758-5.185093 117.305683-20.739347 156.840094-99.807147 20.553105-41.107233 41.805128-93.250824 58.386782-146.138358l-.055259.185218 167.855986 193.263655s22.035876-51.847855 5.832845-108.880803L371.045711 650.610918l-42.1244 31.157627-.045025.151449c11.69946-41.020252 20.11206-81.5749 22.726607-116.858498C351.665315 564.212152 351.72876 563.345412 351.791182 562.469462z"/><path d="M584.918753 182.033893v668.840094h70.318532l28.807093 80.512708 121.875768-80.512708h153.600307L959.520453 182.033893h-374.6017zM887.150192 778.934538h-79.837326l-99.578949 65.782216-23.537066-65.782216h-24.855084L659.341766 256.673847h227.807403V778.934538z"/></svg></a><a href=https://huweim.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667v-199.04c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2020 -
2023
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>Cory</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script>
<script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script>
<script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/load-photoswipe.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>