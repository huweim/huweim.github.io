<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>GPGPU-Sim中的CTA & warp scheduling - Cory Code</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="GPGPU-Sim中的CTA & warp scheduling"><meta itemprop=description content="CTA Scheduling CTA/Thread Block/Work Group
调度发生在 shader_core_ctx::issue_block2core(...)，shader_core_config::max_cta(...) 计算 core 中能容纳的 max CTA。这个取决于各硬件资源的短板，在报告中能看到是被什么限制了。
printf(&#34;GPGPU-Sim uArch: CTA/core = %u, limited by:&#34;, result); if (result == result_thread) printf(&#34; threads&#34;); if (result == result_shmem) printf(&#34; shmem&#34;); if (result == result_regs) printf(&#34; regs&#34;); if (result == result_cta) printf(&#34; cta_limit&#34;); 在矩阵乘法代码中 create, verify are both limited by thread, multiple is limited by regs.
When each thread finishes, the SIMT core calls register_cta_thread_exit(...) to update the active thread block&rsquo;s state."><meta itemprop=datePublished content="2021-11-14T20:42:52+08:00"><meta itemprop=dateModified content="2021-11-14T20:42:52+08:00"><meta itemprop=wordCount content="650"><meta itemprop=keywords content="gpgpu-sim,warp,"><meta property="og:title" content="GPGPU-Sim中的CTA & warp scheduling"><meta property="og:description" content="CTA Scheduling CTA/Thread Block/Work Group
调度发生在 shader_core_ctx::issue_block2core(...)，shader_core_config::max_cta(...) 计算 core 中能容纳的 max CTA。这个取决于各硬件资源的短板，在报告中能看到是被什么限制了。
printf(&#34;GPGPU-Sim uArch: CTA/core = %u, limited by:&#34;, result); if (result == result_thread) printf(&#34; threads&#34;); if (result == result_shmem) printf(&#34; shmem&#34;); if (result == result_regs) printf(&#34; regs&#34;); if (result == result_cta) printf(&#34; cta_limit&#34;); 在矩阵乘法代码中 create, verify are both limited by thread, multiple is limited by regs.
When each thread finishes, the SIMT core calls register_cta_thread_exit(...) to update the active thread block&rsquo;s state."><meta property="og:type" content="article"><meta property="og:url" content="https://huweim.github.io/posts/%E7%BC%96%E7%A8%8B_gpgpu-sim%E4%B8%AD%E7%9A%84cta-warp-scheduling/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-14T20:42:52+08:00"><meta property="article:modified_time" content="2021-11-14T20:42:52+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GPGPU-Sim中的CTA & warp scheduling"><meta name=twitter:description content="CTA Scheduling CTA/Thread Block/Work Group
调度发生在 shader_core_ctx::issue_block2core(...)，shader_core_config::max_cta(...) 计算 core 中能容纳的 max CTA。这个取决于各硬件资源的短板，在报告中能看到是被什么限制了。
printf(&#34;GPGPU-Sim uArch: CTA/core = %u, limited by:&#34;, result); if (result == result_thread) printf(&#34; threads&#34;); if (result == result_shmem) printf(&#34; shmem&#34;); if (result == result_regs) printf(&#34; regs&#34;); if (result == result_cta) printf(&#34; cta_limit&#34;); 在矩阵乘法代码中 create, verify are both limited by thread, multiple is limited by regs.
When each thread finishes, the SIMT core calls register_cta_thread_exit(...) to update the active thread block&rsquo;s state."><link href="https://fonts.googleapis.com/css?family=Playfair+Display:700" rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://huweim.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://huweim.github.io/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://huweim.github.io/css/dark.css><script src=https://huweim.github.io/js/feather.min.js></script>
<script src=https://huweim.github.io/js/main.js></script></head><body><div class="container wrapper"><div class=header><h1 class=site-title><a href=https://huweim.github.io/>Cory Code</a></h1><div class=site-description><p>Minimal and Clean <a href=https://github.com/athul/hugo-ink>blog theme for Hugo</a></p><nav class="nav social"><ul class=flat><li><a href=https://github.com/huweim title=GitHub><i data-feather=github></i></a></li><li><a href=https://twitter.com/athulcajay/ title=Twitter><i data-feather=twitter></i></a></li><li><a href=https://gitlab.com/athul/ title=GitLab><i data-feather=gitlab></i></a></li><li><a href=# class=scheme-toggle id=scheme-toggle></a></li></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>All posts</a></li><li><a href=/categories/>Categories</a></li><li><a href=/archives>Archives</a></li><li><a href=/tags>Tags</a></li><li><a href=/about/>About</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>14</span>
<span class=rest>Nov 2021</span></div></div><div class=matter><h1 class=title>GPGPU-Sim中的CTA & warp scheduling</h1></div></div><div class=markdown><h1 id=cta-scheduling>CTA Scheduling</h1><p>CTA/Thread Block/Work Group</p><p>调度发生在 <code>shader_core_ctx::issue_block2core(...)</code>，<code>shader_core_config::max_cta(...)</code> 计算 core 中能容纳的 max CTA。这个取决于各硬件资源的短板，在报告中能看到是被什么限制了。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>printf</span><span class=p>(</span><span class=s>&#34;GPGPU-Sim uArch: CTA/core = %u, limited by:&#34;</span><span class=p>,</span> <span class=n>result</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>result</span> <span class=o>==</span> <span class=n>result_thread</span><span class=p>)</span> <span class=n>printf</span><span class=p>(</span><span class=s>&#34; threads&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>result</span> <span class=o>==</span> <span class=n>result_shmem</span><span class=p>)</span> <span class=n>printf</span><span class=p>(</span><span class=s>&#34; shmem&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>result</span> <span class=o>==</span> <span class=n>result_regs</span><span class=p>)</span> <span class=n>printf</span><span class=p>(</span><span class=s>&#34; regs&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>result</span> <span class=o>==</span> <span class=n>result_cta</span><span class=p>)</span> <span class=n>printf</span><span class=p>(</span><span class=s>&#34; cta_limit&#34;</span><span class=p>);</span>
</span></span></code></pre></div><p>在矩阵乘法代码中 create, verify are both limited by thread, multiple is limited by regs.</p><p>When each thread finishes, the SIMT core calls <code>register_cta_thread_exit(...)</code> to update the active thread block&rsquo;s state. CTA 中的所有线程执行完毕后，active CTA 数量减一，允许新的 CTA 在下个周期被调度。</p><h1 id=warp-scheduling>Warp Scheduling</h1><h2 id=1-from-code>1. From Code</h2><ul><li>A new front-end that models instruction caches and separates the <strong>warp scheduling (issue) stage</strong> from the fetch and decode stage</li></ul><h5 id=flow>Flow</h5><p><code>shader_core_ctx::issue()</code> call <code>scheduler_unit::cycle()</code>。</p><p>In function <code>scheduler_unit::cycle()</code> , call <code>order_warps()</code> to sort warps according to their priority.</p><p>排序后的 warp 放在 vector <code>m_next_cycle_prioritized_warps</code> 中，对其进行遍历来处理这个 vector 中的 warp。</p><p>进入 for 循环，拿到 warp id，判断</p><ul><li>I-Buffer 是否为空；是否处于 waiting 状态。如果都通过，进入一个 while 循环<ul><li>如果指令是有效的 <code>if(pI)</code><ul><li>如果出现分支 <code>if(pc != pI->pc)</code>，刷掉 I-Buffer</li><li>如果没有分支，此时 <code>valid=true</code>，指令是有效的。如果通过 scoreboard 检测，终于可以执行了。先读取 active mask 确定要执行哪些线程，然后判断 <code>pI->op</code> 是 内存操作 还是 运算操作。如果相应的寄存器可以使用 <code>has_free()</code>，则 call <code>issue_warp()</code> 将寄存器、指令、active mask、warp id、scheduler id 发送并执行。</li><li><code>warp_inst_issued = true;</code></li></ul></li><li>else if 下一条指令是有效的<ul><li>&mldr;</li></ul></li><li>如果指令成功发射 <code>if (warp_inst_issued)</code><ul><li>call <code>do_on_warp_issued(warp_id, issued, iter);</code></li></ul></li><li>checked++</li></ul></li></ul><h5 id=scheduler_size>scheduler_size()</h5><p>scheduler.size 就是2，代表一个 core 中 warp scheduler 的数量</p><h5 id=lrr-特征>lrr 特征</h5><ul><li>单双数 round-robin 1-3-5-7-9, 2-4-6-8-10</li></ul><p>Why? there are two schedulers per SM, an even and odd scheduler that concurrently execute even and odd warps.</p><h5 id=gto-特征>gto 特征</h5><p>贪心，没有 stall 的话会看到连续的 warp id 相同</p><p>OP: 8 (LOAD) 后面可能会接一个相同 warp id 的 OP: 1 (ALU_OP)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>warp id:24 OP:6	<span class=c1>#INTP_OP</span>
</span></span><span class=line><span class=cl>warp id:24 OP:8
</span></span><span class=line><span class=cl>warp id:24 OP:1
</span></span></code></pre></div><h3 id=12-构造-scheduler>1.2 构造 scheduler</h3><p>根据 warp scheduling policy 构造 scheduler。config 文件传入调度策略，<code>shader_core_ctx::create_schedulers()</code> 根据传入的字符构造对应的 scheduler 类。</p><h4 id=131-class-scheduler_unit>1.3.1 class scheduler_unit</h4><h5 id=i-stdvector-shd_warp_t--m_next_cycle_prioritized_warps>I. std::vector&lt; shd_warp_t* > m_next_cycle_prioritized_warps</h5><p>This is the prioritized warp list that is looped over each cycle to determine which warp gets to issue.</p><p>作为 order 函数的 result_list。每个 cycle 遍历这个 list，决定发射哪个 warp。已经按调度策略/优先级排序。size 为 24</p><h5 id=ii-stdvector-shd_warp_t--m_supervised_warps>II. std::vector&lt; shd_warp_t* > m_supervised_warps;</h5><p>The m_supervised_warps list is <strong>all the warps</strong> this scheduler is supposed to arbitrate between. This is useful in systems where there is more than one warp scheduler. In a single scheduler system, this is simply all the warps assigned to this core.</p><p>这个作为 order 函数的 input_list，里面装了所有待仲裁的 warp。size 为 24</p><p>在构造 scheduler 时，通过函数 <code>add_supervised_war_id(i)</code> 向 m_supervised_warps 添加 warp。warp 0 -> scheduler 0, warp 1 -> scheduler 1, warp 2 -> scheduler 0&mldr;</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>unsigned</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>m_warp</span><span class=p>.</span><span class=n>size</span><span class=p>();</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>//distribute i&#39;s evenly though schedulers;
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>schedulers</span><span class=p>[</span><span class=n>i</span><span class=o>%</span><span class=n>m_config</span><span class=o>-&gt;</span><span class=n>gpgpu_num_sched_per_core</span><span class=p>]</span><span class=o>-&gt;</span><span class=n>add_supervised_warp_id</span><span class=p>(</span><span class=n>i</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><h5 id=iii-stdvector-shd_warp_t-const_iterator-m_last_supervised_issued>III. std::vector&lt; shd_warp_t* >::const_iterator m_last_supervised_issued;</h5><p>记录上一个被发射的 warp</p><h2 id=2-from-paper>2. From Paper</h2><h3 id=21-warp-scheduler>2.1 Warp Scheduler</h3><p>The warp scheduler connects to the back end of the SIMT processor and is responsible for issuing instructions.</p><blockquote><p>warp scheduler 是 front-end and back-end 之间的连接</p></blockquote><p>Because the registers for each warp are independent, saving and restoring warp states are not needed.</p><blockquote><p>warp 有自己独享的寄存器，所以无需专门保存 warp state</p></blockquote><p>The warp scheduler only needs to keep track of those warps whose instructions are ready for issue.</p><blockquote><p>在 ready intructions 选一个来发射</p></blockquote><p>If none of the warps are ready for issue or the back-end of the SIMT processor does not have enough free space, the warps are <strong>stalled</strong> and no instruction is sent to the back-end of the SIMT processor.</p><blockquote><p>可以在 Output 中观察到，LRR and GTO 的 stall cycle 是不同的</p></blockquote><h3 id=22-functional-units>2.2 Functional Units</h3><p>The memory instructions and arithmetic instructions operate in different pipelines.</p><p>Each back-end pipeline owns a set of dedicated collector units shared in a pool. The data from the collector units are sent to the functional units including arithmetic logic units (ALU) and load-store units (LSU).</p><p>When an instruction from a warp occupies the arithmetic logic units, the other warps may use the load-store units to enhance instruction level parallelism and hide the latency.</p><blockquote><p>一个 warp 使用 ALU，其他的 warp 指令仍然可以使用 LSU。以此来隐藏延迟，增加并行度</p></blockquote><p>As for the load-store units, the coalescer combines the memory requests within a warp, which allows fewer requests to be sent to the L1 cache memory. There are also miss status holding registers (MSHR) supporting multiple outstanding requests to external memory, which enhances the memory level parallelism to hide the latency.</p><blockquote><p>coalescing access, MSHR</p></blockquote></div><div class=tags><ul class=flat><li><a href=/tags/gpgpu-sim>gpgpu-sim</a></li><li><a href=/tags/warp>warp</a></li></ul></div></div></div><div class="footer wrapper"><nav class=nav><div>2022 © Athul | <a href=https://github.com/knadh/hugo-ink>Ink</a> theme on <a href=https://gohugo.io>Hugo</a></div></nav></div><script>feather.replace()</script></body></html>