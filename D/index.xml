<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cory Code</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Cory Code</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Athul</copyright>
    <lastBuildDate>Tue, 28 Sep 2021 13:39:14 +0800</lastBuildDate>
    
	<atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Thu, 08 Jul 2021 17:46:31 +0800</pubDate>
      
      <guid>http://localhost:1313/about/</guid>
      <description>一个计算机体系结构方向的CS学生，喜欢体系结构，喜欢编程，喜欢篮球。目前的课题主要关注 GPU/GPGPU。
不确定自己是否喜欢折腾，是否喜欢新鲜事物，是否擅长编程。希望在一步一步探索中找到自己真正感兴趣的事情，或者确定这就是自己真正感兴趣的事情。</description>
    </item>
    
    <item>
      <title>GPGPU-Sim 运行机制</title>
      <link>http://localhost:1313/posts/%E7%BC%96%E7%A8%8B_gpgpu-sim-%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Tue, 28 Sep 2021 13:39:14 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/%E7%BC%96%E7%A8%8B_gpgpu-sim-%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6/</guid>
      <description>0. 前言 在 GPGPU-Sim 跑一些比较大的 benchmark, 或者想要同时跑很多组 benchmark 的时候，在自己的电脑上跑，或者在虚拟机上运行的话速度肯定达不到要求，会成为工作中瓶颈。因此了解一下如何在服务器上跑 simulation, 以及如何提高运行 benchmark 的速度。
1. GPGPU-Sim 运行机制 首先要理解 application 是如何运行在 real machine 以及 GPGPU-Sim 上的，他们的区别在哪里？这里以 CUDA 代码为例。
GPGPU-Sim_vs_Real_Machine
1.1 Real Machine CUDA application 分为 host code and device code, 使用 nvcc 编译 .cu 代码时, 会将 host code 和 device code 分开。device code 被编译为 .ptx 文件，再通过 ptxas 编译为 cubin.bin 文件。host code, libcuda.a, cubin.bin 文件由 C/C++编译器编译了解生成可执行文件。
如何运行 CUDA application? 调用 libcuda 内的接口以在 GPU 上运行 device code.</description>
    </item>
    
    <item>
      <title>搭建GPGPU-Sim实验环境</title>
      <link>http://localhost:1313/posts/%E7%BC%96%E7%A8%8B_%E6%90%AD%E5%BB%BAgpgpu-sim%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Mon, 27 Sep 2021 22:54:06 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/%E7%BC%96%E7%A8%8B_%E6%90%AD%E5%BB%BAgpgpu-sim%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83/</guid>
      <description>0. 前言 第一个思路是
 服务器OS-&amp;gt;Docker Container-&amp;gt;Ubuntu中运行GPGPU-Sim。 Docker Container update Docker Image-&amp;gt;Docker Image-&amp;gt;XXX.tar-&amp;gt;复制到你的电脑Windows-&amp;gt;复制到你的虚拟机Ubuntu-&amp;gt;XXX.tar-&amp;gt;Docker Image-&amp;gt;Docker Container-&amp;gt;Ubuntu中运行GPGPU-Sim-&amp;gt;修改GPGPU-Sim 然后同样使用上述过程移植到服务器，运行  这样是有问题的。首先这个过程没有意义，如果这样在你自己的虚拟机里面运行Docker, 那么仍然是命令行界面，和在服务器上运行的区别在哪？
这样实现了Docker的其中一个作用
 我在服务器上能跑，在我自己的虚拟机上也能跑。实现了在不同的环境下运行，而且无需安装多余的依赖。因为本质上我用的是 Docker 中的 Ubuntu 14 但我没有实现自己的目的  我的目的是什么？
  在自己的Ubuntu上使用VScode修改模拟器，简单地编译测试性能。修改后需要跑大量benchmark, 这个时候我不能用自己的电脑跑了，我需要移植了。
  把跑benchmark需要用到的东西放在服务器上，用服务器的计算资源运行。需要用到的东西是什么？
  benchmark: 一般是一些 .cu/.cl 代码编译后生成的可执行文件
   编译成功gpgpusim以后，实际上主要是生成了一个libcudart.so。
 那么就需要这个 libcudart.so
    所以理论上来说如果我使用一台固定的服务器，好像不需要一直更新Docker?无需安装 gcc4.5.1, cuda4.2。每次把这几个文件拷贝过去即可。
  0.1 测试 在 gpgpu-sim_distribution 目录下只放置 lib 文件夹 也是可以 Run 的，说明程序运行时只会 call libcudart.so 这个文件</description>
    </item>
    
    <item>
      <title>Docker Image and Container</title>
      <link>http://localhost:1313/posts/docker%E5%B8%B8%E7%94%A8%E7%9A%84%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Fri, 17 Sep 2021 16:25:25 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/docker%E5%B8%B8%E7%94%A8%E7%9A%84%E6%93%8D%E4%BD%9C/</guid>
      <description>0. 前言 最近需要使用到 Docker, 记一下笔记和常用的操作。主要是参考菜鸟教程和阮一峰老师的教程。
1. 启动Docker服务 # service 命令的用法 $ sudo service docker start # systemctl 命令的用法 $ sudo systemctl start docker 2. Image文件 **Docker 把应用程序及其依赖，打包在 image 文件里面。**只有通过这个文件，才能生成 Docker 容器。 image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。
# 列出本机的所有 image 文件。 $ docker image ls $ docler images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest fb52e22af1b0 2 weeks ago 72.8MB hello-world latest d1165f221234 6 months ago 13.3kB ubuntu 15.</description>
    </item>
    
    <item>
      <title>SIMT_Core</title>
      <link>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_simt_core/</link>
      <pubDate>Sat, 04 Sep 2021 19:04:57 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_simt_core/</guid>
      <description>0. 前言 搞懂 SIMT Core 对于理解 GPGPU 的指令 fetch、指令发射、内存访问、数据传输等步骤非常重要，按照 GPGPU-Sim 的官方文档进行一个简单的梳理
SIMT Core 的微架构模型中有几个比较重要的硬件单元，接下来会一一介绍他们的作用，
000 放一个硬件概念对应表 1. Front End  Instruction cache access Instruction buffer logic Scoreboard Scheduling logic SIMT stack  1.1 Fetch and Decode 这里介绍整个指令 Fetch and Decode 阶段，涉及到的硬件单元主要是 Fetch, I-Cache, Decode, I-Buffer, ScoreBoard
I. Fetch Fetch 单元是一个调度器，作用
 根据 PC 的值，从 I-Cache 中取指令，即发送内存请求。  对于一个 warp，如果在 I-Buffer 中没有任何 valid 指令 (valid bit 作用在 III. I-Buffer 中有介绍)，那么这个 warp 就可以进行 instruction fetch。</description>
    </item>
    
    <item>
      <title>CUDA_set_gridDim</title>
      <link>http://localhost:1313/posts/cuda_set_griddim/</link>
      <pubDate>Tue, 17 Aug 2021 14:12:38 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/cuda_set_griddim/</guid>
      <description>0. 前言 在一个 CUDA 课程的考试中由于这个地方的理解问题导致没有成功 pass，应该如何设置 BlockNum 呢？
1. 参数  compute capability, CC  这个也就是计算架构，对应于具体的 NVIDIA 显卡型号，可以在编译时作为 option 输入
 ThreadsPerBlock  这个参数是最常见的，也就是 blockDim, 自己设置的是1024, 计算架构会决定 block 内线程数的上限
 RegistersPerThread  一直没有设置过这个参数，ptxas info 会给给出具体的使用数据
1.1 问题 接下来问题出现了，到底应该怎么设置 gridDim 呢？也就是 BlockNums. 在测试代码中数据量 N=10000000, 自己的理解是我一个 block 用1024个 threads，使用 256 个block，这样的话总共有 256*1024 个 threads 可以并行工作，那么我用 for 循环加上步长 stride = blockDim.x * gridDim.x 来实现 N 个数据的计算，也就是
#define N 10000000#define THREADS_PER_BLOCK 1024#define BLOCK_NUMS 256//#define BLOCK_NUMS ((N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK)  __global__ void gpu_histogram(int *input, int count, int *output) { int index = blockIdx.</description>
    </item>
    
    <item>
      <title>Ca2_lab2</title>
      <link>http://localhost:1313/posts/ca2_lab2/</link>
      <pubDate>Wed, 28 Jul 2021 18:23:41 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/ca2_lab2/</guid>
      <description>0. 前言 很久之前就想总结一下 Computer Architecture II (CA2) 这门课上学得一些东西了，尤其是关于这几个 lab。当时无论是在 Linux, C++, 还是体系结构方面，都帮助我加深了理解。现在试着整理也是复习一下，把他放在博客的文章中。
1. Goal </description>
    </item>
    
    <item>
      <title>Ca2_lab1</title>
      <link>http://localhost:1313/posts/ca2_lab1/</link>
      <pubDate>Wed, 28 Jul 2021 16:09:45 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/ca2_lab1/</guid>
      <description>0. 前言 很久之前就想总结一下 Computer Architecture II (CA2) 这门课上学得一些东西了，尤其是关于这几个 lab。当时无论是在 Linux, C++, 还是体系结构方面，都帮助我加深了理解。现在试着整理也是复习一下，把他放在博客的文章中。
1. Goal 主要是由两个目标
 实现 Cache Replacement Policy 中的 OPT 算法，也就是假设已经得知了对 cache line 的访问序列，每次都 evict 最久之后访问的那个 cache line 将 Sniper 中的 inclusive policy 改为 non-inclusive policy  通过这个 lab 更深刻地理解了一些 cache replacement, 模拟器中访问 cache 和内存的 flow, cache 的地址映射方式等等
2. 思路和 Report Optimal算法  首先，每一条指令的access操作都会经过函数 AccessSingleLine，在这个地方执行文件 IO 操作。  第一遍执行程序的时候进行文件写入（假设两次执行程序的指令序列是完全一样的），将指令的 set_index 和 tag 写入future_list.txt文件（预先将指令序列写入文件） 第二遍指令的时候，相当于我们是已知未来序列的，在第一次调用 AccessSingleLine 的时候，将文件读入一个二维数组future_list，存放所有指令的 set_index 和 tag。后续调用 AccessSingleLine 的时候不再进行文件 IO 操作（写一个条件判断，只执行一次文件 IO 操作）。 二维数组 future_list 中存放了指令 access 序列，将其在 class Cache 中定义，定义为 long long int 型的静态变量，并且有足够大的空间。   根据对 sniper 代码的阅读，此模拟器执行的是 LRU 替换算法，为了不大量修改一些函数接口和逻辑（比如当 cache 为空时的替换和替换算法的选择），我选择直接在 cache_set_lru.</description>
    </item>
    
    <item>
      <title>Ca2_lab0</title>
      <link>http://localhost:1313/posts/ca2_lab0/</link>
      <pubDate>Wed, 28 Jul 2021 15:23:39 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/ca2_lab0/</guid>
      <description>0. 前言 很久之前就想总结一下 Computer Architecture II (CA2) 这门课上学得一些东西了，尤其是关于这几个 lab。当时无论是在 Linux, C++, 还是体系结构方面，都帮助我加深了理解。现在试着整理也是复习一下，把他放在博客的文章中。
1. Goal  Through this lab, you would compile Sniper, a next-generation parallel, high-speed and accurate x86 simulator [0.5 points]. Then you need to modify the source code of Sniper to show expected output and upload a summary to BlackBoard [1.5 points]. Total is 2 points.
 提供了一个 C 代码文件 toy-lab0.c， 编译并运行模拟器 Sniper。然后修改 Sniper 源代码，找到 CLFLUSH 这条指令，在这条指令每次执行后打印 [STUDENT-EMAIL-ACCOUNT, function, line number] CLFLUSH instruction executed</description>
    </item>
    
    <item>
      <title>CUDA_driver, nvcc, cuda, cudatoolkit,cudnn浅析</title>
      <link>http://localhost:1313/posts/cuda_driver-nvcc-cuda-cudatoolkitcudnn%E6%B5%85%E6%9E%90/</link>
      <pubDate>Sun, 25 Jul 2021 11:00:17 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/cuda_driver-nvcc-cuda-cudatoolkitcudnn%E6%B5%85%E6%9E%90/</guid>
      <description>前言 文章转载自知乎答主 marsggbo，当做笔记记录一下这些 CUDA 中经常接触的内容。
0 在使用深度学习框架的过程中一定会经常碰到这些东西，虽然anaconda有时会帮助我们自动地解决这些设置，但是有些特殊的库却还是需要我们手动配置环境，但是我对标题上的这些名词其实并不十分清楚，所以老是被网上的教程绕得云里雾里，所以觉得有必要写下一篇文章当做笔记供之后参考。
0. GPU型号含义  参考【GPU编程系列之一】从深度学习选择什么样的gpu来谈谈gpu的硬件架构
  显卡： 简单理解这个就是我们前面说的GPU，尤其指NVIDIA公司生产的GPU系列，因为后面介绍的cuda,cudnn都是NVIDIA公司针对自身的GPU独家设计的。 显卡驱动：很明显就是字面意思，通常指NVIDIA Driver，其实它就是一个驱动软件，而前面的显卡就是硬件。 gpu架构：Tesla、Fermi、Kepler、Maxwell、Pascal 芯片型号：GT200、GK210、GM104、GF104等 显卡系列：GeForce、Quadro、Tesla GeForce显卡型号：G/GS、GT、GTS、GTX  gpu架构指的是硬件的设计方式，例如流处理器簇中有多少个core、是否有L1 or L2缓存、是否有双精度计算单元等等。每一代的架构是一种思想，如何去更好完成并行的思想
芯片就是对上述gpu架构思想的实现，例如芯片型号GT200中第二个字母代表是哪一代架构，有时会有100和200代的芯片，它们基本设计思路是跟这一代的架构一致，只是在细节上做了一些改变，例如GK210比GK110的寄存器就多一倍。有时候一张显卡里面可能有两张芯片，Tesla k80用了两块GK210芯片。这里第一代的gpu架构的命名也是Tesla，但现在基本已经没有这种设计的卡了，下文如果提到了会用Tesla架构和Tesla系列来进行区分。
而显卡系列在本质上并没有什么区别，只是NVIDIA希望区分成三种选择，
 GeFore用于家庭娱乐 Quadro用于工作站 Tesla系列用于服务器。  Tesla的k型号卡为了高性能科学计算而设计，比较突出的优点是双精度浮点运算能力高并且支持ECC内存，但是双精度能力好在深度学习训练上并没有什么卵用，所以Tesla系列又推出了M型号来做专门的训练深度学习网络的显卡。需要注意的是Tesla系列没有显示输出接口，它专注于数据计算而不是图形显示。
最后一个GeForce的显卡型号是不同的硬件定制，越往后性能越好，时钟频率越高显存越大，即G/GS&amp;lt;GT&amp;lt;GTS&amp;lt;GTX。
1. CUDA名称含义 1.1 CUDA 看了很多答案，有人说CUDA就是一门编程语言，像C,C++,python 一样，也有人说CUDA是API。CUDA英文全称是Compute Unified Device Architecture，是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。按照官方的说法是，CUDA是一个并行计算平台和编程模型，能够使得使用GPU进行通用计算变得简单和优雅。
1.2 cudnn 这个其实就是一个专门为深度学习计算设计的软件库，里面提供了很多专门的计算函数，如卷积等。从上图也可以看到，还有很多其他的软件库和中间件，包括实现c++ STL的thrust、实现gpu版本blas的cublas、实现快速傅里叶变换的cuFFT、实现稀疏矩阵运算操作的cuSparse以及实现深度学习网络加速的cuDNN等等，具体细节可参阅GPU-Accelerated Libraries
1.2 CUDA Toolkit  参考CUDA Toolkit
 CUDA Toolkit由以下组件组成：
1.2.1 Compiler CUDA-C和CUDA-C++编译器NVCC位于bin/目录中。它建立在NVVM优化器之上，而NVVM优化器本身构建在LLVM编译器基础结构之上。因此开发人员可以使用nvm/目录下的Compiler SDK来直接针对NVVM进行开发。
1.2.2 Tools 提供一些像profiler,debuggers等工具，这些工具可以从bin/目录中获取
1.2.3 Libraries 下面列出的部分科学库和实用程序库可以在lib/目录中使用(Windows上的DLL位于bin/中)，它们的接口在include/目录中可获取。
 cudart: CUDA Runtime cudadevrt: CUDA device runtime cupti: CUDA profiling tools interface nvml: NVIDIA management library nvrtc: CUDA runtime compilation cublas: BLAS (Basic Linear Algebra Subprograms，基础线性代数程序集) cublas_device: BLAS kernel interface &amp;hellip;  1.</description>
    </item>
    
    <item>
      <title>Blog_Hugo_About页面制作</title>
      <link>http://localhost:1313/posts/blog_hugo_about%E9%A1%B5%E9%9D%A2%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Sat, 24 Jul 2021 19:19:09 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/blog_hugo_about%E9%A1%B5%E9%9D%A2%E5%88%B6%E4%BD%9C/</guid>
      <description>前言 最近在学习使用 hugo 制作自己的博客，把制作过程的记录下来。我想博客应该会是之后的学习工作中会频繁使用和交互的东西。
本文记录添加 about 页面的过程。
注：这个是使用 markdown 进行添加，并非制作 html 页面。Hugo 主题基于 Ink
添加 About 页面 右键打开 Git 命令行，输入
hugo new about.md 在文件夹 posts 的同级目录下新建了一个 about.md 文件。
修改 markdown 文件顶部的选项使其能够出现在首页菜单栏
title: &amp;#34;About&amp;#34; date: 2021-07-08T17:46:31+08:00 menu: &amp;#34;main&amp;#34; weight: 60 comment: false 预览 在根目录下打开 Git，输入命令
hugo -D server 在自己的浏览器上访问网址：http://localhost:1313/ 即可预览
Reference https://cpurely.github.io/post/hugo%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0about%E5%92%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E9%A1%B5%E9%9D%A2/</description>
    </item>
    
    <item>
      <title>GPGPU_Arch</title>
      <link>http://localhost:1313/posts/gpgpu_arch/</link>
      <pubDate>Sat, 24 Jul 2021 16:35:27 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/gpgpu_arch/</guid>
      <description>GPGPU Architecture 从有缩进的那一段开始成为第一段
1. Introduction 1.1 The Landspace Of Computation Accelerators 1 提升性能不能光依赖于摩尔定律了，需要从 Computer Arch 中去寻找提升
2 GPU 的性能优势, vector HW
3 专用的硬件对应用的性能提升帮助很大，如谷歌 TPU
4 modern GPUs support a Turing Complete programming model, 这是人们对 GPU 感兴趣的一大原因
By Turing Complete, we mean that any computation can be run given enough time and memory.
1.2 GPU Hardware Basic 1 API for GPUs,
 These APIs function by providing convenient interfaces that hide the complexity of managing communication between the CPU and GPU rather than eliminating the need for a CPU entirely.</description>
    </item>
    
    <item>
      <title>Blog_Hugo_windows_install</title>
      <link>http://localhost:1313/posts/blog_hugo_win_install/</link>
      <pubDate>Sat, 24 Jul 2021 16:33:34 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/blog_hugo_win_install/</guid>
      <description>Installing on Windows 假设  你知道如何打开一个命令提示窗口。 你运行的是一个现代64位的 Windows。 你的网站地址是 example.com。 你将使用 D:\Hugo\Sites 作为网站的起点。 你将使用 D:\Hugo\bin 存储可执行文件。  设置你的文件夹 你将需要一个存储 Hugo 可执行文件、博客内容（你创建的的那些文件），以及生成文件（Hugo 为你创建的 HTML）的地方。
 打开 Windows Explorer。 创建一个新的文件夹，D:\Hugo。 创建一个新的文件夹，D:\Hugo\bin。 创建一个新的文件夹，D:\Hugo\Sites。  下载预先编译好的 Windows 版本的 Hugo 可执行文件  2021-07-07 11:25:22 为什么找不到 hugo 的命令了，可能是因为把文件夹 STU 改名改成了 ShanghaiTech，而 windows 需要配置环境变量。重新配置环境变量中的路径应该就可以了
:heavy_check_mark: 就是这个原因，不过注意是系统变量中
环境变量，简单来说就是在系统层面给这个程序的安装路径进行登记，使得我们通过CMD或Git直接输入程序名就能全局调用。
  使用 go 编译 Hugo 的一个优势就是仅有一个二进制文件。你不需要运行安装程序来使用它。相反，你需要把这个二进制文件复制到你的硬盘上。我假设你将把它放在 D:\Hugo\bin 文件夹内。如果你选择放在其它的位置，你需要在下面的命令中替换为那些路径。
 在浏览器中打开 https://github.com/spf13/hugo/releases。 当前的版本是 hugo_0.13_windows_amd64.zip。 下载那个 ZIP 文件，并保存到 D:\Hugo\bin 文件夹中。 在 Windows Explorer 中找到那个 ZIP 文件，并从中提取所有的文件。 你应该可以看到一个 hugo_0.</description>
    </item>
    
    <item>
      <title>Blog_Hugo_基本部署</title>
      <link>http://localhost:1313/posts/blog_hugo_%E5%9F%BA%E6%9C%AC%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Sat, 24 Jul 2021 16:28:22 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/blog_hugo_%E5%9F%BA%E6%9C%AC%E9%83%A8%E7%BD%B2/</guid>
      <description>前言 记录一下从0开始的部署，之前3月弄的没有记笔记，7月就忘记了，还是 要好好整理好好记录。
假设现在已经安装好了 Hugo 环境，我使用的是 windows 下安装。
添加主题 有很多 Hugo Theme 可以选择
这里一开始用的是 archie，现在改成 Ink
cd blog;\ git init;\ git submodule add https://github.com/knadh/hugo-ink.git themes/hugo-ink;\ # Edit your config.toml configuration file # and add the Ananke theme. echo &amp;#39;theme = &amp;#34;ananke&amp;#34;&amp;#39; &amp;gt;&amp;gt; config.toml  切换主题后 push github 报错的本质原因是没有执行 git submodule add, 即没有在文件 .gitmodules 中加入新的主题
 发布文章 hugo new posts/XXX.md 会在 contene/posts 文件夹下生成 XXX.md 文件
title: &amp;#34;Blog_Hugo_基本部署&amp;#34; date: 2021-07-24T16:28:22+08:00 draft: false tags: [&amp;#34;博客&amp;#34;, &amp;#34;技巧&amp;#34;] categories: [&amp;#34;Hugo&amp;#34;] 为文章添加 tags, categories 以便分类</description>
    </item>
    
    <item>
      <title>Ubuntu 20.04 下安装运行 GPGPU-Sim</title>
      <link>http://localhost:1313/posts/%E7%BC%96%E7%A8%8B_ubuntu-20.04-%E4%B8%8B%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C-gpgpu-sim/</link>
      <pubDate>Mon, 10 May 2021 15:17:51 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/%E7%BC%96%E7%A8%8B_ubuntu-20.04-%E4%B8%8B%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C-gpgpu-sim/</guid>
      <description>0. 前言 最近因为课程 Project 需要使用 GPGPU-Sim 复现一篇 paper，在之后的课题中可能也会用到这个模拟器。所以收集了相关资料以搭建 GPGPU-Sim 的环境并运行 Demo。GPGPU-Sim 的参考资料实在是不多，主要参考了官方文档、Github 中 README 文件，还有一些相关的 Blog。
本次只跑了一个非常简单的 Demo，关于 CUDA 实例可以参考 Textbook 《CUDA by Example》。里面提供了一些 CUDA 编程的源码介绍。有人在 Github 上提供了《CUDA by Example》的源代码。
不过自己搭建 GPGPU-Sim 的环境坑比较多，一定要注意 gcc/g++ 版本问题以及链接库。所以我个人还是建议如果不是长期使用，可以直接下载官方提供的 fully setup virtual machine 。在 http://gpgpu-sim.org/ 下载，然后导入 Virtual Box 使用。他们提供的虚拟机已经配置好了环境，可以直接使用 GPGPU-Sim 编译 .cu 文件然后在模拟器上运行。具体步骤可以参考 UCR 给的流程。
1. 介绍 GPGPU-sim能够在Linux系统下，提供对GPU的功能模拟和性能仿真，让你在没有装NVIDIA显卡的情况下可以编译并运行CUDA程序。当然它更重要的意义是，可以通过修改仿真参数，让开发者修改GPU内部架构，并进行性能仿真，以针对自己的项目需求进行更好的代码设计，获得更好的性能表现。
我使用的环境是
 Ubuntu 20.04 CUDA Toolkit 11.1 gcc/g++ 5 ⚠️ 注意：在 Build GPGPU-Sim 之前最好就确保使用 gcc/g++ 5 版本 当时 GPGPU-Sim 作者测试时使用的是 CUDA 4.</description>
    </item>
    
    <item>
      <title>读田渊栋博士《博士五年总结系列》</title>
      <link>http://localhost:1313/posts/summary_of_phd/</link>
      <pubDate>Fri, 12 Mar 2021 16:54:33 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/summary_of_phd/</guid>
      <description>读田渊栋博士《博士五年总结系列》 ​	有幸找到了田渊栋博士的《博士五年总结系列》文章，对文章的内容进行品读、摘抄，同时慢慢加入自己的理解。希望能从文字中去体会到田渊栋博士走过的那五年，希望自己能在未来发展上产生更加深入的思考。
一  首先，心静下来才能钻进某个领域里认真做事。  真要体会大自然的美丽，那是一 定要涉足别人达不到的地方，要有目标有耐心有毅力，做长久的打算。   其次，有毅力有决心不一定能成事，方法也是不可少的。 最好的态 度是“不抱怨，不解释”，把自己力所能及的事情做好，把自己的错误搬回家，好 好分析，才能有所进步。  因为没有实现的理想，对他人而言，一文不值。 克服困难的过程中，没人喝彩。    Research Tips  第一，要喜欢自己的研究题目。做研究有内心动力(motivation)是非常重要的。  如果自己确实喜欢探索，喜欢解难题，但是导师的方向自己不喜欢怎么办？我的答 案是，多发挥主观能动性，找一个喜欢的小点慢慢扩大。 先找一个两边都能接受的题目，把它做好。   My heart is in the work  需要培养思考的习惯，提高思考的效率。 为达到这一步，一开始需要大量 的投入来找到适合自己的正确方案。  这句话背后是有很多实践的。   可能会很不习惯，想到晚上睡不着，做事吃饭都没心思，生活琐事全都不管，俗话说是“入魔了”。 像我经常有做梦做到自己要 思考的问题，或者每天一早还没完全醒来，就想着某个问题要怎么解，结果真醒来 一看发现全想错了-_-。 在这个阶段，挫折感会特别强烈，会有放弃的念头。 但是，只要坚持下去，大脑会适应，会成习惯，效率会高，会知道一个问题中有哪些地方是关键，会知道思考到什么地步是可以停手的存盘点。 然后你就有了一具不论何时 何地都能进行后台运行的思考机器，能够积累上每天的边角时间，每时每刻在ᨀ升 进步。 正如一句话所说，不疯魔，怎成活。   第三，有思路(idea)就写下来。  写下来本身就是一种“我已经完成了什么”的标志，对士气是很鼓舞 的，也有利于下一次从中断点恢复思考。   第四，多看看别人的工作，但别看太多，抓住主线就好。  我目前认为的最好办法，莫过于在看完几篇本领域最重要的文 章后认真总结，猜出大部分人的路数还有各自方法的优缺点，然后在面对新文章时 采用跳跃式读法，边看边猜，猜对有奖。  有了一定的熟练度之后，看了一部分去猜想作者的idea，这个不是第一次听到，但还是很震撼。      二  选导师，他做什么研究并不是最重要的，比这更重要 的，是人品及交流和表达能力。  Writing 初级  paper不光要有先做啥，再做啥，最后做啥，实验结果是啥，还要有 为什么这么做，原因是什么，激发了怎样的思考，这样的方法对什么样的数据有效，有什么局限。 举个例子，写目标函数是什么，如何用梯度下降优化，数学上就 两个公式，但是段落里可以说明如何选初始点，初始点在这个具体应用中的意义何 在，如何取步长，为何这样选，收敛速度通常多快，哪里可以加速，哪里可以并行 化再加 GPU，等等，这样内容就丰富多了。 克服了这两点，做到开局有理有据，正文言之有物，实验让人信服，那这篇文章基 本上可以中稿了。接下来，就可以进入高级模式了。  中高级  首先，立意要高远。  一篇文章规矩着写，说“我们加了新特征，因为新特征针对数 据集的某些特性建模，实验效果更好”，虽然基本可被录用，但一般不会出彩； 如果说“我们提了新的框架，统一了以前的诸多方法，在这个框架下，算法能自动分析数据加入新特征，实验效果更好”，那这篇就有戏。   其次，故事要流畅。  作者老板说过，一篇好的文章，就如同带着读者在一个花园里行走，路面平坦舒适，左边有山，右边有水，引人入胜，读者漫步欣赏美景，走过亭台楼阁，一点不费劲，一下子就逛完所有还意犹未尽。 细节上，全篇重要的论点要适当重复，每次出现都要和 上下文语境相符，无聊冗长的段落适当精简，但必要的实验步骤需要交代； 每一段 都要有总起有概括，像是花园的指路牌，让读者不至于晕头转向； 不设弯路，反复 推敲逻辑关系，能用一层逻辑说清的绝不用两层，能用简单故事说明白的不用复杂 公式，就算有复杂公式也放进附录里； 繁简要有计划，细节要略写以免让人费解， 主干则要用重笔让人印象深刻； 插图要不言自明，要出现在该出现的地方，能恰当 地作成段落注解； 语句不能太长，避免从句套从句，长短结合比较好，等等。    三 Talk  克服困局  一方面是充分的准备，另一方面是对 自己研究课题的自信。 自己热爱它，为它自豪，愿意讲给别人听，也知道自己如何 遣词造句，那这时大脑就会聚焦到内容上，说着说着就入情入境，英语也就自然地 变得慢了，流畅了。 等到说完，发现自己居然说得还可以，那下一次就更不会紧张 ，久而久之，终于就可以摆脱恶性循环，进入良性轨道。    如何才是好的讲稿呢？  如同写作一样，最好的演讲是一个有唯一主题的流畅故事。 所谓流畅故事，是指幻 灯片和幻灯片之间要有自然过渡，让人不知不觉就听完整个演讲，而不觉得有什么 转折生硬的地方。 相比作文，演讲的流畅性更为重要，因为读者看文章时可以细细 琢磨，听众听演讲时则是一晃而过。 为了流畅性，一个好的演讲可以不惜牺牲大量 细节，只把最重要最易记忆的主线说出来——  但条件是，这最重要的主线不能失了 应有的大转折和大逻辑，不能让人觉得太过简单无聊，或太过跳跃而没有说服力。  找到主线这个点，作者反复强调，确实最需要磨练才能掌握的一个地方   最好的平衡点，在于“意料之外，情理之中”，听完有一种“原来如此，我怎么没 有想到”的感受。   一个看似无关却很有效的办法是  先把幻灯片做好，写好演讲词，然后看看是否能 在规定时间脱稿讲得出来。  先有一个雏形   最好的演讲，是看了幻灯片自然而然能说出句子，而不 需要死记硬背，转折流畅，故事清楚。 另一个常见的错误，是把自己想说的话写在幻灯 片上。但其实幻灯片上写的，应该是在演讲者说完之后，最希望观众记得的内容 （所谓的“take home message”)。 为此，一张幻灯片来来回回做个几次都是正常 的，往往是初稿的字数非常多，往后思考越深则字数越少，到最后只放一张图或者 两张图，但在观众看来，一望即明。   好的幻灯片有几种类型  可以只含一张大图，或互有联系的若干图片，或一个前人 工作的列表，或一件事物的优劣二分法，或一个算法的三个主要步骤，或一些事物 的相互关系，等等。 一句话，如果盯着它十秒钟没看出来重点是什么，那就打回去 重做吧。    四 About time management</description>
    </item>
    
    <item>
      <title>MPI Intro and Practice</title>
      <link>http://localhost:1313/posts/mpi_intro_and_practice/</link>
      <pubDate>Tue, 09 Mar 2021 22:36:19 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/mpi_intro_and_practice/</guid>
      <description>MPI Intro and Practice Intro Definition wiki:
 Message Passing Interface (MPI) is a standardized and portable message-passing standard designed by a group of researchers from academia and industry to function on a wide variety of parallel computing architectures.  Feature  an interface, not a programming language Main model of HPC a cross-language communication protocol  Functions  Communication  Point-to-point communication  Send Recv   Collective communication  Broadcast, scatter/ gather, all to all, reduce, scan, barrier     Almost all parallel programs can be described using the message passing model.</description>
    </item>
    
    <item>
      <title>Git中常用的操作</title>
      <link>http://localhost:1313/posts/git_operation/</link>
      <pubDate>Mon, 08 Mar 2021 19:12:40 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/git_operation/</guid>
      <description>Git中常用的操作 1. Remote Repo 1.1 Add Remote Repo $ git remote add origin git@github.com:huweim/repo_name.git 1.2 Delete Remote Repo  Add a wrong remote repo, we could delete it.  $ git remote -v origin git@github.com:huweim/huweim.github.io.git (fetch) origin git@github.com:huweim/huweim.github.io.git (push)  Delete it  $ git remote rm origin 2. Clone from Remote Repo $ git clone git@github.com:huweim/repo_name.git $ git clone https://github.com/huweim/repo_name.git  很多时候http速度很慢，会clone失败报错，所以建议使用ssh。  3. Delete 3.1 删除工作区文件   与win中右键删除没有区别</description>
    </item>
    
    <item>
      <title>My First Post</title>
      <link>http://localhost:1313/posts/my-first-post/</link>
      <pubDate>Mon, 08 Mar 2021 11:24:04 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/my-first-post/</guid>
      <description>About 3 days, it finally works!</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_gpgpu-sim-performance-simulation-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_gpgpu-sim-performance-simulation-engine/</guid>
      <description>GPGPU-sim - Performance Simulation Engine
1 Performance Model Software Objects ldst_unit *m_ldst_unit;前面的 m 可能表示这个类型的变量
1.2 SIMT Core Class SIMT Core 中的微架构在 shader.h/cc 的类 shader_core_ctx 中实现
 shd_warp_t objects 的集合用于建模每个 warp 在 core 中的状态 simt_stack object, 处理每个 warp 的分支 set of scheduler_unit obj, 选择 set 中 warp 的一条 or 多条指令发射执行 Scoreboard obj 处理 data hazard opndcoll_rfu_t obj, model operand collector set of simd_function_unit obj 实现 ALU pipeline ldst_unit 实现 memory pipeline shader_memroy_interface 将 SIMT Core 连接到相应的 SIMT Core Cluster  每个 core cycle, 调用 shader_core_ctx::cycle() 来模拟 SIMT Core 的一个 cycle。cycle function 以按从下往上的顺序 (也就是从 writeback() 到 fetch()) 调用下列函数</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_gpgpu-sim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_gpgpu-sim/</guid>
      <description>0. 前言 资料主要是参考 GPGPU-Sim 官方文档
1. Front End 在整个 SIMT Core 中有3个调度器，组成三个循环调度流水线，这里先简单的介绍一下
 instruction fetch loop  includes the blocks labeled Fetch, I-Cache, Decode, and I-Buffer   instruction issue loop  includes the blocks labeled I-Buffer, Scoreboard, Issue, and SIMT Stack   register access scheduling loop  includes the blocks labeled Operand Collector, ALU, and Memory    1.1 Fetch Fetch 是3个调度器中的一个(1/3)。Fetch 单元根据 PC 的值，从 I-Cache 中取指令。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_software-design-of-gpgpu-sim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/%E6%96%87%E6%A1%A3_software-design-of-gpgpu-sim/</guid>
      <description>0. 前言 这部分介绍了模拟器源码中比较重要的几个部分，主要是关于 SIMT Core 中的几个单元的代码。
4. Software Design of GPGPU-Sim 4.4 GPGPU-Sim - Performance Simulation Engine 4.4.1 Performance Model Software Objects 4.4.1.1 SIMT Core Cluster Class SIMT Core Cluster 通过类 simt_core_cluster 建模。这个类在 m_core 中包含一组 SIMT core 对象。simt_core_cluster::core_cycle() 方法只是按顺序 循环调用 (cycles) 每个 SIMT core.
simt_core_cluster::icnt_cycle() 方法将内存请求从 interconnection network push 到 SIMT Core Cluster&amp;rsquo;s response FIFO. 它也将 FIFO 中的请求出队，送到合适的 core&amp;rsquo;s instruction cache or LDST unit. 这些与前面描述的硬件块密切对应。
4.4.1.2 SIMT Core Class SIMT Core 中的微架构在 shader.</description>
    </item>
    
    <item>
      <title>Archives</title>
      <link>http://localhost:1313/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/archives/</guid>
      <description>历史文章按照年月归档.</description>
    </item>
    
  </channel>
</rss>