---
title: "如何读论文（李沐）"
date: 2022-05-08T20:16:17+08:00
lastmod: 
draft: false
author: "Cory"
tags: ["Reading"]
categories: ["科研"]
---

# 0. 前言

最近（2022-05-08 16:40:17）看到了李沐的这个系列，之前没看过。其实读研以来陆陆续续也看过很多阅读文章，写文章的技巧，教程等等，自己也有一些感触和经验。而最近做完 GPU 微架构相关的一部分课题之后，恰好会接触一个比较新的领域。去读这种新的领域的文章，慢慢开拓到熟悉，也是一种体验。正好这个时间点，就结合自己之前读论文的经验，学到的技巧，结合李沐大神在视频中传授的方法，以及他阅读和看待文章的视角，来做一个总结和记录。

# 1. 方法

第一遍，title, abs, conclusion, conclusion 一般和 abs 结构是一样的，可能会有更加详细的数据，性能提升 XX% 等等; 浏览 method 和实验部分的图表，大概花十几分钟看看这篇文章在讲什么，是不是值得继续读下去。

标题中的作者，学校，也很重要，你对领域内比较熟悉的话，对这些作者肯定也是有所了解的。

第二遍，精读，知道每个 part 在讲什么，但是不需要知道所有的细节，看懂重要的 figure description. 判断是不是自己领域内的，**圈出没看过的参考文献**，如果这篇比较难读，可以从参考文献中的经典入手。

第三遍，带着自己的思考，提出的是什么问题，你能想到这个问题吗？用的什么解决方法，你来做的话会用什么方法？如果用同样的方法，你能做出什么优化？你能拓展出新的东西吗，他的做法可参考吗，你能够复现吗，他有什么地方讲得有问题吗？等等等等，带着一系列问题，给自己设置一系列问题。这一遍看完，合上文章之后，也知道文章在讲什么，也可以给别人讲述出技术细节（所以有人讨论，有一个输出的机会也是很重要的）。

# 2. 第二遍精读，关于 AlexNet

看视频，从李沐阅读 paper 的视角，去思考一位领域内大牛看到文章中的一些句子，脑海中的想法是怎样的。

## 2.1 Intro

看到第一段，李沐不光介绍了段落表达的意思，还做了额外的科普，过拟合是 ML 中的一大派别，我想他应该能继续讲出哪些人在做这方面的工作。推测出，从现在的眼光来看，正则好像又没那么重要。

第一段主要是讲故事，比如自己，需要介绍 GPU 广泛应用于XXX，以及 warp 调度等等。

之后，分析文章写法的问题。在介绍完数据集之后，直接就开始提，用 CNN 解决问题。写法的问题在于，当时的主流并非 CNN，其实应该介绍一些主流算法，然后过渡到 CNN。（同意这个观点）

最后一段开始讲 paper 的贡献，讲自己的工作，做了什么，做了 5 层卷积，3 层全连接，等等。他们用了一些新的技术（挖坑，可以继续往下做），阐述了他们不光卖点很好，也有很多新的东西。

> 从这个角度管中窥豹，一篇文章有好的结果本身已经是一个卖点了，但是他们的方法可能过于复杂，难以复现，别的研究者没法跟进，那么文章可能能中但是不太会有人跟；另一方面，有好的结果，同时有创新的东西，大家都可以 follow，那么这个工作很可能是一篇开山之作，挖坑之作，会有比较高的引用率。

## 2.2 The DataSet

AlexNet 这篇文章标题带了 ImageNet，因此大概介绍了一下数据集。自己写的时候，也是大概交代一下 GPU 的 baseline 架构背景，和简单的领域内知识。这个部分对同行来说不是很重要，完善文章结构，没有涉及到 paper 的 idea。

最后一段还说明了对图片做的一些处理。

什么是 End to end: 原始的图片\文本直接进去，不做任何特征提取，神经网络能帮你做出来。end to end 是他当时的亮点，但是当时由于历史局限性，可能没有发现。

## 2.3 The Architecture

Section 3.1 首先介绍了 ReLU。有一些技术细节，其实刚接触的时候是不知道具体意思的，没有关系，可以圈一下，记录一下。比如 3.1 的 saturating non-linearity, non-saturating non-linearity.  

Section 3.2 介绍了 Training on Multiple GPU, 如何划分 GPU 的。这个部分，站在 ML 研究者角度，第二遍可以略过，除非需要复现，因为可能看不懂，而且不是 ML 方法上的东西；对于 系统/体系结构 研究者，可以关注一下。

Section 3.3 讲了 LRN 的一些细节，第二遍读的时候也可以忽略掉

Section 3.4 讲了 Overlap Pooling，实际上一般 pooling 是没有 overlap，如果不知道 pooling 是什么可以先跳过，去看了 pooling 之后再细读。这个地方可以大概知道用的 pooling 方法不太常规。

文章类似于一个技术报告的风格，没有 highlight, 宣传做了什么。

李沐老师对于 Figure 2 的讲解通俗易懂。

为什么模型图切到两块 GPU？可能是 Alex 刚好是这么做了，并且写了大量的代码去支撑这个工作，所以将其作为一个贡献放在里面。

## 2.4 Reducing Overfitting

介绍如何降低过拟合。

Section 4.2 Dropout, 很多模型融合到一起太贵了

> 这两部分主要是关于技术细节，所以结合自己经历的感触可能不是特别多。

## 2.5 Details of Learning

介绍模型是如何训练的。逐渐出现了一些看起来玄学的部分，比如一些参数的设置。

当时需要训练 5-6 天，在两个 GPU 上。

## 2.6 Result

进入到了实验部分，对于不复现的人来说，或者不是同领域的人，实验相对没有那么重要。

# 3. 如何找到研究想法

## 3.1 打补丁

在原作者想法的基础上，做一些加减法。比如他用了新的 loss，用了两个（我能不能用 3 个？做数据增强，我能不能做？），用故事把你的补丁串联起来。

如果一篇文章已经是打补丁的论文，可能难以找到机会。如果脑洞比较大的工作，可以做更多尝试。比如 iPAWs 已经是 16 年的文章，follow 的人有但不是特别多，回头来看可能不是一个特别好的选择。
