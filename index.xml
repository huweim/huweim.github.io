<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cory</title><link>https://huweim.github.io/</link><description>Recent content on Cory</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 13 Jan 2023 14:25:14 +0800</lastBuildDate><atom:link href="https://huweim.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>About</title><link>https://huweim.github.io/about/</link><pubDate>Thu, 08 Jul 2021 17:46:31 +0800</pubDate><guid>https://huweim.github.io/about/</guid><description>&lt;p>oh man, I love computer science and coding.&lt;/p>
&lt;p>一个计算机体系结构方向的学生，喜欢体系结构，喜欢编程，喜欢篮球。目前的课题主要关注 GPU/GPGPU。&lt;/p>
&lt;p>不确定自己是否喜欢折腾，是否喜欢新鲜事物，是否擅长编程。希望在一步一步探索中找到自己真正感兴趣的事情，或者确定这就是自己真正感兴趣的事情。&lt;/p></description></item><item><title>Xx</title><link>https://huweim.github.io/posts/xx/</link><pubDate>Fri, 13 Jan 2023 14:25:14 +0800</pubDate><guid>https://huweim.github.io/posts/xx/</guid><description/></item><item><title>（转载）Having Effective Meetings Between Advisors and Students</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_having-effective-meetings-between-advisors-and-students/</link><pubDate>Mon, 26 Sep 2022 02:42:41 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_having-effective-meetings-between-advisors-and-students/</guid><description>&lt;h1 id="having-effective-meetings-between-advisors-and-students">Having Effective Meetings Between Advisors and Students&lt;/h1>
&lt;blockquote>
&lt;p>by Mingyu Gao on Nov 2, 2020 | Tags: Academia, Advice, Students&lt;/p>
&lt;/blockquote>
&lt;p>链接：https://www.sigarch.org/having-effective-meetings-between-advisors-and-students/#undefined&lt;/p>
&lt;p>来自高鸣宇老师，以下是翻译：&lt;/p>
&lt;p>导师和学生之间的有效合作和交流是学术成功的关键。不幸的是，在大多数情况下，导师只能提供一周一次或者两周一次，而非日常的一对一 meeting。这意味着学生一个月也无法和导师交流几次。因此如何优化会议，实现学生和导师之间的有效交流就很值得思考。&lt;/p>
&lt;p>实际上，对我来说现在有很多重要的课题。过去一年半内，我们组扩张到了 8 位研究生（包括下一年即将入学的学生）和一些本科生，以及 gap-year 实习生。几乎每个人都有自己的独立课题，需要时间开会和讨论。每周都会有这样的一次会议，每次会议的时间限制在 30-45 分钟。在这种情况下，交流的有效性十分重要。&lt;/p>
&lt;p>我站在导师和学生这两个角色的视角思考了这件事，我会基于最近的经验分享一些建议，希望这个一手的建议对于新的研究生和导师有用。&lt;/p>
&lt;h5 id="在开始的时候建立合适的风格">在开始的时候建立合适的风格&lt;/h5>
&lt;p>在给出具体的建议之前，我想强调每个人有不同的偏好和工作风格。下面的建议不适用于所有人。因此了解彼此是很重要的，在师生关系开始的时候设置好清晰的期望。比如，学生可能会想知道导师的风格是怎样的，项目的里程碑和论文 deadline 可以预见一年内的节奏。导师也需要认知到学生有多独立，学生一周会在哪些时候工作，会工作多久。直接对话并不是一个坏主意，而不是一直观察并且慢慢适应。&lt;/p>
&lt;blockquote>
&lt;p>这个可以理解为前期的沟通，这也十分重要。适应彼此的节奏。&lt;/p>
&lt;/blockquote>
&lt;h5 id="专注于长期路线图和脚下的路">专注于长期路线图和脚下的路&lt;/h5>
&lt;p>很多低年级学生喜欢准备很长很细节的进展报告，描述他们上周做过的所有事情。因为他们担心到时会觉得他们做的太少了。我认为这是不够的。为了从宝贵的时间中获得更多的信息，学生应该 &lt;strong>花更多的时间来寻求他们下一步应该做什么的建议&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>我完全同意这一点，实际上我在 meeting 的早期也是这么做的。描述了一些工作，询问下一步应该做 A 好一点还是做 B 好一点。但是有的时候投入的时间不足，自己都没怎么思考，也没什么进度，那肯定也就没有 &lt;strong>带着问题&lt;/strong> 去开会。这样的话可能只会觉得水过去就好了，浪费了宝贵的开会时间。&lt;/p>
&lt;/blockquote>
&lt;p>这是从导师那里学习如何思考研究问题，以及在 high level 审视项目的时间。更好的方法是学生简单地总结 &lt;strong>where they are now in the overall plan&lt;/strong>，以及 &lt;strong>接下来的计划是什么&lt;/strong>。之后导师可以开始提供评论和建议。&lt;/p>
&lt;blockquote>
&lt;p>我认为之前有过简单的总结，但是缺少一个 overall 的整理，确实，我们需要先总结之前工作的结论，然后总结目前推进到了哪一步，对于整体进度到了哪一步，然后再阐述接下来的计划。&lt;/p>
&lt;/blockquote>
&lt;p>有两种形式的讨论比较通用。在项目早期，快速地根据精炼的总结，和 high level 的预期来建立 &lt;strong>overall research roadmap&lt;/strong> 是非常重要的。低年级学生可能会提出一个有前途的想法，但是通常不会预见所有的设计问题和全部潜力。导师有助于彻底地分析挑战和机会，更好地找到期望的关键贡献。在投入阶段，应该把更多的时间花在 &lt;strong>immediate next steps&lt;/strong>，也就是下周应该关注什么。比如诸多事情里面优先级最高的，如何调整以处理实验中不好的结果，为了让这个工作更完整还需要补充什么探索。&lt;/p>
&lt;p>在这样的会议之后，学生对于接下来做什么应该有清晰的蓝图，不管是对于最终的文章，还是对于下次会议前的工作。在会议之后总结一个 to do list 总是有所帮助的，它可以作为便于查询的一次记录，下次会议之前也可以检查进度。&lt;/p>
&lt;h5 id="在线下写作中保存细节">在线下写作中保存细节&lt;/h5>
&lt;p>如果会议都是关于 high level 的想法，我们什么时候应该讨论同样重要的底层细节，例如定理证明、模型细节、工作进展描述和具体案例研究？我的建议是把他们全部保存下来。换句话说，学生应该 &lt;strong>write detailed and comprehensive documents&lt;/strong>，包括这次会议中所有重要的，支撑 high level 观点的细节。导师可以在会议前后查阅这份文档。&lt;/p>
&lt;p>我想强调的是这份文章需要 &lt;strong>尽可能地详细&lt;/strong>，这个和仅仅总结要点的，简明的会议 slide 不同。一眼看上去，这似乎非常低效，因为写作是非常耗时的。但是相比其优点，这都是值得的。&lt;/p>
&lt;blockquote>
&lt;p>目前参与了接近 3 个月 intern，在其中几次会议时会记下很多东西和细节，但是在过去一两个月之后，很多东西已经忘记了。一个研究项目的周期是很长的，因此这件事情确实也是有必要做的。&lt;/p>
&lt;/blockquote>
&lt;p>首先，当你开始写下这件事情的时候，作为学生，有机会组织整个 idea，回顾整个设计。有些时候总是会忘记一些事情，除非把他们都写下来，并且详尽地检查。其次，对老师来说，可以用充足的，线下的时间来仔细地浏览所有细节，而不是在背靠背的会议上，顶着紧凑的时间压力来做这些事情。第三点，对学生来说也是练习写作的好机会。如果你发现难以用文字清晰地解释完整的设计细节，总是将通过直接交谈来表达，那你应该记住，你是没有机会和文章审稿人交谈的。最后，这些内部文档的段落和图片很可能被重复使用，或者直接拷贝到最终的文章中。总之，你的时间没有被浪费。&lt;/p>
&lt;blockquote>
&lt;p>不知道有哪种工具适于这种 &lt;strong>内部文档&lt;/strong>，这个听起来是一个大家都能看到的，前期的 paper manuscript，应该有详细的图片和阐述。格式可能都接近于 paper 了，这个倒是可以请教一下高老师。&lt;/p>
&lt;/blockquote>
&lt;h5 id="学会做深度的分析">学会做深度的分析&lt;/h5>
&lt;p>学生应该掌握的另一个技能是自己处理简单的问题，节约会议的时间，用这个时间来解决更重要和更困难的问题。这意味着学生应该学习如何独立地进行深度分析，以解决遇到的问题，或者至少收集足够的数据细节和证据，用于下一次会议的讨论。&lt;/p>
&lt;p>一个例子是，当实验结果和初试期望不符时。与其等到下一次会议才报告给导师，不如通过分析数据主动找出合理的原因。一个技巧是 &lt;strong>zoom in to the next level of details&lt;/strong>，或者 &lt;strong>always measure one level deeper&lt;/strong>。例如，当网络数据包的时延呈现长尾（long tail）时，可以在 tail 中选择几个有代表性的数据包，检查它们的路由是否规律（从所有数据包进入到单个数据包），或者检查它们是否存在沿这些路径的常见热点（hotspot）（从整个路径（routes）进入到单个链接（link））。&lt;/p>
&lt;blockquote>
&lt;p>首先，道理就是很简单的要自己主动。但是行动起来并不容易。并且，这并不是一个简单的按钮，“行动”或者“不行动”，通常，人会在几种状态中切换。有的时候是不想去推进这个课题，由于遇到了一些阻力，可能是没有找到充足的论文支撑，可能是受到了导师的批评，可能是 peer pressure，可能是审稿人的质疑，可能是程序的 bug。这是一个螺旋推进的过程，要不断地调整自己的状态，周期性地审视。&lt;/p>
&lt;p>根据高老师举的例子，理解为做更加底层/细粒度的分析。&lt;/p>
&lt;/blockquote>
&lt;h5 id="寻求帮助">寻求帮助&lt;/h5>
&lt;p>reach out for help&lt;/p>
&lt;p>最后的建议，同样也是最重要的，就是 &lt;strong>actively seek help when needed&lt;/strong>。从来没有人能够独自做所有事情。在你尝试了你能想到的一切方法后（比如那些前人的建议），是时候和你的同门交流，或者直接向导师寻求帮助。承认你遇到了问题，这个场景并不会羞耻或者尴尬。&lt;strong>你不必等到下一次会议&lt;/strong>。发送一封短邮件，或者直接去导师的办公室，简单交谈 5 分钟。或许导师的一两个词就是一盏明灯，让你有更多进展，而不是被卡在原地。生活和研究总是有起有落。请记住，你并不孤单，导师在这里提供帮助和支持。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>不必等到下一次会议&lt;/strong>，一定要主动，积极。&lt;/p>
&lt;p>&lt;strong>请记住，你并不孤单，导师在这里提供帮助和支持&lt;/strong>，当你找到了一起同行的人，一起讨论的人，一定要珍惜。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>作为总结&lt;/strong>，有效的会议是 &lt;strong>make a good plan for time and content&lt;/strong>。我的经验是按以下内容来组织。快速查看当前进度。主要花时间在改进建议和下一步的计划上。以待办事项摘要来结束会议。将简单和次要的问题，以及详细的审视作为离线任务。学习如何独立和全面地分析问题。在需要时积极寻求帮助。&lt;/p></description></item><item><title>论文阅读的笔记模板</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%9A%84%E7%AC%94%E8%AE%B0%E6%A8%A1%E6%9D%BF/</link><pubDate>Fri, 23 Sep 2022 11:15:41 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%9A%84%E7%AC%94%E8%AE%B0%E6%A8%A1%E6%9D%BF/</guid><description>&lt;h1 id="论文标题-title">论文标题 Title&lt;/h1>
&lt;p>会议/期刊的信息，例：&lt;strong>MICRO'21&lt;/strong>&lt;/p>
&lt;p>&lt;strong>作者信息：&lt;/strong> 大概总结一下一作和其导师目前的情况，之前的一些工作，目前的机构等等。&lt;/p>
&lt;h2 id="abs">Abs&lt;/h2>
&lt;p>总结一下摘要。&lt;/p>
&lt;p>浏览文章的图片，可以稍微总结一下图片传递的信息。&lt;/p>
&lt;h2 id="1-intro-总体介绍">1. Intro 总体介绍&lt;/h2>
&lt;p>看懂原文内容后，翻译或者用自己的话阐述，这部分要把几个问题说清楚，并且要有逻辑。&lt;/p>
&lt;ul>
&lt;li>文章大概的背景，之后会引申出，目前的问题是什么？&lt;/li>
&lt;li>目前有哪些解决方案，有哪些工作，之后会阐述目前的工作的问题&lt;/li>
&lt;li>介绍他们的解决方案（他们的 paper 做了什么，contribution 是什么）&lt;/li>
&lt;/ul>
&lt;h2 id="2-背景知识的相关工作">2. 背景知识的相关工作&lt;/h2>
&lt;p>这部分会根据文章背景，把一些需要了解的概念稍微展开讲一下。&lt;/p>
&lt;p>可能会把相关工作归类之后再讲一遍，如果是第一次读这个子领域的顶会，那么这部分内容涉及到的工作可以都浏览一下，了解大概的研究情况。&lt;/p>
&lt;h2 id="3-method">3. Method&lt;/h2>
&lt;p>这部分占据了主要的篇幅，一篇 12 页的文章可能有一半篇幅。我认为核心的地方是看懂每一张图片，暂时先不要过分沉浸于细节（比如各种参数是怎么设置的，除非图片有清晰的说明，否则这个需求应该是为复现服务的）。&lt;/p>
&lt;p>总结一下，可能会有以下几个部分。&lt;/p>
&lt;ul>
&lt;li>展示一些实验结果，主要是为了说明： XX 参数我们为什么要设置为 YY；根据这个观察（例：前几层 layer 不需要高精度）所以我们有了 XX insight；主要是阐述为什么这个系统/架构中的一些参数/模块为什么要这么设计。&lt;/li>
&lt;li>架构/系统的设计细节，能够完整地介绍工作的 flow&lt;/li>
&lt;/ul>
&lt;h2 id="4-实验评估结果">4. 实验/评估/结果&lt;/h2>
&lt;p>介绍实验是怎么做的，用的什么模型/benchmark，在什么框架/系统/模拟器上实现的。&lt;/p>
&lt;p>结果怎么样。一般很多人浏览时会在看完 intro 之后直接看结果如何。所以结果一定是直观的，一目了然的。&lt;/p>
&lt;h2 id="5-结论">5. 结论&lt;/h2>
&lt;p>总结一下作者结论部分的内容。&lt;/p>
&lt;h2 id="6-总结">6. 总结&lt;/h2>
&lt;p>⭐ 这部分是用自己的话总结，也是对自己的锻炼。阐述这篇文章解决的问题，他们的效果如何，他们和之前看的一些 &lt;strong>文章/方法/解决方案&lt;/strong> 相比如何。如果对小领域有了一些了解，还可以写下自己的 “主观” 评价，他们对于实际的 模型/硬件/框架 是否有作用，文章图片画的如何，文章结构如何，写作怎么样，有哪些可以学习的地方，工作量怎么样，有哪些地方比较 confuse，有哪些可以借鉴的地方，如果是你在设计实验，你会怎么做。&lt;/p></description></item><item><title>Python 语法学习</title><link>https://huweim.github.io/post/%E7%BC%96%E7%A8%8B_python%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0/</link><pubDate>Wed, 14 Sep 2022 11:15:41 +0800</pubDate><guid>https://huweim.github.io/post/%E7%BC%96%E7%A8%8B_python%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h5 id="函数">函数&lt;/h5>
&lt;p>&lt;code>def forward(self, x: torch.Tensor) -&amp;gt; torch.Tensor:&lt;/code>，箭头 &lt;code>-&amp;gt;&lt;/code> 后面表示返回值&lt;/p>
&lt;p>尝试在 class 内部调用成员函数，报了错。调用 class 内部成员函数要加上前缀 &lt;code>self.&lt;/code>，否则会被当成外部函数&lt;/p>
&lt;p>&lt;strong>Python 不支持函数重载&lt;/strong>&lt;/p>
&lt;h1 id="1-attribute">1. Attribute&lt;/h1>
&lt;h2 id="11-string">1.1 string&lt;/h2>
&lt;p>用 for 循环实现了 list 中的元素转为 string，目的是用来索引。for 感觉比较麻烦，不易读也不优雅，有没有更好的方法？&lt;/p>
&lt;h3 id="111-operation">1.1.1 Operation&lt;/h3>
&lt;h4 id="1111-slice">1.1.1.1 slice&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>b &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Hello, World!&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(b[&lt;span style="color:#ae81ff">2&lt;/span>:&lt;span style="color:#ae81ff">5&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># llo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(b[&lt;span style="color:#ae81ff">2&lt;/span>:])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># llo, World! &lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="1112-replace">1.1.1.2 replace&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>string&lt;span style="color:#f92672">.&lt;/span>replace(oldvalue, newvalue, count)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># replaces a specified phrase with another specified phrase.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="1113-split">1.1.1.3 Split&lt;/h4>
&lt;p>The split() method splits a string into a list&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>string&lt;span style="color:#f92672">.&lt;/span>split(separator, maxsplit)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> file &lt;span style="color:#f92672">in&lt;/span> files:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># file -&amp;gt; &amp;#39;64_768_192.log&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> str_tmp &lt;span style="color:#f92672">=&lt;/span> file&lt;span style="color:#f92672">.&lt;/span>split(&lt;span style="color:#e6db74">&amp;#39;.&amp;#39;&lt;/span>)[&lt;span style="color:#ae81ff">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># file.split(&amp;#39;.&amp;#39;) -&amp;gt; [&amp;#39;64_768_192&amp;#39;, &amp;#39;log&amp;#39;]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># file.split(&amp;#39;.&amp;#39;)[0] -&amp;gt; &amp;#39;64_768_192&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gemm_data[str_tmp] &lt;span style="color:#f92672">=&lt;/span> process_result(&lt;span style="color:#e6db74">&amp;#34;ant&amp;#34;&lt;/span>, file)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="1114-format">1.1.1.4 Format&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>txt1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;My name is &lt;/span>&lt;span style="color:#e6db74">{fname}&lt;/span>&lt;span style="color:#e6db74">, I&amp;#39;m &lt;/span>&lt;span style="color:#e6db74">{age}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>format(fname &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;John&amp;#34;&lt;/span>, age &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">36&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>txt2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;My name is &lt;/span>&lt;span style="color:#e6db74">{0}&lt;/span>&lt;span style="color:#e6db74">, I&amp;#39;m &lt;/span>&lt;span style="color:#e6db74">{1}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>format(&lt;span style="color:#e6db74">&amp;#34;John&amp;#34;&lt;/span>,&lt;span style="color:#ae81ff">36&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>txt3 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;My name is &lt;/span>&lt;span style="color:#e6db74">{}&lt;/span>&lt;span style="color:#e6db74">, I&amp;#39;m &lt;/span>&lt;span style="color:#e6db74">{}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>format(&lt;span style="color:#e6db74">&amp;#34;John&amp;#34;&lt;/span>,&lt;span style="color:#ae81ff">36&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(txt1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># My name is John, I&amp;#39;m 36&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(txt2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># My name is John, I&amp;#39;m 36&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(txt3)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># My name is John, I&amp;#39;m 36&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="112-实际使用时的一些需求">1.1.2 实际使用时的一些需求&lt;/h3>
&lt;h4 id="1121-list-转-string">1.1.2.1 list 转 string&lt;/h4>
&lt;p>&lt;strong>list item 是 string 类型，join&lt;/strong>&lt;/p>
&lt;p>join, return values are strings&lt;/p>
&lt;p>注意，使用这个方法，list 中的元素也需要是字符串，所以对于 list item 不是字符串的情况，也许还是得用 for loop。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># list = [1, 2, 3, 4, 5]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>list &lt;span style="color:#f92672">=&lt;/span> [&lt;span style="color:#e6db74">&amp;#39;1&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;2&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;3&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;4&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;5&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>join(list) &lt;span style="color:#75715e"># get &amp;#34;12345&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;,&amp;#39;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>join(list) &lt;span style="color:#75715e"># get &amp;#34;1,2,3,4,5&amp;#34; &lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>list item 不是 string 类型&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> it &lt;span style="color:#f92672">in&lt;/span> list:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conv_list &lt;span style="color:#f92672">+=&lt;/span> str(it) &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#e6db74">&amp;#39; &amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="1122-string-转-list">1.1.2.2 string 转 list&lt;/h4>
&lt;p>&lt;strong>使用 list 函数&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> string
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>str_ &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;abcde&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>list1 &lt;span style="color:#f92672">=&lt;/span> list(str_)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(list1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># [&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;, &amp;#39;d&amp;#39;, &amp;#39;e&amp;#39;]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>使用 split() 函数&lt;/strong>，根据 string 中的某个分隔符来划分 element&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> file &lt;span style="color:#f92672">in&lt;/span> files:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># file -&amp;gt; &amp;#39;64_768_192.log&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> str_tmp &lt;span style="color:#f92672">=&lt;/span> file&lt;span style="color:#f92672">.&lt;/span>split(&lt;span style="color:#e6db74">&amp;#39;.&amp;#39;&lt;/span>)[&lt;span style="color:#ae81ff">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># file.split(&amp;#39;.&amp;#39;) -&amp;gt; [&amp;#39;64_768_192&amp;#39;, &amp;#39;log&amp;#39;]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># file.split(&amp;#39;.&amp;#39;)[0] -&amp;gt; &amp;#39;64_768_192&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gemm_data[str_tmp] &lt;span style="color:#f92672">=&lt;/span> process_result(&lt;span style="color:#e6db74">&amp;#34;ant&amp;#34;&lt;/span>, file)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="12-list">1.2 list&lt;/h1>
&lt;h3 id="121-一些性质">1.2.1 一些性质&lt;/h3>
&lt;ul>
&lt;li>&lt;code>m_list[-1]&lt;/code> 直接索引到最后一个元素；&lt;code>m_list[-2]&lt;/code> 索引倒数第二个&lt;/li>
&lt;/ul>
&lt;h3 id="122-实际使用时的需要">1.2.2 实际使用时的需要&lt;/h3>
&lt;h4 id="1221-multi-dimension-to-one-dimension">1.2.2.1 multi-dimension to one-dimension&lt;/h4>
&lt;p>也就是多维列表转一维列表，有些时候只是想拿到所有数据并绘图或者做其他处理，不需要其 shape&lt;/p>
&lt;p>对于二维列表，方法比较多，对于多维，目前找到一种方法，并且必须知道具体维度，写对应数量的 for 循环处理&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># from_iterable 方法&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> itertools &lt;span style="color:#f92672">import&lt;/span> chain
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tmp_list &lt;span style="color:#f92672">=&lt;/span> list(chain&lt;span style="color:#f92672">.&lt;/span>from_iterable(grad_output[&lt;span style="color:#ae81ff">0&lt;/span>]&lt;span style="color:#f92672">.&lt;/span>tolist()))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># grad_output[0].tolist() 是一个二维 list&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 使用 for 循环遍历，对于 四维 list&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tmp_list &lt;span style="color:#f92672">=&lt;/span> [element &lt;span style="color:#66d9ef">for&lt;/span> batch &lt;span style="color:#f92672">in&lt;/span> grad_output[&lt;span style="color:#ae81ff">0&lt;/span>]&lt;span style="color:#f92672">.&lt;/span>tolist() &lt;span style="color:#66d9ef">for&lt;/span> channel &lt;span style="color:#f92672">in&lt;/span> batch &lt;span style="color:#66d9ef">for&lt;/span> height &lt;span style="color:#f92672">in&lt;/span> channel &lt;span style="color:#66d9ef">for&lt;/span> element &lt;span style="color:#f92672">in&lt;/span> height]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="1222-切片">1.2.2.2 切片&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>tmp_list &lt;span style="color:#f92672">=&lt;/span> [&lt;span style="color:#e6db74">&amp;#39;CS&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;EE&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;EECS&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 取前两个元素&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tmp_list[:&lt;span style="color:#ae81ff">2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 从 index = 1 开始，取两个元素&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tmp_list[&lt;span style="color:#ae81ff">1&lt;/span>:&lt;span style="color:#ae81ff">3&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 前 10 个数，每 2 个数取一个&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tmp_list[:&lt;span style="color:#ae81ff">10&lt;/span>:&lt;span style="color:#ae81ff">2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 所有数，每 5 个取一个&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tmp_list[::&lt;span style="color:#ae81ff">5&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="1223-迭代">1.2.2.3 迭代&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 判断是否可以迭代&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#f92672">from&lt;/span> collections.abc &lt;span style="color:#f92672">import&lt;/span> Iterable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> isinstance(&lt;span style="color:#e6db74">&amp;#39;abc&amp;#39;&lt;/span>, Iterable) &lt;span style="color:#75715e"># str是否可迭代&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">True&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> isinstance(&lt;span style="color:#ae81ff">123&lt;/span>, Iterable) &lt;span style="color:#75715e"># 整数是否可迭代&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 下标循环，用 enumerate&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> i, value &lt;span style="color:#f92672">in&lt;/span> enumerate([&lt;span style="color:#e6db74">&amp;#39;A&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;B&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;C&amp;#39;&lt;/span>]):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span> print(i, value)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">0&lt;/span> A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">1&lt;/span> B
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">2&lt;/span> C
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 二维 list 遍历&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> x, y &lt;span style="color:#f92672">in&lt;/span> [(&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>), (&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>), (&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">9&lt;/span>)]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span> print(x, y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#ae81ff">9&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="1224-列表生成">1.2.2.4 列表生成&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 最基本的用 range&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> list(range(&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">11&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, &lt;span style="color:#ae81ff">6&lt;/span>, &lt;span style="color:#ae81ff">7&lt;/span>, &lt;span style="color:#ae81ff">8&lt;/span>, &lt;span style="color:#ae81ff">9&lt;/span>, &lt;span style="color:#ae81ff">10&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 用 for 循环&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> [x &lt;span style="color:#f92672">*&lt;/span> x &lt;span style="color:#66d9ef">for&lt;/span> x &lt;span style="color:#f92672">in&lt;/span> range(&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">11&lt;/span>)]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">9&lt;/span>, &lt;span style="color:#ae81ff">16&lt;/span>, &lt;span style="color:#ae81ff">25&lt;/span>, &lt;span style="color:#ae81ff">36&lt;/span>, &lt;span style="color:#ae81ff">49&lt;/span>, &lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">81&lt;/span>, &lt;span style="color:#ae81ff">100&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 比如列出当前目录下的所有文件和目录名&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#f92672">import&lt;/span> os
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> [d &lt;span style="color:#66d9ef">for&lt;/span> d &lt;span style="color:#f92672">in&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>listdir(&lt;span style="color:#e6db74">&amp;#39;.&amp;#39;&lt;/span>)] &lt;span style="color:#75715e"># os.listdir可以列出文件和目录&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#e6db74">&amp;#39;.emacs.d&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;.ssh&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;.Trash&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Adlm&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Applications&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Desktop&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Documents&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Downloads&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Library&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Movies&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Music&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Pictures&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Public&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;VirtualBox VMs&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Workspace&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;XCode&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="13-tuple">1.3 tuple&lt;/h2>
&lt;h3 id="131-基本性质">1.3.1 基本性质&lt;/h3>
&lt;p>&lt;code>m_tuple = ('CS', 'EE', 'EECS')&lt;/code>&lt;/p>
&lt;p>和 list 比较类似，不过 tuple 不能修改，只读；不过看了一些拓展，这个不可变似乎只是指针的指向不变，而指针对象本身是否改变就不知道了。&lt;/p>
&lt;p>&lt;strong>只有一个元素的 tuple 为了消除歧义&lt;/strong>，会表示为 &lt;code>t = (1, )&lt;/code>；这一点和我打印出来的结果是一致的，一开始还困惑为什么有个 &lt;code>,&lt;/code>&lt;/p>
&lt;h3 id="132-操作">1.3.2 操作&lt;/h3>
&lt;p>&lt;strong>切片&lt;/strong>，和 list 类似&lt;/p>
&lt;h2 id="14-dict">1.4 dict&lt;/h2>
&lt;h3 id="141-判断-keyvalue-是否存在">1.4.1 判断 key/value 是否存在&lt;/h3>
&lt;p>&lt;strong>key&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>dict_tmp&lt;span style="color:#f92672">.&lt;/span>has_key()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>value&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 判断 key 是否存在&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;CS&amp;#39;&lt;/span> &lt;span style="color:#f92672">in&lt;/span> Major
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 不存在则返回 None&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Major&lt;span style="color:#f92672">.&lt;/span>get(&lt;span style="color:#e6db74">&amp;#39;CS&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 不存在则返回 -1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Major&lt;span style="color:#f92672">.&lt;/span>get(&lt;span style="color:#e6db74">&amp;#39;CS&amp;#39;&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="142-存放">1.4.2 存放&lt;/h3>
&lt;p>dict 内部存放的顺序和 key 放入的顺序是没有关系的。&lt;/p>
&lt;p>key 必须是不可变对象，比如 string, 整数可以作为 key，但是 list 不能作为 key，因为 list 可变&lt;/p>
&lt;h3 id="143-迭代">1.4.3 迭代&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 默认迭代 key&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#f92672">=&lt;/span> {&lt;span style="color:#e6db74">&amp;#39;a&amp;#39;&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;b&amp;#39;&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;c&amp;#39;&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> key &lt;span style="color:#f92672">in&lt;/span> d:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 迭代 value&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> value &lt;span style="color:#f92672">in&lt;/span> d&lt;span style="color:#f92672">.&lt;/span>values():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 同时迭代&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> k, v &lt;span style="color:#f92672">in&lt;/span> d&lt;span style="color:#f92672">.&lt;/span>items():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="15-set">1.5 set&lt;/h2>
&lt;p>看起来就是去重的数组&lt;/p>
&lt;h2 id="16-array">1.6 array&lt;/h2>
&lt;h3 id="161-什么是-array-like">1.6.1 什么是 array like&lt;/h3>
&lt;p>&lt;a href="https://stackoverflow.com/questions/40378427/numpy-formal-definition-of-array-like-objects">https://stackoverflow.com/questions/40378427/numpy-formal-definition-of-array-like-objects&lt;/a> 回答 in stackoverflow&lt;/p>
&lt;blockquote>
&lt;p>It turns out almost anything is technically an array-like. &amp;ldquo;Array-like&amp;rdquo; is more of a statement of how the input will be interpreted than a restriction on what the input can be; if a parameter is documented as array-like, NumPy will try to interpret it as an array.&lt;/p>
&lt;/blockquote>
&lt;p>一个陈述，如何去理解 input，而非一个限制。&lt;/p>
&lt;blockquote>
&lt;p>The term &amp;ldquo;array-like&amp;rdquo; is used in NumPy, referring to anything that can be passed as first parameter to &lt;code>numpy.array()&lt;/code> to create an array ().&lt;/p>
&lt;/blockquote>
&lt;p>直接的定义，可以作为 &lt;code>numpy.array()&lt;/code> 的第一个参数&lt;/p>
&lt;p>&lt;a href="https://thecleverprogrammer.com/2021/03/19/difference-between-tensors-and-arrays/">https://thecleverprogrammer.com/2021/03/19/difference-between-tensors-and-arrays/&lt;/a>&lt;/p>
&lt;p>The difference between a NumPy array and a tensor is that the tensors are backed by the accelerator memory like GPU and they are immutable, unlike NumPy arrays.&lt;/p>
&lt;p>总的来说，&lt;strong>Tensors are like arrays&lt;/strong>, both are data structures that are used to store data that can be indexed individually.&lt;/p>
&lt;p>&lt;strong>综上，tensor 是否是 array-like?&lt;/strong>&lt;/p>
&lt;p>&lt;strong>是的&lt;/strong>&lt;/p>
&lt;h2 id="1x-其他特性">1.X 其他特性&lt;/h2>
&lt;h3 id="1x1-generator">1.X.1 generator&lt;/h3>
&lt;p>暂时没有用过，先挖个坑&lt;/p>
&lt;h1 id="2-数据类型变量语法">2. 数据类型，变量，语法&lt;/h1>
&lt;h2 id="21-data-type">2.1 data type&lt;/h2>
&lt;p>python 支持的 data type 应该足够满足日常使用了。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>complex 在 cutlass 里面也有支持，不过平时没有使用过。tuple 使用也比较少&lt;/p>
&lt;h3 id="211-float">2.1.1 float&lt;/h3>
&lt;p>可以写成 &lt;code>1.23e9&lt;/code>&lt;/p>
&lt;h3 id="212-string">2.1.2 string&lt;/h3>
&lt;p>python 中，用 &lt;code>''&lt;/code> 和 &lt;code>&amp;quot;&amp;quot;&lt;/code> 有什么区别？&lt;/p>
&lt;p>从 stackoverflow 回答来看是完全一样的&lt;/p>
&lt;h2 id="22-类型转换">2.2 类型转换&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>str_ &lt;span style="color:#f92672">=&lt;/span> str(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>int_ &lt;span style="color:#f92672">=&lt;/span> int(str_)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="23-编码">2.3 编码&lt;/h2>
&lt;blockquote>
&lt;p>UTF-8 编码，把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。&lt;/p>
&lt;/blockquote>
&lt;p>看来这里也有哈夫曼的思想&lt;/p>
&lt;h2 id="24-global-variables">2.4 Global Variables&lt;/h2>
&lt;p>在函数内部创建变量时，默认是局部的&lt;/p>
&lt;p>在函数外部定义变量时，默认是全局的，此时无需使用 global 关键字；在函数外使用 global 没有作用。&lt;/p>
&lt;p>在函数内读写全局变量时，需要使用 global 关键字&lt;/p>
&lt;h1 id="3-file-操作">3. File 操作&lt;/h1>
&lt;h2 id="31-文件读写">3.1 文件读写&lt;/h2>
&lt;p>read, open, write&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>f &lt;span style="color:#f92672">=&lt;/span> open(&lt;span style="color:#e6db74">&amp;#34;demofile.txt&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;r&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(f&lt;span style="color:#f92672">.&lt;/span>read())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f &lt;span style="color:#f92672">=&lt;/span> open(&lt;span style="color:#e6db74">&amp;#34;demofile2.txt&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;a&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># a, append; w, overwrite&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f&lt;span style="color:#f92672">.&lt;/span>write(&lt;span style="color:#e6db74">&amp;#34;Now the file has more content!&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f&lt;span style="color:#f92672">.&lt;/span>close()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>实际使用&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> i, line &lt;span style="color:#f92672">in&lt;/span> enumerate(open(get_file_path() &lt;span style="color:#f92672">+&lt;/span> file)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> match &lt;span style="color:#f92672">in&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>finditer(pattern_cycle, line):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tmp &lt;span style="color:#f92672">=&lt;/span> list(match&lt;span style="color:#f92672">.&lt;/span>group(&lt;span style="color:#ae81ff">1&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tmp &lt;span style="color:#f92672">=&lt;/span> float(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>join(tmp))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> res_data[&lt;span style="color:#e6db74">&amp;#34;cycle&amp;#34;&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> tmp
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="32-文件目录操作">3.2 文件目录操作&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>torch&lt;span style="color:#f92672">)&lt;/span> zdli@GPU74:/nvme/wmhu$ &lt;span style="color:#75715e"># 工作目录&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/nvme/wmhu/shader_docker/cutlass-gpgpu-sim &lt;span style="color:#75715e"># python 文件所在的目录&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>一些常用的路径&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>path_ &lt;span style="color:#f92672">=&lt;/span> os.getcwd&lt;span style="color:#f92672">()&lt;/span> &lt;span style="color:#75715e"># 获取当前工作目录的绝对路径, /nvme/wmhu&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>path_ &lt;span style="color:#f92672">=&lt;/span> os.path.abspath&lt;span style="color:#f92672">(&lt;/span>__ file __&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#75715e"># 获取当前文件的绝对路径, /nvme/wmhu/shader_docker/cutlass-gpgpu-sim/data_analyze.py&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>path_ &lt;span style="color:#f92672">=&lt;/span> os.path.dirname&lt;span style="color:#f92672">(&lt;/span>__file__&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#75715e"># 获得当前文件所在目录的绝对路径, /nvme/wmhu/shader_docker/cutlass-gpgpu-sim&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="4-函数">4. 函数&lt;/h1>
&lt;h2 id="41-一些特性">4.1 一些特性&lt;/h2>
&lt;h3 id="411-别名和引用">4.1.1 别名和引用&lt;/h3>
&lt;p>Python 中可以把函数名赋值给一个对象&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> a &lt;span style="color:#f92672">=&lt;/span> abs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> a(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="412-pass-占位符">4.1.2 pass 占位符&lt;/h3>
&lt;p>还没想好函数或者条件语句的内容，可以用一个 pass，让代码可以先运行&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">nop&lt;/span>():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> agr &lt;span style="color:#f92672">&amp;gt;=&lt;/span> &lt;span style="color:#ae81ff">18&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="413-参数检查">4.1.3 参数检查&lt;/h3>
&lt;p>参数数量不对时，Python 解释器可以检查；如果是参数 type 错误，Python 解释器可能看不出来。可以用函数 &lt;code>isinstance()&lt;/code> 来检查，这个看起来有点像 assert()，一个典型的错误和异常处理&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">my_abs&lt;/span>(x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> isinstance(x, (int, float)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">raise&lt;/span> &lt;span style="color:#a6e22e">TypeError&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;bad operand type&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> x &lt;span style="color:#f92672">&amp;gt;=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#f92672">-&lt;/span>x
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="42-__init__-and-forward">4.2 &lt;strong>init&lt;/strong> and forward()&lt;/h2>
&lt;p>了解函数的定义，以及调用，比如 &lt;strong>init&lt;/strong>&lt;/p>
&lt;h3 id="421-__init__">4.2.1 &lt;strong>init&lt;/strong>&lt;/h3>
&lt;blockquote>
&lt;p>个人将其理解为 C 中的构造函数&lt;/p>
&lt;/blockquote>
&lt;p>创建对象时，python 解释器会自动调用它。&lt;/p>
&lt;h3 id="422-forward">4.2.2 forward()&lt;/h3>
&lt;p>使用 pytorch 的训练模型的时候，不需要调用 forward 函数，只需要在实例化一个对象中（&lt;code>model = ViT(model_name, pretrained=True)&lt;/code>）传入对应的参数，就可以自动调用 forward 函数，下面展示一个例子。来自 &lt;a href="https://zhuanlan.zhihu.com/p/357021687">https://zhuanlan.zhihu.com/p/357021687&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Module&lt;/span>(nn&lt;span style="color:#f92672">.&lt;/span>Module):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> super()&lt;span style="color:#f92672">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># ......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># ......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">......&lt;/span> &lt;span style="color:#75715e"># 输入数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 实例化一个对象&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>model &lt;span style="color:#f92672">=&lt;/span> Module()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 前向传播&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>model(data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 而不是使用下面的&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># model.forward(data) &lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="422-why">4.2.2 Why?&lt;/h3>
&lt;p>为什么有这种等价关系？&lt;code>model(data)&lt;/code> 等价于 &lt;code>model.forward(data)&lt;/code>，因为在 class 中使用了 &lt;strong>call&lt;/strong> 函数。更深的就不继续展开了。&lt;/p>
&lt;h2 id="43-unittest">4.3 unittest&lt;/h2>
&lt;p>2022-08-01 14:38:19，今天接触到这玩意儿。因为组里面这边在做和 ngp 相关的东西。有同学用 jax 写了一版代码，test.py 文件看起来没有入口，所有函数都在 class 里面。调试的方式就是 &lt;code>python -m unittest test.py&lt;/code>。所以算是接触到了一个新的东西。&lt;/p>
&lt;p>编写单元测试时，我们需要编写一个测试类，从unittest.TestCase继承。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 如下，继承自 unittest.TestCase&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">TestFoward&lt;/span>(unittest&lt;span style="color:#f92672">.&lt;/span>TestCase):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="44-pytest">4.4 pytest&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>pip install pytest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="441-命名规则">4.4.1 命名规则&lt;/h3>
&lt;ul>
&lt;li>测试文件名必须以“test_”开头&lt;/li>
&lt;li>测试类以Test开头，并且不能带有 init 方法&lt;/li>
&lt;li>测试函数必须以“test_”开头&lt;/li>
&lt;li>除了有setup/teardown，还能更自由的定义fixture装载测试用例&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h3 id="442-case">4.4.2 case&lt;/h3>
&lt;p>来自 &lt;a href="https://www.jianshu.com/p/75c27fe23b4e">https://www.jianshu.com/p/75c27fe23b4e&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># test_class.py&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">TestClass&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">test_one&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;this&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">assert&lt;/span> &lt;span style="color:#e6db74">&amp;#39;h&amp;#39;&lt;/span> &lt;span style="color:#f92672">in&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">test_two&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;hello&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">assert&lt;/span> hasattr(x, &lt;span style="color:#e6db74">&amp;#39;check&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>运行&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ pytest -v test_
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="45-参数">4.5 参数&lt;/h2>
&lt;p>函数作为另一个函数的参数传入：&lt;/p>
&lt;h3 id="451-返回多个参数">4.5.1 返回多个参数&lt;/h3>
&lt;p>其实就是返回一个 tuple，但是在语法上，接收时不需要用括号&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> math
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">move&lt;/span>(x, y, step, angle&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nx &lt;span style="color:#f92672">=&lt;/span> x &lt;span style="color:#f92672">+&lt;/span> step &lt;span style="color:#f92672">*&lt;/span> math&lt;span style="color:#f92672">.&lt;/span>cos(angle)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ny &lt;span style="color:#f92672">=&lt;/span> y &lt;span style="color:#f92672">-&lt;/span> step &lt;span style="color:#f92672">*&lt;/span> math&lt;span style="color:#f92672">.&lt;/span>sin(angle)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> nx, ny
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>x, y &lt;span style="color:#f92672">=&lt;/span> move(&lt;span style="color:#ae81ff">100&lt;/span>, &lt;span style="color:#ae81ff">100&lt;/span>, &lt;span style="color:#ae81ff">60&lt;/span>, math&lt;span style="color:#f92672">.&lt;/span>pi &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#ae81ff">6&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="452-默认参数">4.5.2 默认参数&lt;/h3>
&lt;p>Python 应该不像 C 语言那样有函数的重构，可以用默认参数；必选参数必须全部放在前面，而全部可选参数/默认参数放在后面&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">power&lt;/span>(x, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> n &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n &lt;span style="color:#f92672">=&lt;/span> n &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> s &lt;span style="color:#f92672">=&lt;/span> s &lt;span style="color:#f92672">*&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> power(&lt;span style="color:#ae81ff">5&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> power(&lt;span style="color:#ae81ff">5&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">25&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="453-可变参数">4.5.3 可变参数&lt;/h3>
&lt;p>和返回的情况类似，传入的参数是一个 tuple，下面给出语法&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 加上 * 代表传入可变参数&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">calc&lt;/span>(&lt;span style="color:#f92672">*&lt;/span>numbers):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sum &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> n &lt;span style="color:#f92672">in&lt;/span> numbers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sum &lt;span style="color:#f92672">=&lt;/span> sum &lt;span style="color:#f92672">+&lt;/span> n &lt;span style="color:#f92672">*&lt;/span> n
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> sum
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nums &lt;span style="color:#f92672">=&lt;/span> [&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 传可变参数，过于繁琐&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>calc(nums[&lt;span style="color:#ae81ff">0&lt;/span>], nums[&lt;span style="color:#ae81ff">1&lt;/span>], nums[&lt;span style="color:#ae81ff">2&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 类似于返回多参数的写法&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>calc(&lt;span style="color:#f92672">*&lt;/span>nums)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="454-关键字参数">4.5.4 关键字参数&lt;/h3>
&lt;p>2022-08-11 15:28:10，这一块就是在理论上比较薄弱的地方，也是为什么之前阅读 Python 代码存在一些阻碍。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 除了必选参数 name, age，还可以传入任意个关键字参数 kw&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>def person&lt;span style="color:#f92672">(&lt;/span>name, age, **kw&lt;span style="color:#f92672">)&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;name:&amp;#39;&lt;/span>, name, &lt;span style="color:#e6db74">&amp;#39;age:&amp;#39;&lt;/span>, age, &lt;span style="color:#e6db74">&amp;#39;other:&amp;#39;&lt;/span>, kw&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="455-命名关键字参数">4.5.5 命名关键字参数&lt;/h3>
&lt;p>这个感觉在很多 Python 内置库中常用，限制关键字参数的名字。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># * 作为分隔符，* 之后的参数被视为命名关键字参数&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">person&lt;/span>(name, age, &lt;span style="color:#f92672">*&lt;/span>, city, job):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(name, age, city, job)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 也可以由一个可变参数来分隔&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">person&lt;/span>(name, age, &lt;span style="color:#f92672">*&lt;/span>args, city, job):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(name, age, args, city, job)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> person(&lt;span style="color:#e6db74">&amp;#39;Jack&amp;#39;&lt;/span>, &lt;span style="color:#ae81ff">24&lt;/span>, city&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;Beijing&amp;#39;&lt;/span>, job&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;Engineer&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Jack &lt;span style="color:#ae81ff">24&lt;/span> Beijing Engineer
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。&lt;/p>
&lt;h2 id="45-高阶函数">4.5 高阶函数&lt;/h2>
&lt;p>2022-08-11 16:02:01，之前疑惑的一个地方，函数作为另一个函数的参数传入。这个就是高阶函数。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 高阶函数的例子&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">add&lt;/span>(x, y, f):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> f(x) &lt;span style="color:#f92672">+&lt;/span> f(y)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="451-map-reduce">4.5.1 map, reduce&lt;/h3>
&lt;p>&lt;code>map()&lt;/code> 的参数，一个是函数，另一个是可迭代序列，&lt;code>map()&lt;/code> 会把传入的函数依次作用到序列的每个元素，然后把结果返回到序列中&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">f&lt;/span>(x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span> &lt;span style="color:#66d9ef">return&lt;/span> x &lt;span style="color:#f92672">*&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> r &lt;span style="color:#f92672">=&lt;/span> map(f, [&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, &lt;span style="color:#ae81ff">6&lt;/span>, &lt;span style="color:#ae81ff">7&lt;/span>, &lt;span style="color:#ae81ff">8&lt;/span>, &lt;span style="color:#ae81ff">9&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> list(r)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">9&lt;/span>, &lt;span style="color:#ae81ff">16&lt;/span>, &lt;span style="color:#ae81ff">25&lt;/span>, &lt;span style="color:#ae81ff">36&lt;/span>, &lt;span style="color:#ae81ff">49&lt;/span>, &lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">81&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>reduce()&lt;/code> 也就接受函数作为参数，他会把函数作用在一个序列上，并把结果和序列中的下一个元素做累计计算&lt;/p>
&lt;p>&lt;code>reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)&lt;/code>&lt;/p>
&lt;h3 id="452-lambda">4.5.2 lambda&lt;/h3>
&lt;p>匿名函数，简化代码的一种方式&lt;/p>
&lt;h1 id="5-class">5. Class&lt;/h1>
&lt;h2 id="51-构造函数">5.1 构造函数&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Student&lt;/span>(object):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(self, name, score):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>name &lt;span style="color:#f92672">=&lt;/span> name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>score &lt;span style="color:#f92672">=&lt;/span> score
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 如果要定义为 私有成员，加两个下划线&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>__name &lt;span style="color:#f92672">=&lt;/span> name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>__score &lt;span style="color:#f92672">=&lt;/span> score
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> bart &lt;span style="color:#f92672">=&lt;/span> Student(&lt;span style="color:#e6db74">&amp;#39;Bart Simpson&amp;#39;&lt;/span>, &lt;span style="color:#ae81ff">59&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> bart&lt;span style="color:#f92672">.&lt;/span>name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;Bart Simpson&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> bart&lt;span style="color:#f92672">.&lt;/span>score
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">59&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="52-继承">5.2 继承&lt;/h2>
&lt;p>继承的语法&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Animal&lt;/span>(object):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">run&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;Animal is running...&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 把这个 object 换成需要继承的 class 即可&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Dog&lt;/span>(Animal):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Cat&lt;/span>(Animal):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="6-module--模块">6. module / 模块&lt;/h1>
&lt;h2 id="61-获取模块中的元素--遍历模块">6.1 获取模块中的元素 / 遍历模块&lt;/h2>
&lt;p>定义了一个模块 &lt;code>coda_int.py&lt;/code> 用来装需要绘图的所有 list，一个接一个输入名字过于繁琐，尝试获取模块的中对象来自动遍历。&lt;/p>
&lt;h3 id="611-遍历模块">6.1.1 遍历模块&lt;/h3>
&lt;p>2022-09-14 11:13:48 搞定。用 &lt;code>if not key.startswith('__')&lt;/code> 来过滤掉一些自带的方法，剩下的就是 &lt;code>cola_int&lt;/code> 这个 module 中的 list。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> key, value &lt;span style="color:#f92672">in&lt;/span> cola_int&lt;span style="color:#f92672">.&lt;/span>__dict__&lt;span style="color:#f92672">.&lt;/span>items():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> key&lt;span style="color:#f92672">.&lt;/span>startswith(&lt;span style="color:#e6db74">&amp;#39;__&amp;#39;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(value[&lt;span style="color:#e6db74">&amp;#39;max&amp;#39;&lt;/span>])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="debug">Debug&lt;/h1>
&lt;blockquote>
&lt;p>用Python开发程序，完全可以一边在文本编辑器里写代码，一边开一个交互式命令窗口，在写代码的过程中，把部分代码粘到命令行去验证，事半功倍！前提是得有个27&amp;rsquo;的超大显示器！&lt;/p>
&lt;/blockquote>
&lt;p>这倒是这类语言的一个debug 方法&lt;/p>
&lt;h1 id="bug">BUG&lt;/h1>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;dict_values&amp;#39;&lt;/span> object &lt;span style="color:#f92672">is&lt;/span> &lt;span style="color:#f92672">not&lt;/span> subscriptable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tot_stat[model_name &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#e6db74">&amp;#34;_&amp;#34;&lt;/span> &lt;span style="color:#f92672">+&lt;/span> mode] &lt;span style="color:#f92672">=&lt;/span> sum_stat&lt;span style="color:#f92672">.&lt;/span>values()[len_ &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>:]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>注意 sum_stat.values() 返回的是 dict_keys 对象而不是 list，不支持 index 索引，可以使用&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>tot_stat[model_name &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#e6db74">&amp;#34;_&amp;#34;&lt;/span> &lt;span style="color:#f92672">+&lt;/span> mode] &lt;span style="color:#f92672">=&lt;/span> list(sum_stat&lt;span style="color:#f92672">.&lt;/span>values())[len_ &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>:]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="reference">Reference&lt;/h1>
&lt;p>&lt;a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017024645952992">https://www.liaoxuefeng.com/wiki/1016959663602400/1017024645952992&lt;/a>&lt;/p></description></item><item><title>Tips to Become a Better (Computer Science) Ph.D. Student (By Ziyang Xu)</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_tips-to-become-a-better-computer-science-ph.d.-student/</link><pubDate>Sun, 28 Aug 2022 00:04:17 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_tips-to-become-a-better-computer-science-ph.d.-student/</guid><description>&lt;p>转载北大本科生，本科在梁云老师那里实习，现在是 princeton phd&lt;/p>
&lt;p>Other amazing blogs out there:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.pgbovine.net/PhD-memoir.htm">The Ph.D. Grind&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.ifs.tuwien.ac.at/~silvia/research-tips/">Tips: How to Do Research&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cs.unc.edu/~azuma/hitch4.html">So long, and thanks for the Ph.D.!&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.math.waikato.ac.nz/~seano/grad-school-advice.html">Graduate School Survival Guide&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.sigarch.org/tips-for-a-new-computer-architecture-phd-student/">Tips for a New Computer Architecture PhD Student&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="5-topics">5 Topics&lt;/h2>
&lt;h3 id="research--course">Research &amp;gt; Course&lt;/h3>
&lt;p>Young Ph.D. students tend to spend too much time on courses. However, research outweighs courses.&lt;/p>
&lt;p>&lt;strong>Take courses with a grain of salt&lt;/strong>&lt;/p>
&lt;p>Courses are not as important as they seem to be. The priority of a Ph.D. student is to do research – the earlier you start your research, the better off you’ll be in the long run.&lt;/p>
&lt;p>However, &lt;strong>don’t go to extremes&lt;/strong>! A poor grade can also be a huge problem. You should always be familiar with the requirement of qualification exams or generals and meet all the standards about the courses.&lt;/p>
&lt;p>&lt;strong>Remember the main ideas of courses&lt;/strong>&lt;/p>
&lt;p>Trapping ourselves in trivial details of a course is easy. However, most of the specifics are not important to our research even if the topic is related to our area.&lt;/p>
&lt;p>A good approach is to use what you’ve learned from one course and apply it to a different field (e.g., taking an analysis tool from a compiler course and applying it in computer networks).&lt;/p>
&lt;blockquote>
&lt;p>低年级的 phd 容易迷失在课程中，但是这也完全可以理解。我自己研一的时候由于不适应节奏，感觉几乎所有时间都是花在课程上的。科研比课程重要，我相信大多数人都知道，只是去调整，去适应还需要花费一点时间。&lt;/p>
&lt;/blockquote>
&lt;h3 id="be-professional">Be professional&lt;/h3>
&lt;p>Treat your Ph.D. as a job. You get paid (albeit not much) for being a Ph.D. candidate, so make your work worth the money. This professional mindset should also be apparent to your advisor. Some advisors take on a more hands-off approach, for instance letting you work from home, but this is no reason for slacking; you should be responsible for your research schedule, such as reminding your advisor of plans from previous group meetings. Your status is not that of a student but rather that of a peer in the research community.&lt;/p>
&lt;blockquote>
&lt;p>个人理解举几个具体一点的例子吧，比如 meeting 之类的做好 PPT，有什么事情快速回复和确认，不要看到了之后想着做完了再回复；做不出来的东西也要及时反馈；有时间观念&lt;/p>
&lt;/blockquote>
&lt;h3 id="read-a-lot-and-read-broadly">Read a lot and read broadly&lt;/h3>
&lt;p>Though it can be very daunting starting out, reading papers is an essential part of the Ph.D. life. Previously, you may have read papers when it was necessary for a class or a project. However, you should put reading papers in your daily routine. Doing so allows you to draw inspiration from a sea of knowledge and prevents yourself from reinventing the wheel. Besides, it’s a great way to be productive on a slow day.&lt;/p>
&lt;h3 id="make-a-plan-to-read">Make a plan to read&lt;/h3>
&lt;p>When scheduling your day, assign one period just for reading papers. You can read one paper in depth or compare several papers; regardless of your choice, allotting time to this task is the key.&lt;/p>
&lt;h3 id="read-broadly">Read broadly&lt;/h3>
&lt;p>Reading papers from different subfields of computer science is a great way to learn the jargon, the method, and the mindset of researchers in each field. This can be the first step towards discovering opportunities for collaboration.&lt;/p>
&lt;blockquote>
&lt;p>很惭愧还没有养成每天固定阅读的习惯，我觉得这个必须在入学前做好。&lt;/p>
&lt;/blockquote>
&lt;h3 id="fail-fast">Fail fast&lt;/h3>
&lt;p>It is not uncommon for a Ph.D. student to spend several years building a system that turns out to be fundamentally flawed or not as applicable as expected. Don’t worry! There is nothing wrong with failing, and perhaps we should even expect failure to be part of the journey. But we should aim to fail early in order to have time to work on another project (and graduate!).&lt;/p>
&lt;p>&lt;strong>Perform a limit study&lt;/strong>&lt;/p>
&lt;p>Perform a quick limit study before sticking with a project. A limit study includes in-depth analyses of implicit assumptions we make when coming up with an idea, a related works search, and the potential of the work if everything goes well. A great limit study can itself be a publishable paper. An example can be found &lt;a href="https://homes.cs.washington.edu/~luisceze/publications/fortuna-iiswc2010.pdf">&lt;strong>here&lt;/strong>&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Hacky implementation can be useful&lt;/strong>&lt;/p>
&lt;p>Being a researcher, your work is to develop proof-of-concepts. Nevertheless, you need to demonstrate that your concept is sound for the simplest of cases before continuing to the full-blown system. Hack in the minimum set to show that your idea is possible while resisting the temptation to build a robust infrastructure – if your idea fails, you will know to stop earlier.&lt;/p>
&lt;h3 id="impact-humankind">Impact humankind&lt;/h3>
&lt;p>Impacting humankind may sound too ambitious, but it should be the ultimate reason why we embark on this journey.&lt;/p>
&lt;p>&lt;strong>Choose an impactful research topic&lt;/strong>&lt;/p>
&lt;p>In terms of how our Ph.D. research could impact human knowledge, I would like to refer to &lt;a href="http://matt.might.net/articles/phd-school-in-pictures/">&lt;strong>The Illustrated Guide to a Ph.D.&lt;/strong>&lt;/a> by Matt Might. All we will do in five years is pushing the boundary of human knowledge by a minute margin. Choose a topic that you are able to contribute to, feel passionate about, and can explain the importance of to a layman in a 3-min talk.&lt;/p>
&lt;p>Check out why &lt;a href="http://matt.might.net/">&lt;strong>Matt Might&lt;/strong>&lt;/a> changed his research focus from programming languages to precise medicine.&lt;/p>
&lt;p>&lt;strong>How can our research actually impact people from other fields?&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://liberty.princeton.edu/Publications/sc11_survey.pdf">&lt;strong>A survey paper&lt;/strong>&lt;/a> by the Liberty Research Group sheds light on how the improvement of programming tools impacts (&lt;del>computational scientists&lt;/del>) all scientists. Thinking about how your research affects people from other fields can help you define the scope of your contribution.&lt;/p>
&lt;blockquote>
&lt;p>做有影响力的课题&lt;/p>
&lt;/blockquote>
&lt;h2 id="5-tips">5 Tips&lt;/h2>
&lt;h3 id="dont-give-up-on-your-research-topic-easily">Don’t give up on your research topic easily&lt;/h3>
&lt;p>At some point, we will get bored with our research topic and find something else interesting. Think twice before switching topics. You must differentiate between your project heading nowhere and you getting tired of being stuck.&lt;/p>
&lt;blockquote>
&lt;p>这是显然的，不过如何选课题也要慎重。目前自己还暂时没有去自主选择一个课题，但是毫无疑问，当你真正开始做这个事的时候，可能是需要投入时间最多的时候。这极大可能决定了之后6-9个月的努力是否有效。&lt;/p>
&lt;/blockquote>
&lt;h3 id="aim-for-top-tier-conferences">Aim for top-tier conferences&lt;/h3>
&lt;p>You should focus on publishing at only top-tier conferences. Don’t consider second-tier venues unless the work has been rejected several times by top-tier conferences. This can prevent you from doing incremental work to make your publication list look better.&lt;/p>
&lt;blockquote>
&lt;p>当你做好准备，并且有合适的老师指导的时候，确实可以把这个当做研究的动机，只瞄准顶级会议，加油。&lt;/p>
&lt;/blockquote>
&lt;h3 id="use-existing-resources-in-your-group">Use existing resources in your group&lt;/h3>
&lt;p>For many fields in computer science, a mature infrastructure requires several years of development by multiple graduate students. Think about how to make use of the infrastructure and resources in the group to boost your research progress.&lt;/p>
&lt;blockquote>
&lt;p>这个其实就很好理解，组里面做硬件很强的学长，以及发过顶会的学长，各个方向的，都是非常宝贵资源。&lt;/p>
&lt;/blockquote>
&lt;h3 id="you-are-powerful">You are powerful&lt;/h3>
&lt;p>Even though we are just junior graduate students, we can have a massive impact on ourselves, our group, and even our department. For example, if there is no reading group for your field in your department, start one!&lt;/p>
&lt;blockquote>
&lt;p>心态调整好&lt;/p>
&lt;/blockquote>
&lt;h3 id="focus-on-publishing">Focus on publishing&lt;/h3>
&lt;p>Needless to say, publications are essential since those are what people look at once we graduate.&lt;/p>
&lt;blockquote>
&lt;p>pub 是一名 phd 唯一要做的事情&lt;/p>
&lt;/blockquote></description></item><item><title>Tips for a New Computer Architecture PhD Student (By Swapnil Haria)</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_tips-for-a-new-computer-architecture-phd-student/</link><pubDate>Sat, 27 Aug 2022 23:57:17 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_tips-for-a-new-computer-architecture-phd-student/</guid><description>&lt;p>原文网址：https://www.sigarch.org/tips-for-a-new-computer-architecture-phd-student/&lt;/p>
&lt;p>作者在 2019 年是 UWM 5 年级 phd，导师是 Mark Hill，毕业后去了 Google 做一名软件开发工程师。&lt;/p>
&lt;p>原文&lt;/p>
&lt;p>I have been fortunate enough to have many helpful senior students and two wonderful advisors to learn from throughout my PhD. Now as a senior PhD student myself, I have identified some lessons that proved most valuable as well as common mistakes made by younger graduate students. While these may be common knowledge, I am documenting these tips here to pass on some of our collectively learned lessons to future graduate students.&lt;/p>
&lt;p>&lt;strong>1. Learn to read a paper efficiently.&lt;/strong>
Fifty years of architecture research has produced a venerable but sizeable collection of research literature, growing by almost 200+ papers a year just from the top four conferences. To make good progress as we wade through the ocean of related work, it is important to read papers efficiently. &lt;a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">Here&lt;/a> is a useful three-pass technique for reading papers efficiently. For me, reading a paper involves finding an answer to these questions (inspired by &lt;a href="http://pages.cs.wisc.edu/~markhill/grant-tips.html">grant writing tips&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>What is the problem that the paper is trying to solve?&lt;/li>
&lt;li>Why is the problem relevant?&lt;/li>
&lt;li>What is the insight that drives the solution proposed by the paper?&lt;/li>
&lt;li>What are the trade-offs of the proposed solution?&lt;/li>
&lt;li>How have the authors evaluated the solution?&lt;/li>
&lt;li>What is one good trait in the paper (problem, solution, evaluation, presentation)?&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>甚至可以把这些问题贴在墙边，读论文的时候才能反复地去思考问题的答案，提升效率。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>2. Know when to stop reading papers.&lt;/strong>
Whenever one begins a new project, it is easy to fall into the trap of endlessly reading paper after paper of related work. While surveying related literature is essential, it tends to yield diminishing returns after a certain point. it is equally important to get one’s feet wet and actually start working on a project. There will be gaps in one’s knowledge but any critical gaps can be filled when that information is actually needed. After all, a graduate student is expected to be an information producer and not just a consumer.&lt;/p>
&lt;blockquote>
&lt;p>这个其实很难。对我自己来说，读了一段时间我就会想去做一些实验，一方面也是一些调剂，一种做同一件事情总会觉得疲劳。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>3. Join or start a reading group.&lt;/strong>
One way of being social is to participate in a reading group, which is a group of students that meet regularly to discuss an interesting paper (old or new) related to a particular area (e.g., architecture). In each meeting, one of the members is responsible for leading the discussion while the others are expected to have read the paper so as to contribute to the discussion. Through reading groups, one is forced to read papers outside of one’s particular niche. Ph.D. graduates often end up working in an area outside of the focus of their thesis. Thus, being exposed to a wide variety of topics is extremely useful. Group discussions also help bring out multiple perspectives into the same paper.&lt;/p>
&lt;blockquote>
&lt;p>这个是一个很有意思的建议，不过推动这件事情通常需要一些有影响力的 host，或者是导师组织的，不知道美国是否会有比较多的自发组织的 reading group&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>4. Use appropriate evaluation methodologies.&lt;/strong>
There is no one size fits all when it comes to &lt;a href="https://www.morganclaypool.com/doi/abs/10.2200/S00273ED1V01Y201006CAC010">evaluation methodologies&lt;/a>. Always select an appropriate methodology by taking into account development time, simulation speed and accuracy of results. There are three broad types of evaluation methodologies. Analytical modelling involves building mathematical models of the area of interest. At early stages of a project, it may be more useful to build simple mathematical models to get a broad sense of the efficacy of a research idea. Popular analytical models include the Roofline Model, Little’s law, Bottleneck analysis and others. Trace-based simulation involves using simulators that read in instruction or memory accesses sequences (traces) to provide reasonable estimates of runtime. Finally, execution-based simulators model hardware behavior at a cycle granularity. While such simulators offer the most accurate results, they also require the most development time as well as simulation time. As such, they may be overkill for simpler experiments. Hence, it is good to be familiar with different evaluation methodologies and use the right one at the right time.&lt;/p>
&lt;blockquote>
&lt;p>paper 看多了之后，基本能知道一些常用的分析方法&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>5. Work on real systems.&lt;/strong>
Although simulators are becoming more sophisticated, there is no substitute for working on real systems. This involves prototyping ideas in the linux kernel, modifying device drivers, building &lt;a href="https://rise.cs.berkeley.edu/projects/firesim/">FPGA-based systems&lt;/a> or &lt;a href="https://mshahrad.github.io/openpiton-asplos16.html">taping out chips&lt;/a>. Looking at real systems gives us an idea of the complexity and robustness of industrial-strength solutions. This also forces us to confront the practical limitations of our ideas. The rise of open-source hardware along with existing open-source software has made it easy to work on real-world &lt;a href="https://www.linux.org/">operating systems&lt;/a>, &lt;a href="https://llvm.org/">compilers&lt;/a>, &lt;a href="https://riscv.org/">CPUs&lt;/a> and &lt;a href="http://miaowgpu.org/">GPUs&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>这一点是我非常赞同的，simulation work 是一件非常危险的事情，有条件的话一定要转向真实的系统&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>6. Spend time improving your programming skills.&lt;/strong>
New computer architecture graduates may not already be good software engineers as computer architecture attracts students with backgrounds in hardware as well. However, PhD-level architecture research typically involves writing a lot of code as part of simulators, operating systems or benchmarks. Hence, it is worth spending some time early in the graduate lifecycle to improve one’s programming skills. Here are some handy resources:
&lt;a href="http://cs-www.cs.yale.edu/homes/aspnes/classes/223/notes.pdf">Data Structures&lt;/a>, &lt;a href="http://bigocheatsheet.com/">Complexity Theory&lt;/a>, &lt;a href="http://pages.cs.wisc.edu/~swapnilh/resources/design-pattern-scard.pdf">Design Patterns&lt;/a>, &lt;a href="http://google.github.io/styleguide/">Code Style&lt;/a>, &lt;a href="http://rogerdudler.github.io/git-guide/">Git/Mercurial&lt;/a>, &lt;a href="http://cslibrary.stanford.edu/102/PointersAndMemory.pdf">Pointers&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>体系结构的学生需要同时具备硬件和软件背景，至少他们必须会使用模拟器，熟悉操作系统，各种 benchmark&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>7. Use microbenchmarks wisely.&lt;/strong>
Microbenchmarks are simple and self-contained programs written to achieve a specific outcome. Microbenchmarks are immensely useful for preliminary exploration as well as sanity checking. An example is a program that can deterministically generate ~100K TLB misses on every execution. Our example microbenchmark can validate a TLB simulator by comparing simulator-reported misses with microbenchmark-generated misses. Being simpler and more easily understood than full-fledged benchmarks, they can be used for initial evaluation to get results that demonstrate general trends. However, microbenchmarks should not be relied upon for any meaningful evaluation, particularly they should not be used as a proxy for real applications.&lt;/p>
&lt;blockquote>
&lt;p>目前自己用的比较少，但这个绝对是体系结构领域非常重要的东西&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>8. Create talks/posters/paper drafts to get early feedback.&lt;/strong>
Once we start plugging away at implementation work, we sometimes lose track of the big research picture and instead simply generate a lot of experimental results. It is important to analyze these results in the context of the overall project. Then, crafting a talk or poster even with little to no results forces us to present our ideas in actual words and images. Different presentation media make us think about our research differently and thus refine our story. By presenting our posters and talks to other researchers, we get timely feedback that can help set the direction of our project.&lt;/p>
&lt;blockquote>
&lt;p>2022-08-27 09:53:50，今天就是 poster talk 的 DDL，只能说自己确实比较拖延，正常来说可能应该找别人讨论一下，模拟一下。确实，沉浸在实验中时，容易丢失 high level 的视野，只能不断提醒自己不要忘记最初的 motivation 和故事的框架&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>9. Care for your mental and physical well-being and maintain a support system.&lt;/strong>
The typical &lt;a href="http://phdcomics.com/comics/archive.php?comicid=125">PhD lifetime&lt;/a> is long years of hard work with a few triumphant occasions such as paper acceptances mixed in. Remember that graduate school is a marathon and not a sprint. Hence, it is important to care for one’s overall health by getting enough sleep, having a healthy diet and spending time on exercise and hobbies. More importantly, graduate school is the first time many of us struggle academically or feel unproductive which may lead to &lt;a href="https://www.theatlantic.com/education/archive/2018/11/anxiety-depression-mental-health-graduate-school/576769/">mental health issues&lt;/a>. Just remember that you are not alone in facing these issues and it is okay to ask for help. Most schools offer free or subsidized &lt;a href="https://www.uhs.wisc.edu/mental-health/">mental health resources&lt;/a>. Furthermore, try to build and subsequently maintain a strong support structure for yourself by nurturing personal relationships with family and friends. Be social with your peers and almost never say no to going out for lunch, attending practice talks and other such activities.&lt;/p>
&lt;blockquote>
&lt;p>毫无疑问，保持身体健康和心理健康非常重要&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>
I thank Prof. Mark Hill, Lena Olson and Prof. Jason Lowe-Power for help improving this document. For teaching me these lessons in the first place, I am grateful to Prof. Mark Hill, Prof. Michael Swift, Prof. David Wood, Lena Olson, Jason Lowe-Power, Nilay Vaish, Jayneel Gandhi, Hongil Yoon, Rathijit Sen, Gokul Ravi, Marc Orr, Muhammad Shoaib, Joel Hestness, Prof. Tony Nowatzki, Newsha Ardalani, Vijay Thiruvengadam, Vinay Gangadhar and many others whom I encountered in graduate school.&lt;/p>
&lt;p>&lt;strong>About the author:&lt;/strong> &lt;a href="http://pages.cs.wisc.edu/~swapnilh/">Swapnil Haria&lt;/a> is a 5th-year PhD Candidate at the University of Wisconsin-Madison, advised by Mark Hill and Michael Swift. His research is focused on hardware and software support for persistent memory.&lt;/p>
&lt;p>&lt;strong>Disclaimer:&lt;/strong> &lt;em>These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are per&lt;/em>&lt;/p></description></item><item><title>What My Mentors Taught Me (By Shan Lu)</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_what-my-mentors-taught-me-by-shan-lu/</link><pubDate>Sat, 27 Aug 2022 23:57:17 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_what-my-mentors-taught-me-by-shan-lu/</guid><description>&lt;h1 id="0-前言">0. 前言&lt;/h1>
&lt;p>视频地址：https://www.youtube.com/watch?v=lOOBJix9-Dw&amp;amp;t=5s&lt;/p>
&lt;p>这是来自卢山老师的 talk&lt;/p>
&lt;p>shan lu 老师目前是芝加哥大学的教授，计算机系统领域的大牛。&lt;/p>
&lt;p>&lt;strong>一定要 open to help，千万不要害羞，千万不要害怕向别人寻求帮助&lt;/strong>&lt;/p>
&lt;p>大牛第一篇 paper 被拒了 5 次，第三年还没有一作 paper。&lt;/p>
&lt;p>&lt;strong>Why did you want to be a phd student&lt;/strong>&lt;/p>
&lt;p>第三年换了一个 topic，投到了 ISCA，也被拒绝了。当时老师觉得是他的 phd 最低谷的时候了。&lt;/p>
&lt;p>他的 phd 导师一直鼓励着他，在低估的时候给予指引，在低估的时候给予 support，真的是一个非常 nice 的事情。&lt;/p>
&lt;p>ISCA 被拒之后中了 ASPLOS，有时候真的很难说。之后几年的 research 都非常顺利。YY 告诉她一定要注意英语。同学写信建议她多 read English book，她花了一年多看完了两本难啃的书。那之后他看英文书变得很快很快。有的时候是 advice 来自不同的地方。&lt;/p>
&lt;p>第四年开始 conference presentation. 一定要去找别的老师，要敢于去找 senior 的老师。一定要告诉别人你在 job market 上面。他说了之后，CMU 老师说他们没有 position，但是可以给她一个 mock interview。&lt;/p>
&lt;p>&lt;strong>只要自己鼓起勇气，很多 senior 的老师会给出很多好的建议&lt;/strong>&lt;/p>
&lt;p>&lt;strong>第一次去要 external letter&lt;/strong>&lt;/p>
&lt;p>别人的回复是 &amp;ldquo;No. I don&amp;rsquo;t know your work well enough.&amp;rdquo; 作者说他已经出汗了，但是我觉得这本身已经是很大的勇气了。&lt;/p>
&lt;p>他咨询的教授非常 straight forward&lt;/p>
&lt;p>&lt;strong>My interview trip&lt;/strong>&lt;/p>
&lt;p>&amp;ldquo;I am surprised how much you have improved&amp;rdquo;&lt;/p>
&lt;p>去 UWM，mark 给了他很多 advice。所以还是一定要 open。&lt;/p>
&lt;p>&lt;strong>Say bye to YY&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Do what you believe in. Be yourself&lt;/li>
&lt;li>Family and Health are two glass balls; career is plastic ball&lt;/li>
&lt;li>Integrity is the most important&lt;/li>
&lt;li>Faculty life is a marathon; Ph.D. life is a sprint&lt;/li>
&lt;li>It is a learning experience&lt;/li>
&lt;/ul>
&lt;p>在不同的环境中也要像不同的人学习&lt;/p>
&lt;p>&lt;strong>My first ASPLOS paper as a faculty&lt;/strong>&lt;/p>
&lt;p>Mark 作为同事仍然在帮助他修改 paper。Mark 从 introduction 中没有读到他的 passion，希望他重新想一下，重新构思这个 story。&lt;/p>
&lt;p>Mark 说他确实有不错的 idea，但是写的不行。&lt;/p>
&lt;p>⭐ Mark 提了一个建议。画一个 2-dimension 图片，以前的 paper 是专注于 Effects 这个维度，我们的工作和他们不一样在哪？This paper 专注于 Causes 这个维度。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Mark 说我来告诉你你的 introduction 每个 paragraph 应该写什么，应该怎么写。而且不能是大概，每一个词都必须要想清楚。&lt;/p>
&lt;blockquote>
&lt;p>这一点也得说一下，有些时候自己也会觉得大概知道了要讲什么，但实际到了真正的细节那里还是一片模糊，这样是不行的。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>My first undergrad. teaching&lt;/strong>&lt;/p>
&lt;p>don&amp;rsquo;t give back, give forward；回馈给下一代&lt;/p>
&lt;p>&lt;strong>My first NSF proposal&lt;/strong>&lt;/p>
&lt;p>&amp;ldquo;You are not a Turing Award Winner, why worrying about rejection&amp;rdquo;&lt;/p>
&lt;p>Mark 仍然在帮助他修改 proposal。&lt;/p>
&lt;p>如果从头再来，他会主动地去找 Mark 帮忙。&lt;/p>
&lt;p>&lt;strong>My student first conference presentation&lt;/strong>&lt;/p>
&lt;p>一定要 seek advice&lt;/p>
&lt;p>写一篇 paper 一定是有 6 months delay，当你 feel good，是因为半年前做得好，而不是现在做得好。当你 feel bad，也不要太 bad，因为收益也是延后的&lt;/p>
&lt;p>他的 co-author 给他反馈了 comment&lt;/p>
&lt;p>&lt;strong>My depression&lt;/strong>&lt;/p>
&lt;p>2011 年有段时间，看起来很累，晚上没法入睡。Andrea 主动邀请他一起上画画课，走出沮丧的情绪。&lt;/p>
&lt;p>&lt;strong>Is my research too incremental&lt;/strong>&lt;/p>
&lt;p>他担心自己的工作过于 incremental。同行告诉不用太担心，还是要不断地去做探索，才能够发掘出新的工作。&lt;/p>
&lt;p>&lt;strong>My first leaving student&lt;/strong>&lt;/p>
&lt;p>他的有一个学生想转学去 CMU。Mark 的建议是要给学生写好的推荐信&lt;/p>
&lt;p>&lt;strong>Service, Service&lt;/strong>&lt;/p>
&lt;p>指的是一些会议的 service，这些东西其实需要去平衡。因为很容易就花费太多时间在上面。&lt;/p>
&lt;p>&lt;strong>提问环境&lt;/strong>&lt;/p>
&lt;p>卢山老师起点很高，怎么去看 2 流会议，或者非 top 会议？&lt;/p>
&lt;ul>
&lt;li>先从最好的 conference 上发表&lt;/li>
&lt;/ul>
&lt;p>如果一篇 paper 被领域内的会议拒稿太多次只有，会做什么&lt;/p>
&lt;ul>
&lt;li>他跟着 YY 做的时候，基本觉得想出一个 idea 都能投到 top 会议。去了 UW 之后，觉得不要有一个特别牛的 idea 才开始做。只要解决了问题，就可以去做一些，因为学生也需要 training，不能总投 top 会议。&lt;/li>
&lt;li>他一开始的时候想了 ASPLOS 的一个点，做着做着就开始拓展了。做完之后觉得好像也成了 top 会议了&lt;/li>
&lt;/ul></description></item><item><title>Hugo 文章页面添加固定目录栏</title><link>https://huweim.github.io/post/blog_hugo_%E6%96%87%E7%AB%A0%E9%A1%B5%E9%9D%A2%E6%B7%BB%E5%8A%A0%E5%9B%BA%E5%AE%9A%E7%9B%AE%E5%BD%95/</link><pubDate>Sun, 14 Aug 2022 19:33:17 +0800</pubDate><guid>https://huweim.github.io/post/blog_hugo_%E6%96%87%E7%AB%A0%E9%A1%B5%E9%9D%A2%E6%B7%BB%E5%8A%A0%E5%9B%BA%E5%AE%9A%E7%9B%AE%E5%BD%95/</guid><description>&lt;h1 id="0-前言">0. 前言&lt;/h1>
&lt;p>阅读博客文章时有一个固定的目录会舒服很多，现在就来探索一下怎么添加这个功能。&lt;/p>
&lt;h1 id="1-固定目录">1. 固定目录&lt;/h1>
&lt;p>以主题 &lt;code>jane&lt;/code> 为例，在文件 &lt;code>theme/jane/layouts/post/single.html&lt;/code> 中存放着如何显示 post。根据文件中的代码，关于 table of content 的设定存放在 &lt;code>theme/jane/layouts/partials/post/toc.html&lt;/code>&lt;/p>
&lt;p>之后把原来的 tableofcontent 那部分代码放到 nav 中即可，如下，注释掉原来的代码（15-17 行）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>{{ if or .Params.toc (and .Site.Params.toc (ne .Params.toc false)) }}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;&lt;span style="color:#f92672">div&lt;/span> &lt;span style="color:#a6e22e">class&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;post-toc&amp;#34;&lt;/span> &lt;span style="color:#a6e22e">id&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;post-toc&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;&lt;span style="color:#f92672">h2&lt;/span> &lt;span style="color:#a6e22e">class&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;post-toc-title&amp;#34;&lt;/span>&amp;gt;{{ i18n &amp;#34;toc&amp;#34; }}&amp;lt;/&lt;span style="color:#f92672">h2&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">&amp;lt;!-- &amp;lt;div class=&amp;#34;post-toc-content&amp;#34;&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> {{.TableOfContents}}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> &amp;lt;/div&amp;gt; --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;&lt;span style="color:#f92672">nav&lt;/span> &lt;span style="color:#a6e22e">class&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;hide-on-mobile section-nav&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;&lt;span style="color:#f92672">h3&lt;/span> &lt;span style="color:#a6e22e">class&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;ml-1&amp;#34;&lt;/span>&amp;gt;Table of contents&amp;lt;/&lt;span style="color:#f92672">h3&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {{ .TableOfContents }}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;/&lt;span style="color:#f92672">nav&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;/&lt;span style="color:#f92672">div&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{- end }}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>PyTorch 学习</title><link>https://huweim.github.io/post/%E6%96%87%E6%A1%A3_pytorch%E5%AD%A6%E4%B9%A0/</link><pubDate>Thu, 28 Jul 2022 16:05:35 +0800</pubDate><guid>https://huweim.github.io/post/%E6%96%87%E6%A1%A3_pytorch%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h2 id="12-tensor">1.2 Tensor&lt;/h2>
&lt;p>张量，多维数组。&lt;/p>
&lt;p>数据类型需要注意一下&lt;/p>
&lt;blockquote>
&lt;p>关于 dtype，PyTorch 提供了 9 种数据类型，共分为 3 大类：float (16-bit, 32-bit, 64-bit)、integer (unsigned-8-bit ,8-bit, 16-bit, 32-bit, 64-bit)、Boolean。模型参数和数据用的最多的类型是 float-32-bit。label 常用的类型是 integer-64-bit。&lt;/p>
&lt;/blockquote>
&lt;h1 id="1-python-模块">1. Python 模块&lt;/h1>
&lt;h2 id="11-parser-模块">1.1 parser 模块&lt;/h2>
&lt;h3 id="111-parseradd_argument">1.1.1 parser.add_argument()&lt;/h3>
&lt;p>在命令行给代码赋值，不需要反复在 python 中修改代码。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parser&lt;span style="color:#f92672">.&lt;/span>add_argument(&lt;span style="color:#e6db74">&amp;#39;--file-dir&amp;#39;&lt;/span>,type&lt;span style="color:#f92672">=&lt;/span>str, required&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,help&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Input file directory&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 实例&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parser&lt;span style="color:#f92672">.&lt;/span>add_argument(&lt;span style="color:#e6db74">&amp;#39;--dataset&amp;#39;&lt;/span>, default&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;cifar10&amp;#39;&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>str,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> help&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;dataset name&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parser&lt;span style="color:#f92672">.&lt;/span>add_argument(&lt;span style="color:#e6db74">&amp;#39;--dataset_path&amp;#39;&lt;/span>, default&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;/state/partition/imagenet-raw-data&amp;#39;&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>str,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> help&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;dataset path&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parser&lt;span style="color:#f92672">.&lt;/span>add_argument(&lt;span style="color:#e6db74">&amp;#39;--model&amp;#39;&lt;/span>, default&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;resnet18&amp;#39;&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>str,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> help&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;model name&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parser&lt;span style="color:#f92672">.&lt;/span>add_argument(&lt;span style="color:#e6db74">&amp;#39;--train&amp;#39;&lt;/span>, default&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>, action&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;store_true&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> help&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;train&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>action&lt;/code>: &lt;code>-train&lt;/code> 设置成一个开关，&lt;/p>
&lt;ul>
&lt;li>如果使用了 &lt;code>python -u -m --train ...&lt;/code>，就会把参数 &lt;code>--train&lt;/code> 设置为 True&lt;/li>
&lt;li>&lt;code>python -u -m ...&lt;/code>，没有这个开关，则参数存储为 False&lt;/li>
&lt;/ul>
&lt;h3 id="112-parserparse_args">1.1.2 parser.parse_args()&lt;/h3>
&lt;h1 id="2-torch">2. torch&lt;/h1>
&lt;p>有很多方便的数学操作，同理，先了解有这个东西，需要用到时看具体的用法。包括 torch.rand(), torch.range(), torch.chunk(), torch.normal(), torch.add()&lt;/p>
&lt;p>pytorch 主要分为五大模块&lt;/p>
&lt;ul>
&lt;li>dataset&lt;/li>
&lt;li>model&lt;/li>
&lt;li>loss funtion&lt;/li>
&lt;li>optimizer&lt;/li>
&lt;li>迭代训练&lt;/li>
&lt;/ul>
&lt;h2 id="21-nn">2.1 nn&lt;/h2>
&lt;p>&lt;code>torch.nn&lt;/code> 主要包含 4 个模块&lt;/p>
&lt;ul>
&lt;li>nn.Parameter, Tensor 子类，表示可学习的参数，如 weights, bias&lt;/li>
&lt;li>nn.Modules, 所有模型的基类，用于管理网络的属性&lt;/li>
&lt;li>nn.functional, 函数具体实现，如 conv, pool, 激活函数&lt;/li>
&lt;li>nn,init, 网络参数初始化方法&lt;/li>
&lt;/ul>
&lt;h3 id="211-nnmodule">2.1.1 nn.Module&lt;/h3>
&lt;p>class torch.nn.Module 是所有网络的基类（Base class for all neural network modules），每个模型都应该继承这个类，参考lab1的网络模型&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Net&lt;/span>(nn&lt;span style="color:#f92672">.&lt;/span>Module):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> super(Net, self)&lt;span style="color:#f92672">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv1 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Conv2d(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">6&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, bias&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>pool &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>MaxPool2d(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>) &lt;span style="color:#75715e"># run after each conv (hence the 5x5 FC layer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv2 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Conv2d(&lt;span style="color:#ae81ff">6&lt;/span>, &lt;span style="color:#ae81ff">16&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, bias&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc1 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Linear(&lt;span style="color:#ae81ff">16&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>, &lt;span style="color:#ae81ff">120&lt;/span>, bias&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc2 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Linear(&lt;span style="color:#ae81ff">120&lt;/span>, &lt;span style="color:#ae81ff">84&lt;/span>, bias&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc3 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Linear(&lt;span style="color:#ae81ff">84&lt;/span>, &lt;span style="color:#ae81ff">10&lt;/span>, bias&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, x: torch&lt;span style="color:#f92672">.&lt;/span>Tensor) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>Tensor:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>pool(F&lt;span style="color:#f92672">.&lt;/span>relu(self&lt;span style="color:#f92672">.&lt;/span>conv1(x)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>pool(F&lt;span style="color:#f92672">.&lt;/span>relu(self&lt;span style="color:#f92672">.&lt;/span>conv2(x)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> x&lt;span style="color:#f92672">.&lt;/span>view(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">16&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> F&lt;span style="color:#f92672">.&lt;/span>relu(self&lt;span style="color:#f92672">.&lt;/span>fc1(x)) &lt;span style="color:#75715e">#输入是列向量&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> F&lt;span style="color:#f92672">.&lt;/span>relu(self&lt;span style="color:#f92672">.&lt;/span>fc2(x))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>fc3(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>net &lt;span style="color:#f92672">=&lt;/span> Net()&lt;span style="color:#f92672">.&lt;/span>to(device)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>一般在 model.py 文件中定义 NN model，再举一个 ViT 的例子&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">ViT&lt;/span>(nn&lt;span style="color:#f92672">.&lt;/span>Module):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> name (str): Model name, e.g. &amp;#39;B_16&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> pretrained (bool): Load pretrained weights
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> in_channels (int): Number of channels in input data
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> num_classes (int): Number of classes, default 1000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> References:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> [1] https://openreview.net/forum?id=YicbFdNTTy
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: Optional[str] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pretrained: bool &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> patches: int &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">16&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dim: int &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">768&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ff_dim: int &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">3072&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_heads: int &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">12&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_layers: int &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">12&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> attention_dropout_rate: float &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.0&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dropout_rate: float &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.1&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> representation_size: Optional[int] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> load_repr_layer: bool &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> classifier: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;token&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> positional_embedding: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;1d&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> in_channels: int &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">3&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image_size: Optional[int] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_classes: Optional[int] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> super()&lt;span style="color:#f92672">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="2111-一些子函数">2.1.1.1 一些子函数&lt;/h4>
&lt;p>2022-08-06 22:31:40&lt;/p>
&lt;p>named_ 系列&lt;/p>
&lt;p>&lt;strong>model.named_parameters()&lt;/strong>，返回两个变量，比如赋值给 name(e.g. &lt;code>name&lt;/code> -&amp;gt; &lt;code>stage_1.0.conv_b.weight&lt;/code>) 和 param (e.g. &lt;code>param.requires_grad&lt;/code> -&amp;gt; &lt;code>False&lt;/code>)&lt;/p>
&lt;h3 id="212-nnlayer">2.1.2 nn.Layer&lt;/h3>
&lt;p>&lt;strong>model.named_modules()&lt;/strong>，返回所有模块的迭代器。打印的话会输出模型的结构，如同 &lt;code>print(model)&lt;/code>&lt;/p>
&lt;p>&lt;strong>model.named_children&lt;/strong>，named_modules 的子集，返回子模块的迭代器&lt;/p>
&lt;p>&lt;strong>model.children()&lt;/strong>，返回下一级模块的迭代器&lt;/p>
&lt;blockquote>
&lt;p>所以这个只是访问到一级，如果下一级是一个 Sequential，那么就还得继续迭代，这个时候用 .modules() 可能会更好&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>model.modules()&lt;/strong>，Returns an iterator over all modules in the network&lt;/p>
&lt;p>model.named_modules() 会有冗余的返回，这种情况下需要结合一些函数来过滤。&lt;/p>
&lt;p>nn 还包含了很多 layer，比如 &lt;code>nn.Conv2d&lt;/code>, &lt;code>nn.MaxPool1d&lt;/code>, &lt;code>nn.ReLU&lt;/code>&lt;/p>
&lt;h3 id="213-model-的创建">2.1.3 model 的创建&lt;/h3>
&lt;p>主要是 2 个要素，构建子模块和拼接子模块，把子模块理解为 layer，构建子模块就是 &lt;code>__init__&lt;/code>，拼接子模块就是 &lt;code>forward()&lt;/code>&lt;/p>
&lt;ul>
&lt;li>调用 &lt;code>model = ViT(model_name, pretrained=True)&lt;/code> 创建模型时，会调用 &lt;code>__init__()&lt;/code> 方法创建模型的子模块&lt;/li>
&lt;li>训练时调用 &lt;code>outputs = net(inputs)&lt;/code> 时，会进入 &lt;code>module.py&lt;/code> 的 &lt;code>call()&lt;/code> 函数中&lt;/li>
&lt;li>在 &lt;code>__call__&lt;/code> 中调用 &lt;code>result = self.forward(*input, **kwargs)&lt;/code> 函数，进入到模型的 &lt;code>forward()&lt;/code> 函数中，进行前向传播&lt;/li>
&lt;/ul>
&lt;h4 id="2131-model-实例">2.1.3.1 model() 实例&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>model &lt;span style="color:#f92672">=&lt;/span> quantize_model(model)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>outputs &lt;span style="color:#f92672">=&lt;/span> model(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#f92672">=&lt;/span> criterion(outputs, targets)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在 &lt;code>outputs = model(inputs)&lt;/code> 语句中会进入到 &lt;code>class Conv2dQuantizer(nn.Module)&lt;/code> 的 forward 函数，&lt;code>super(Conv2dQuantizer, self).__init__()&lt;/code> 是继承父类的构造函数 &lt;code>__init__()&lt;/code>，从而使得 Conv2dQuantizer 中包含了父类&lt;/p>
&lt;h4 id="2132-modeleval">2.1.3.2 model.eval()&lt;/h4>
&lt;p>作用：不启动 BatchNormalization 和 Dropout，保证 BN 和 Dropout 不发生变化，pytorch 框架会自动把 BN 和 Dropout 固定住，不会取平均值，而是用训练好的值，不然的话，一旦 test 的 batch_size 过小，很容易就会被 BN 层导致生成图片颜色失真极大。&lt;/p>
&lt;p>Reference: &lt;a href="https://zhuanlan.zhihu.com/p/357075502">https://zhuanlan.zhihu.com/p/357075502&lt;/a>&lt;/p>
&lt;h4 id="2133-torchno_grad">2.1.3.3 torch.no_grad()&lt;/h4>
&lt;p>tensor 有一个参数是 &lt;code>requires_grad&lt;/code>，如果设置为 True，则反向传播时该 tensor 会自动求导，默认为 False，反向传播时不求导，可以极大地节约显存或者内存。&lt;/p>
&lt;p>&lt;code>with torch.no_grad&lt;/code> 的作用：所有计算得出的 tensor 的 requires_grad 都自动设置为 False&lt;/p>
&lt;h3 id="214-crossentropyloss">2.1.4 CrossEntropyLoss&lt;/h3>
&lt;p>This criterion combines LogSoftmax and NLLLoss in one single class.&lt;/p>
&lt;h2 id="22-tensor">2.2 Tensor&lt;/h2>
&lt;p>&lt;code>Tensor&lt;/code> is a multi-dimensional matrix containing elements of a single data type&lt;/p>
&lt;p>可以用 list 作为参数来构造 tensor&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>tensor([[&lt;span style="color:#ae81ff">1.&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1.&lt;/span>], [&lt;span style="color:#ae81ff">1.&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1.&lt;/span>]])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tensor([[ &lt;span style="color:#ae81ff">1.0000&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1.0000&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [ &lt;span style="color:#ae81ff">1.0000&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1.0000&lt;/span>]])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>tensor(np&lt;span style="color:#f92672">.&lt;/span>array([[&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>], [&lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, &lt;span style="color:#ae81ff">6&lt;/span>]]))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tensor([[ &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [ &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, &lt;span style="color:#ae81ff">6&lt;/span>]])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="23-autograd">2.3 autograd&lt;/h2>
&lt;p>weight 更新依赖于梯度的计算，在 pytorch 中搭建好 forward 计算图，利用 &lt;code>torch.autograd&lt;/code> 自动求导得到所有 gradient of tensor&lt;/p>
&lt;h2 id="24-data-模块">2.4 data 模块&lt;/h2>
&lt;p>数据模块可以细分为 4 个部分&lt;/p>
&lt;ul>
&lt;li>数据收集：样本，label&lt;/li>
&lt;li>数据划分：train set, valid set, test set&lt;/li>
&lt;li>数据读取：pytorch dataloader 模块，dataloader 包括 sampler, dataset
&lt;ul>
&lt;li>sampler: 生成索引(index)&lt;/li>
&lt;li>dataset: 根据生成的索引(index)读取样本以及标签(label)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>数据预处理：对应于 pytorch transforms&lt;/li>
&lt;/ul>
&lt;h3 id="241-dataloader">2.4.1 DataLoader&lt;/h3>
&lt;p>&lt;code>torch.utils.data.DataLoader()&lt;/code>, 构建可迭代的数据装载器&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>torch&lt;span style="color:#f92672">.&lt;/span>utils&lt;span style="color:#f92672">.&lt;/span>data&lt;span style="color:#f92672">.&lt;/span>DataLoader(dataset, batch_size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, shuffle&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>, sampler&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>, batch_sampler&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>, num_workers&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, collate_fn&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>, pin_memory&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>, drop_last&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>, timeout&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, worker_init_fn&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>, multiprocessing_context&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>dataset: torchvision.datasets 类，决定数据从哪里读取，如何读取，以及是否下载，是否训练，给出一个例子&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>cifar100_test &lt;span style="color:#f92672">=&lt;/span> torchvision&lt;span style="color:#f92672">.&lt;/span>datasets&lt;span style="color:#f92672">.&lt;/span>CIFAR100(root&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;./data&amp;#39;&lt;/span>, train&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>, download&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, transform&lt;span style="color:#f92672">=&lt;/span>transform_test)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>num_works: 是否多进程读取，指定读取的进程数量&lt;/li>
&lt;li>shuffle: 每个 epoch 是否乱序&lt;/li>
&lt;li>sampler: 指定一个 &lt;code>torch.utils.data.distributed.DistributedSampler&lt;/code> 类型&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>其他名词&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Epoch: 所有训练样本都已经输入到模型中，称为一个 epoch&lt;/li>
&lt;li>Iteration: a batch of 样本已经输入到模型中&lt;/li>
&lt;li>Batchsize: 批大小，决定一个 iteration 有多少样本，也决定了一个 Epoch 有多少个 Iteration&lt;/li>
&lt;/ul>
&lt;h4 id="2411-nvidiadali">2.4.1.1 NVIDIA.DALI&lt;/h4>
&lt;p>2022-07-28 21:24:17 了解到这玩意儿&lt;/p>
&lt;h3 id="242-dataset">2.4.2 DataSet&lt;/h3>
&lt;p>&lt;code>torch.utils.data.Dataset&lt;/code>, 抽象类，所有自定义大的 DataSet 都需要继承该类。&lt;/p>
&lt;p>在Dataset 的初始化函数中会调用 &lt;code>get_img_info()&lt;/code> 方法。&lt;/p>
&lt;h3 id="242-torchvision">2.4.2 torchvision&lt;/h3>
&lt;p>计算机视觉工具包，有 3 个主要的模块&lt;/p>
&lt;ul>
&lt;li>&lt;code>torchvision.transforms&lt;/code>, 包括常用的图像预处理方法&lt;/li>
&lt;li>&lt;code>torchvision.datasets&lt;/code>, 包括常用的 dataset, e.g. MNIST, CIFAR-10, ImageNet&lt;/li>
&lt;li>&lt;code>torchvision.models&lt;/code>, 常用的 pre-trained models, e.g. AlexNet, VGG, ResNet, GoogleNet&lt;/li>
&lt;/ul>
&lt;p>data 的数量和分布对模型训练的结果起决定性的作用，需要对 data 进行 pre-process 和数据增强。目的是增加数据的多样性，提高模型的泛化能力。&lt;/p>
&lt;h3 id="243-batch-size">2.4.3 Batch Size&lt;/h3>
&lt;p>2022-08-09 18:24:56，学习一下梯度，训练，和 batch size 的关系。&lt;/p>
&lt;p>通过举例来学习，比如目前使用的网络，batch_size = 128，训练数据行数为 $|x| = 1024$，代表每次网络模型的迭代使用了 128 个样本，128 个样本来自 $x$，可能是无序抽样，也可能是有序抽样。&lt;/p>
&lt;p>每个 epoch 包含 1024/128 iterations&lt;/p>
&lt;blockquote>
&lt;p>同一个 epoch，我用第一个 batch 完成了一次前向反向，接下来的第二次 iteration，换了一个 batch，但是 weight 已经更新过了
这个就是之前卡住我的点，要理解这一点。训练是为了让 weight 收敛。&lt;/p>
&lt;/blockquote>
&lt;h4 id="2431-epoch-bach-iteration">2.4.3.1 epoch, bach, iteration&lt;/h4>
&lt;p>epoch: 把所有训练数据丢进网络的周期&lt;/p>
&lt;p>batch_size: 一次迭代的数据量；这个从一些说法中，看起来是图片的张数&lt;/p>
&lt;p>iteration: 完成所有训练数据的迭代，所需要的次数。&lt;/p>
&lt;blockquote>
&lt;p>batch_size = 128，训练数据行数为 $|x| = 1024$，代表每次网络模型的迭代使用了 128 个样本，128 个样本来自 $x$，可能是无序抽样，也可能是有序抽样。
每个 epoch 包含 1024/128 iterations&lt;/p>
&lt;/blockquote>
&lt;p>⭐ 注意，第一个 epoch 结束之后，weight 是没有 reset 的，也就是说第二个 epoch 仍然接着更新 weight。&lt;/p>
&lt;blockquote>
&lt;p>epoch，背诵词典次数多了，就记牢了。当然，也有可能背傻了（过拟合）&lt;/p>
&lt;/blockquote>
&lt;h2 id="25-模型训练">2.5 模型训练&lt;/h2>
&lt;h3 id="251-损失函数">2.5.1 损失函数&lt;/h3>
&lt;p>Loss Function, 衡量模型输出与真实标签之间的差异，也就是 &lt;strong>一个&lt;/strong> 样本的 output 和真实标签（label）的差异&lt;/p>
&lt;p>Cost Function, 计算整个样本集的 output 和真实标签（label）的差异&lt;/p>
&lt;p>pytorch 中的损失函数也是继承于 &lt;code>nn.Module&lt;/code>&lt;/p>
&lt;h3 id="252-optimizer">2.5.2 optimizer&lt;/h3>
&lt;p>PyTorch 中的优化器是用于管理并更新模型中 &lt;strong>可学习参数的值&lt;/strong>，使得模型输出更加接近真实标签。&lt;/p>
&lt;h4 id="2521-属性">2.5.2.1 属性&lt;/h4>
&lt;ul>
&lt;li>defaults: 优化器的超参数，如 weight_decay, momentum&lt;/li>
&lt;li>state: 参数的缓存，如 momentum 中需要用到前几次的梯度，缓存在这个变量中&lt;/li>
&lt;li>param_groups: 管理的参数组，是一个 list，其中每个元素是 dict，包括 momentum, lr, weight_decay, params&lt;/li>
&lt;li>_step_count: 记录更新次数，在学习率调整中使用&lt;/li>
&lt;/ul>
&lt;h4 id="2522-optimizer-方法">2.5.2.2 optimizer 方法&lt;/h4>
&lt;ul>
&lt;li>zero_grad(): 清空所管理参数的梯度。因为 pytorch 张量的梯度不会自动清零，因此每次反向传播之后都需要清空梯度&lt;/li>
&lt;li>step(): 执行一步梯度更新&lt;/li>
&lt;li>add_param_group(): 添加参数组&lt;/li>
&lt;li>state_dict(): 获取优化器当前状态&lt;/li>
&lt;li>load_state_dict(): 加载状态信息的 dict&lt;/li>
&lt;/ul>
&lt;h4 id="2523-learning-rate">2.5.2.3 learning rate&lt;/h4>
&lt;p>learning rate, 影响 loss function 收敛的重要因素，控制了梯度下降更新的步伐&lt;/p>
&lt;h2 id="26-regularization-正则化">2.6 Regularization 正则化&lt;/h2>
&lt;p>正则化是一种减少方差的策略&lt;/p>
&lt;h3 id="261-weight-decay">2.6.1 weight decay&lt;/h3>
&lt;p>weight decay 是优化器中的一个参数，在执行 &lt;code>optim_wdecay_step()&lt;/code> 时，会计算 weight decay 后的梯度&lt;/p>
&lt;h3 id="262-dropout">2.6.2 Dropout&lt;/h3>
&lt;p>一种抑制过拟合的方法。理解为放缩数据&lt;/p>
&lt;h3 id="263-normalization">2.6.3 Normalization&lt;/h3>
&lt;p>Batch Normalization, 经过 normalization 后的数据服从 $N(0, 1)$ 分布，有如下优点&lt;/p>
&lt;ul>
&lt;li>可以使用更大的 lr，加速模型收敛&lt;/li>
&lt;li>可以不用精心设计 weight 初始化&lt;/li>
&lt;li>可以不用 dropout 或者较小的 dropout&lt;/li>
&lt;li>可以不用 L2 或者较小的 weight decay&lt;/li>
&lt;li>可以不用 LRN (Local Response Normalization)&lt;/li>
&lt;/ul>
&lt;h2 id="27-model-相关的操作">2.7 Model 相关的操作&lt;/h2>
&lt;h3 id="271-torchsave">2.7.1 torch.save&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>torch&lt;span style="color:#f92672">.&lt;/span>save(obj, f, pickle_module, pickle_protocol&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, _use_new_zipfile_serialization&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>obj 是保存的对象，f 是输出路径。还有 2 种方式&lt;/p>
&lt;ul>
&lt;li>保存整个 Module, &lt;code>torch.savev(net, path)&lt;/code> 这种方法比较耗时，保存的文件比较大&lt;/li>
&lt;li>只保存模型的参数，&lt;code>torch.savev(state_sict, path)&lt;/code>，推荐，保存的文件比较小&lt;/li>
&lt;/ul>
&lt;h3 id="272-torchload">2.7.2 torch.load&lt;/h3>
&lt;p>对应于 save&lt;/p>
&lt;h3 id="273-fine-tuning">2.7.3 Fine-tuning&lt;/h3>
&lt;p>一种迁移学习的方法，比如在人脸识别应用中，ImageNet 作为 source domain，人脸数据作为 target domain。通常 source domain 比 target domain 大很多，可以利用 ImageNet 训练好的网络应用到人脸识别中。&lt;/p>
&lt;p>&lt;strong>理解&lt;/strong>
对于一个模型，可以分为流程在前面的 feature extractor （conv 层） 和后面的 classifier。fine-tune 通常不改变 feature extractor 的 weight，也就是冻结 conv 层；改变最后一个 fc layer 的输出来适应目标任务，训练后面 classifier 的 weight。&lt;/p>
&lt;p>通常 target domain 的数据比较小，不足以训练全部参数，容易导致过拟合，因此不改变 feature extractor 的 weight。&lt;/p>
&lt;p>&lt;strong>Step&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>获取 pre-trained model 参数&lt;/li>
&lt;li>&lt;code>load_state_dict()&lt;/code> 把参数加载到模型中&lt;/li>
&lt;li>修改输出层&lt;/li>
&lt;li>固定 feature extractor 的参数，通常有 2 种做法
&lt;ul>
&lt;li>固定 conv 层的预训练参数。可以设置 &lt;code>requires_grad = False&lt;/code> 或者 &lt;code>lr = 0&lt;/code>&lt;/li>
&lt;li>通过 &lt;code>params_group&lt;/code> 给 feature extractor 设置一个较小的 lr&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3-pytorch-分布式训练">3. pytorch 分布式训练&lt;/h1>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>dist&lt;span style="color:#f92672">.&lt;/span>init_process_group(backend&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;nccl&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># backend是后台利用nccl进行通信&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>调试时报错，如何在调试分布式训练的模型&lt;/p>
&lt;h2 id="31-debug-分布式训练的模型">3.1 debug 分布式训练的模型&lt;/h2>
&lt;p>修改 python &lt;code>launch.json&lt;/code> 文件，把 program 换成 torch.distribution 的 launch.py&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">//&lt;/span> Use IntelliSense to learn about possible attributes&lt;span style="color:#f92672">.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">//&lt;/span> Hover to view descriptions of existing attributes&lt;span style="color:#f92672">.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">//&lt;/span> For more information, visit: https:&lt;span style="color:#f92672">//&lt;/span>go&lt;span style="color:#f92672">.&lt;/span>microsoft&lt;span style="color:#f92672">.&lt;/span>com&lt;span style="color:#f92672">/&lt;/span>fwlink&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">?&lt;/span>linkid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">830387&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0.2.0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;configurations&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Python: Current File&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;python&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;request&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;launch&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;program&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/nvme/wmhu/anaconda3/envs/ant/lib/python3.8/site-packages/torch/distributed/launch.py&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;console&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;integratedTerminal&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;justMyCode&amp;#34;&lt;/span>: true,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;args&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--nproc_per_node=1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/nvme/wmhu/work/ANT/ImageNet/main.py&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--dataset=imagenet&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--model=vit_b_16&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--dataset_path=/nvme/imagenet&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--epoch=4&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--mode=int&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--wbit=4&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;--abit=4&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;env&amp;#34;&lt;/span>:{&lt;span style="color:#e6db74">&amp;#34;CUDA_VISIBLE_DIVICES&amp;#34;&lt;/span>:&lt;span style="color:#e6db74">&amp;#34;0&amp;#34;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>注意 &lt;code>nproc_per_node&lt;/code> 是 Python 自带的参数，因此可以写到里面，对于 &lt;code>--dataset=imagenet&lt;/code> 会有报错。&lt;/p>
&lt;p>&lt;code>--dataset=imagenet&lt;/code> 要放到 &lt;code>main.py&lt;/code> 的后面，这个方法相当于用调整参数的形式来达到目的。不太有通用性，相当于专门改了一个 &lt;code>launch.json&lt;/code> 文件。&lt;/p>
&lt;h1 id="4-hook">4. Hook&lt;/h1>
&lt;p>这个功能被广泛用于可视化神经网络中间层的 feature、gradient，从而诊断神经网络中可能出现的问题，分析网络有效性。&lt;/p>
&lt;p>视频：https://www.youtube.com/watch?v=syLFCVYua6Q&lt;/p>
&lt;p>pytorch 计算图似乎只会保留叶子节点的梯度，舍弃中间的梯度&lt;/p>
&lt;blockquote>
&lt;p>简而言之，register_hook的作用是，反向传播时，除了完成原有的反传，额外多完成一些任务。你可以定义一个中间变量的hook，将它的grad值打印出来，当然你也可以定义一个全局列表，将每次的grad值添加到里面去。&lt;/p>
&lt;/blockquote>
&lt;p>什么是中间变量：有的博客里有提到，似乎是没有直接指定数值，而是通过计算得到的变量。比如下面的 z 就是中间变量。z 的梯度是不会保存的&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>x &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>Tensor([&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>])&lt;span style="color:#f92672">.&lt;/span>requires_grad_()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>Tensor([&lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, &lt;span style="color:#ae81ff">6&lt;/span>, &lt;span style="color:#ae81ff">7&lt;/span>])&lt;span style="color:#f92672">.&lt;/span>requires_grad_()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>z &lt;span style="color:#f92672">=&lt;/span> x &lt;span style="color:#f92672">+&lt;/span> y &lt;span style="color:#75715e"># 中间变量&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>output &lt;span style="color:#f92672">=&lt;/span> model(input)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 此时会做几件事，一个是调用 forward 方法计算结果，一个是判断有没有注册 forward_hook，有的话就将 forward 的输入及结果作为 hook 的实参&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="41-register_hook">4.1 register_hook&lt;/h2>
&lt;p>&lt;code>z.register_hook(hook_fn)&lt;/code>，这个 hook_fn 是一个用户自定义函数，返回 Tensor （如果需要对 grad 进行修改）或者 None（用于直接打印，不修改），所以直接用 lambda 函数即可，&lt;code>z.register_hook(lambda grad: print(grad))&lt;/code>&lt;/p>
&lt;blockquote>
&lt;p>个人理解下来，register_hook 可以实现保留中间变量梯度的功能，而且不像 &lt;code>retain_grad&lt;/code> 那样会带来很大的开销&lt;/p>
&lt;/blockquote>
&lt;h3 id="411-register_forward_hook">4.1.1 register_forward_hook&lt;/h3>
&lt;p>&lt;strong>作用&lt;/strong>：获取中间层的 feature map&lt;/p>
&lt;p>通常，pytorch 只提供了网络整体的输入和输出，对于夹在网络中间的模块，很难获得他的输入输出。除非设计网络时，在 forward 函数的返回值中包含中间 module 的输出。总而言之别的方法都比较麻烦，pytorch 设计好了 register_forward_hook 和 register_backward_hook。&lt;/p>
&lt;p>相比针对 tensor 的 register_hook，这个 forward hook 没有返回值，也就是不能改变输入，只能打印。&lt;/p>
&lt;p>代码实例，讲的非常清晰，来自&lt;a href="https://cloud.tencent.com/developer/article/1475430">博客&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>Class Model(nn&lt;span style="color:#f92672">.&lt;/span>Module):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 全局变量，用于存储中间层的 feature&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total_feature_out &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total_feature_in &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>model &lt;span style="color:#f92672">=&lt;/span> Model()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 定义 forward hook function&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">hook_fn_forward&lt;/span>(module, input, output):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(module) &lt;span style="color:#75715e"># 用于区分模块&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;input&amp;#39;&lt;/span>, input) &lt;span style="color:#75715e"># 首先打印出来&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;output&amp;#39;&lt;/span>, output)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> total_feature_out&lt;span style="color:#f92672">.&lt;/span>append(output) &lt;span style="color:#75715e"># 然后分别存入全局 list 中&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> total_feature_in&lt;span style="color:#f92672">.&lt;/span>append(input)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 给每个 module 都装上 hook&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> name, module &lt;span style="color:#f92672">in&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>named_children():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> module&lt;span style="color:#f92672">.&lt;/span>register_forward_hook(hook_fn_forward)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 前向传播和回传&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>x &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>Tensor([[&lt;span style="color:#ae81ff">1.0&lt;/span>, &lt;span style="color:#ae81ff">1.0&lt;/span>, &lt;span style="color:#ae81ff">1.0&lt;/span>]])&lt;span style="color:#f92672">.&lt;/span>requires_grad_()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>o &lt;span style="color:#f92672">=&lt;/span> model(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>o&lt;span style="color:#f92672">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(&lt;span style="color:#e6db74">&amp;#39;==========Saved inputs and outputs==========&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> idx &lt;span style="color:#f92672">in&lt;/span> range(len(total_feature_in)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;input: &amp;#39;&lt;/span>, total_feature_out[idx])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;output: &amp;#39;&lt;/span>, total_feature_out[idx])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="412-register_backward_hook">4.1.2 register_backward_hook()&lt;/h3>
&lt;h4 id="4121-使用方法和示例">4.1.2.1 使用方法和示例&lt;/h4>
&lt;p>&lt;strong>作用&lt;/strong>：用于获取梯度&lt;/p>
&lt;p>&lt;strong>使用&lt;/strong>：&lt;code>module.register_backward_hook(hook_fn)&lt;/code>, &lt;code>hook_fn(module, grad_input, grad_output) -&amp;gt; Tensor or None&lt;/code>&lt;/p>
&lt;p>如果有多个输入或者输出，grad_input, grad_output 可以是 tuple 类型。比如对于线性模块，grad_input 是一个三元组，分别是 $g_{bias}$, $g_x$, $g_W$，对 bias 的导数，对 x 的导数以及对 weight 的导数。&lt;/p>
&lt;p>直接看代码，这个代码是可以直接跑的，不得不说写的确实很好。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> torch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> torch &lt;span style="color:#f92672">import&lt;/span> nn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Model&lt;/span>(nn&lt;span style="color:#f92672">.&lt;/span>Module):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> super(Model, self)&lt;span style="color:#f92672">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc1 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Linear(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>relu1 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>ReLU()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc2 &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Linear(&lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>initialize()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">initialize&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc1&lt;span style="color:#f92672">.&lt;/span>weight &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>nn&lt;span style="color:#f92672">.&lt;/span>Parameter(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch&lt;span style="color:#f92672">.&lt;/span>Tensor([[&lt;span style="color:#ae81ff">1.&lt;/span>, &lt;span style="color:#ae81ff">2.&lt;/span>, &lt;span style="color:#ae81ff">3.&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">4.&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">5.&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">6.&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ae81ff">7.&lt;/span>, &lt;span style="color:#ae81ff">8.&lt;/span>, &lt;span style="color:#ae81ff">9.&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">10.&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">11.&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">12.&lt;/span>]]))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc1&lt;span style="color:#f92672">.&lt;/span>bias &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>nn&lt;span style="color:#f92672">.&lt;/span>Parameter(torch&lt;span style="color:#f92672">.&lt;/span>Tensor([&lt;span style="color:#ae81ff">1.0&lt;/span>, &lt;span style="color:#ae81ff">2.0&lt;/span>, &lt;span style="color:#ae81ff">3.0&lt;/span>, &lt;span style="color:#ae81ff">4.0&lt;/span>]))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc2&lt;span style="color:#f92672">.&lt;/span>weight &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>nn&lt;span style="color:#f92672">.&lt;/span>Parameter(torch&lt;span style="color:#f92672">.&lt;/span>Tensor([[&lt;span style="color:#ae81ff">1.0&lt;/span>, &lt;span style="color:#ae81ff">2.0&lt;/span>, &lt;span style="color:#ae81ff">3.0&lt;/span>, &lt;span style="color:#ae81ff">4.0&lt;/span>]]))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>fc2&lt;span style="color:#f92672">.&lt;/span>bias &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>nn&lt;span style="color:#f92672">.&lt;/span>Parameter(torch&lt;span style="color:#f92672">.&lt;/span>Tensor([&lt;span style="color:#ae81ff">1.0&lt;/span>]))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> o &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>fc1(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> o &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>relu1(o)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> o &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>fc2(o)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> o
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 全局变量，用于存储中间层的 feature&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total_grad_out &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total_grad_in &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>model &lt;span style="color:#f92672">=&lt;/span> Model()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 定义 forward hook function&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">hook_fn_backward&lt;/span>(module, grad_input, grad_output):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(module) &lt;span style="color:#75715e"># 用于区分模块&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 为了符合反向传播顺序，先打印 grad_output&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;grad_output&amp;#39;&lt;/span>, grad_output)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;grad_input&amp;#39;&lt;/span>, grad_input)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> total_grad_out&lt;span style="color:#f92672">.&lt;/span>append(grad_output)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> total_grad_in&lt;span style="color:#f92672">.&lt;/span>append(grad_input)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 给每个 module 都装上 hook&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> name, module &lt;span style="color:#f92672">in&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>named_children():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> module&lt;span style="color:#f92672">.&lt;/span>register_backward_hook(hook_fn_backward)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 前向传播和回传&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 这里的 requires_grad 很重要，如果不加，backward hook&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 执行到第一层，对 x 的导数将为 None，某英文博客作者这里疏忽了&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 此外再强调一遍 x 的维度，一定不能写成 torch.Tensor([1.0, 1.0, 1.0]).requires_grad_()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 否则 backward hook 会出问题。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>x &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>Tensor([[&lt;span style="color:#ae81ff">1.0&lt;/span>, &lt;span style="color:#ae81ff">1.0&lt;/span>, &lt;span style="color:#ae81ff">1.0&lt;/span>]])&lt;span style="color:#f92672">.&lt;/span>requires_grad_()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>o &lt;span style="color:#f92672">=&lt;/span> model(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>o&lt;span style="color:#f92672">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(&lt;span style="color:#e6db74">&amp;#39;==========Saved inputs and outputs==========&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> idx &lt;span style="color:#f92672">in&lt;/span> range(len(total_grad_in)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;input: &amp;#39;&lt;/span>, total_grad_in[idx])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">&amp;#39;output: &amp;#39;&lt;/span>, total_grad_out[idx])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>注意&lt;/strong>，作者提到“register_backward_hook只能操作简单模块，而不能操作包含多个子模块的复杂模块。如果对复杂模块用了 backward hook，那么我们只能得到该模块最后一次简单操作的梯度信息。”不太确定什么是简单模块，不太确定诸如 resnet18 这样的网络是不是简单模块。&lt;/p>
&lt;p>2022-08-06 23:14:22，这个地方应该是想说，用 for loop 遍历 model.named_children() 是有必要的，否则直接 &lt;code>model = Model()model.register_backward_hook(hook_fn_backward)&lt;/code> 会有问题。&lt;/p>
&lt;h4 id="4122-注意事项">4.1.2.2 注意事项&lt;/h4>
&lt;p>&lt;strong>形状&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在卷积层中，weight 的梯度和 weight 的形状相同&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在全连接层中，weight 的梯度的形状是 weight 形状的转秩（观察上文中代码的输出可以验证）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>grad_input tuple 中各梯度的顺序&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在卷积层中，bias 的梯度位于tuple 的末尾：grad_input = (对feature的导数，对权重 W 的导数，对 bias 的导数)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在全连接层中，bias 的梯度位于 tuple 的开头：grad_input=(对 bias 的导数，对 feature 的导数，对 W 的导数)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>当 batchsize &amp;gt; 1 时，对 bias 的梯度处理不同&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在卷积层，对 bias 的梯度为整个 batch 的数据在 bias 上的梯度之和：grad_input = (对feature的导数，对权重 W 的导数，对 bias 的导数)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在全连接层，对 bias 的梯度是分开的，bach 中每条数据，对应一个 bias 的梯度：grad_input = ((data1 对 bias 的导数，data2 对 bias 的导数 &amp;hellip;)，对 feature 的导数，对 W 的导数)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="misc">Misc&lt;/h1>
&lt;p>@classmethod 用法&lt;/p>
&lt;p>想给初始类再新添功能，不需要改初始类，只要在下一个类内部新写一个方法，方法用@classmethod装饰一下即可。、&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@classmethod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">convert_sync_batchnorm&lt;/span>(cls, module, process_group&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>):
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="问题">问题&lt;/h1>
&lt;ul>
&lt;li>Debug 的时候怎么看 tensor 变量的参数？&lt;/li>
&lt;/ul>
&lt;h1 id="reference">Reference&lt;/h1>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/">https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/&lt;/a> pytorch 中文文档&lt;/p>
&lt;p>&lt;a href="https://cloud.tencent.com/developer/article/1475430#">https://cloud.tencent.com/developer/article/1475430#&lt;/a>&lt;/p></description></item><item><title>国内硕博如何提升自己的英语</title><link>https://huweim.github.io/post/%E5%8D%9A%E5%AE%A2_%E5%9B%BD%E5%86%85%E5%8D%9A%E5%A3%AB%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E8%87%AA%E5%B7%B1%E7%9A%84%E8%8B%B1%E8%AF%AD/</link><pubDate>Sat, 16 Jul 2022 16:39:35 +0800</pubDate><guid>https://huweim.github.io/post/%E5%8D%9A%E5%AE%A2_%E5%9B%BD%E5%86%85%E5%8D%9A%E5%A3%AB%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E8%87%AA%E5%B7%B1%E7%9A%84%E8%8B%B1%E8%AF%AD/</guid><description>&lt;p>这个是最近（2022-07-16 16:09:15）我一直在思考的问题。因为之后可能会选择在国内度过自己的博士生涯。目前的英语水平可以说是完全不够的。并不是说 paper 看多了，英语水平就上去了。听说读写是耦合在一起的，没有国外的英文环境，就需要有意识地去提升自己的英语水平。&lt;/p>
&lt;h5 id="一些建议">一些建议&lt;/h5>
&lt;p>即使到了国外，可能也很难通过和外国人交朋友来提升自己的英文水平，英语提升还是需要刻意去练习的。&lt;/p>
&lt;ul>
&lt;li>可以根据自己的喜好，坚持地去阅读或者听一些东西，比如电视新闻评论，转播的篮球比赛，电视剧，TED 演讲等等。一个是选择自己喜欢的东西，做起来没有额外的开销，另一个是要 &lt;strong>坚持&lt;/strong>，自己的学习就非常零散。&lt;/li>
&lt;li>读自己写的东西，也就是读出声音。&lt;/li>
&lt;/ul>
&lt;h5 id="施一公如何提高科研写作能力">施一公：如何提高科研写作能力&lt;/h5>
&lt;p>&lt;strong>他是怎么做的&lt;/strong>？&lt;/p>
&lt;ul>
&lt;li>每天花 45 分钟读《华盛顿邮报》。他每天早上安排完第一批实验后，会在十点左右花一个小时阅读《华盛顿邮报》，主要看新闻版
&lt;ul>
&lt;li>这里的重点其实不是读什么，而是每天都坚持&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>阅读变多了，他常常会有自己动笔去写的冲动。《巴尔的摩太阳报》也刊登了他的信刊。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>一些经历&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>他在 1994 年写完论文之后感觉很差，自己都不愿意再读第二遍，就交给了老板。（我自己也感觉自己写的是一坨屎，不想再看）&lt;/li>
&lt;li>他的老板花了 4 个小时把文章的整体写完了，只缺少 method 和 reference，并且根本没用施一公的初稿
&lt;ul>
&lt;li>&lt;strong>写文章贵在一气呵成&lt;/strong>，之后他也沿袭这个风格，两次一晚上通宵赶一篇文章。不过前提是对研究领域非常熟悉，对文章整体的大概思路已经深思熟虑，所有的 figures 已经做好了。&lt;/li>
&lt;li>这些前期工作，全身心投入也得花费 3-4 天。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>总结&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>保持阅读英文文章的习惯，每天 30-60 分钟。从英文报纸，英文新闻开始，逐渐转为专业的杂志&lt;/li>
&lt;li>写科研论文，最重要的是逻辑。必须先讨论出一套清晰的思路，根据思路作图（也就是逻辑 flow，或者结构），然后再动笔开始写&lt;/li>
&lt;li>写作时，根据思路写一个 subheading 框架，也就是设计好每个部分的小标题。第一稿切忌每句话都追求完美（这个深有体会，自己第一次写作时知道这一点，但是又忍不住反复琢磨一个词一句话，因为词汇量匮乏，这个要通过反复练习来避免）
&lt;ul>
&lt;li>第一稿追求的重点是逻辑（logic flow），要注意前后句的逻辑关系，相邻两段的逻辑关系&lt;/li>
&lt;li>写作时，全力以赴，不受任何事情干扰（关闭手机，座机，微信等），争取在最短时间内拿出第一稿&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>照葫芦画瓢&lt;/strong>。学习自己领域内文章的一些固定表达，整理一些局势，但切忌抄袭。在美国的一些机构，连续 7 个英文单词和别人完全一样，原则上就被认为是抄袭（plagiarism）&lt;/li>
&lt;li>第一稿写完后，给自己不超过一天的休息时间，开始改第二稿。修改时还是逻辑为主，每句话都要推敲，要学会同义替换（thesaurus），避免过多重复。第二稿非常关键，往后可能不会大改了。&lt;/li>
&lt;li>第二稿以后的修改，关注具体的字句，逻辑基本定型。投稿前，整体读一遍。一篇文章不会因为个别语法错误被拒，但一定会因为逻辑混乱被拒。&lt;/li>
&lt;/ul>
&lt;h5 id="可行的方案">可行的方案&lt;/h5>
&lt;p>实际上，最难的部分还是每天坚持，下面列一些可行的方案。阅读 paper 自然不用说，这个是自然的。但是除了阅读 paper 以外，也应该坚持阅读一些新闻杂志。&lt;/p>
&lt;ul>
&lt;li>观看 &lt;strong>LinusTechTips&lt;/strong> 频道的视频。这是一个科技频道，每个视频 10 分钟左右，也是自己感兴趣的方向。（听，读）&lt;/li>
&lt;li>每天早上阅读一篇英文新闻。（读）&lt;/li>
&lt;li>每周，根据一周阅读的 paper，写一篇英文 reivew。一般 paper reading 都会记下笔记，前期可以把模板固定好。有了笔记的总结和格式模板，熟练之后的开销就只有写作了。（写）&lt;/li>
&lt;li>（option），italk，定期和别人对话，练习口语。（听，说）&lt;/li>
&lt;li>（option），B 站关注听说方面的 up，每天边听边跟读。（听，说）&lt;/li>
&lt;li>坚持&lt;/li>
&lt;li>坚持&lt;/li>
&lt;li>坚持，这个是最难的&lt;/li>
&lt;/ul></description></item><item><title>VSCode, Chrome 常用快捷键</title><link>https://huweim.github.io/post/%E5%B7%A5%E5%85%B7_%E5%BF%AB%E6%8D%B7%E9%94%AE_vscode_chrome/</link><pubDate>Fri, 10 Jun 2022 15:27:08 +0800</pubDate><guid>https://huweim.github.io/post/%E5%B7%A5%E5%85%B7_%E5%BF%AB%E6%8D%B7%E9%94%AE_vscode_chrome/</guid><description>&lt;h1 id="1-vscode">1. VSCode&lt;/h1>
&lt;p>&lt;strong>目前生疏但是需要常用的&lt;/strong> 2022-06-10 15:14:48&lt;/p>
&lt;ul>
&lt;li>⭐ 选择所有当前选中的字符（可以理解为全局版的 Ctrl + D）: Ctrl + Shift + L&lt;/li>
&lt;li>在选中行末尾插入光标：Alt + Shift + i&lt;/li>
&lt;li>在右侧打开 Markdown 预览：Ctrl + K, V&lt;/li>
&lt;li>查看定义：Alt + F12；只是小窗查看，比较方便，注意跳转和查看的区别&lt;/li>
&lt;li>重新打开最后关闭的标签页：Ctrl + Shift + T&lt;/li>
&lt;li>向左切换标签：Ctrl + Tab / Ctrl + PgDn&lt;/li>
&lt;li>向右切换标签：Ctrl + Shift + Tab / Ctrl + PgUp&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>快捷键的意义不在于记住每一个键位，而是你需要知道某个操作是可以通过快捷键达到的。当你需要多次重复这个操作时，可以去查找这个快捷键。&lt;/p>
&lt;/blockquote>
&lt;h2 id="11-编辑">1.1 编辑&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>选中当前单词 Ctrl + D，多次按的话可以批量更改变量&lt;/p>
&lt;/li>
&lt;li>
&lt;p>⭐ 选择所有当前选中的字符（可以理解为全局版的 Ctrl + D）: Ctrl + Shift + L&lt;/p>
&lt;/li>
&lt;li>
&lt;p>选择所有出现的当前单词：Ctrl + F2；不用选中，光标放在单词上即可，相当于先 Ctrl + D 然后 Ctrl + Shift + L&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>控制台终端显示与隐藏&lt;/strong>：Ctrl + ~&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>选中代码 ：&lt;/strong> Shift + Home/End/方向键 （配置 Ctrl 使用效果更佳）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>全局替换：&lt;/strong> Ctrl + Shift + H&lt;/p>
&lt;/li>
&lt;li>
&lt;p>分屏 Ctrl + \&lt;/p>
&lt;/li>
&lt;li>
&lt;p>合并分屏 Ctrl + Alt + \&lt;/p>
&lt;/li>
&lt;li>
&lt;p>重新打开最后关闭的标签页：Ctrl + Shift + T&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在选中行末尾插入光标/也就是 1.3 中的多行编辑：Alt + Shift + i&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在右侧打开 Markdown 预览：Ctrl + K, V&lt;/p>
&lt;/li>
&lt;li>
&lt;p>向左切换标签：Ctrl + Tab / Ctrl + PgDn&lt;/p>
&lt;/li>
&lt;li>
&lt;p>向右切换标签：Ctrl + Shift + Tab / Ctrl + PgUp&lt;/p>
&lt;/li>
&lt;li>
&lt;p>按单词跳转 ⭐ Ctrl + 方向键&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>这个非常常用，可以快速地在单行的单词之前跳转光标&lt;/p>
&lt;/blockquote>
&lt;p>行操作&lt;/p>
&lt;ul>
&lt;li>&lt;strong>复制当前行&lt;/strong>：Shift + Alt + Up/Down / Ctrl + C&lt;/li>
&lt;li>剪切当前行 Ctrl + X&lt;/li>
&lt;li>&lt;strong>删除当前行&lt;/strong>：Shift + Ctrl + K&lt;/li>
&lt;li>选择整行 Ctrl + L&lt;/li>
&lt;li>移动选中的行：Alt + Up/Down&lt;/li>
&lt;li>跳转到指定行 Ctrl + G&lt;/li>
&lt;/ul>
&lt;h2 id="12-编程">1.2 编程&lt;/h2>
&lt;ul>
&lt;li>Go to definition: Ctrl + 鼠标左键/F12&lt;/li>
&lt;li>显示所有引用：Shift + F12&lt;/li>
&lt;li>跳转到光标的上一个位置： Alt + 方向键←&lt;/li>
&lt;li>参数提示：Ctrl + Shift + Space&lt;/li>
&lt;li>查看定义：Alt + F12；只是小窗查看，比较方便，注意跳转和查看的区别&lt;/li>
&lt;/ul>
&lt;h2 id="13-多行编辑">1.3 多行编辑&lt;/h2>
&lt;p>Step&lt;/p>
&lt;ul>
&lt;li>选中多行；可以用鼠标，也可以用 Ctrl + Shift + 方向键/Home/End&lt;/li>
&lt;li>按下快捷键 Alt + Shift + i，进入多行编辑状态&lt;/li>
&lt;/ul>
&lt;h2 id="14-文件">1.4 文件&lt;/h2>
&lt;p>打开新窗口：Ctrl + N
打开文件：Ctrl + P
选择文件：Ctrl + O
最近打开的 workplace: Ctrl + R&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h1 id="2-chrome">2. Chrome&lt;/h1>
&lt;p>&lt;strong>最常用&lt;/strong>并且目前比较生疏 2022-06-10 14:45:52&lt;/p>
&lt;ul>
&lt;li>新建标签页：Ctrl + T&lt;/li>
&lt;li>向左切换标签：Ctrl + Tab / Ctrl + PgDn&lt;/li>
&lt;li>向右切换标签：Ctrl + Shift + Tab / Ctrl + PgUp&lt;/li>
&lt;li>跳转到地址栏：Ctrl + l / Alt + D / F6&lt;/li>
&lt;/ul>
&lt;h2 id="21-标签和窗口">2.1 标签和窗口&lt;/h2>
&lt;p>个人认为会比较常用的&lt;/p>
&lt;ul>
&lt;li>新建标签页：Ctrl + T&lt;/li>
&lt;li>重新打开最后关闭的标签页：Ctrl + Shift + T&lt;/li>
&lt;li>向左切换标签：Ctrl + Tab / Ctrl + PgDn&lt;/li>
&lt;li>向右切换标签：Ctrl + Shift + Tab / Ctrl + PgUp&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>2022-06-10 14:24:50，向左右切换标签正是在寻找的功能，这个也可以用在 VSCode 窗口中使用。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>在新的后台标签页中打开链接：Ctrl + 单击 / 鼠标滚轮单机&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>可以代替 右键 -&amp;gt; 在新标签页中打开链接&lt;/p>
&lt;/blockquote>
&lt;p>其他&lt;/p>
&lt;ul>
&lt;li>新建窗口：Ctrl + N&lt;/li>
&lt;li>新建无痕窗口：Ctrl + Shift + N&lt;/li>
&lt;li>标签内打开 Home 页面：Alt + Home&lt;/li>
&lt;li>最小化当前窗口：Alt + 空格键 + N&lt;/li>
&lt;li>最大化当前窗口：Alt + 空格键 + X&lt;/li>
&lt;li>在新窗口打开链接： Shift + 单击&lt;/li>
&lt;/ul>
&lt;h2 id="22-功能">2.2 功能&lt;/h2>
&lt;p>个人认为常用，好像没有特别常用的。。&lt;/p>
&lt;p>其他&lt;/p>
&lt;ul>
&lt;li>下载内容：Ctrl + J&lt;/li>
&lt;li>历史记录：Ctrl + H&lt;/li>
&lt;li>开发者工具：F12 / Ctrl + Shift + J / Ctrl + Shift + I&lt;/li>
&lt;li>保存网页：Ctrl + S&lt;/li>
&lt;li>添加书签：Ctrl + D&lt;/li>
&lt;li>添加所有打开的网页添加书签：Ctrl + Shift + D&lt;/li>
&lt;li>书签管理器：Ctrl + Shift + O&lt;/li>
&lt;li>用浏览器打开文件：Ctrl + O&lt;/li>
&lt;li>任务管理器：Shift + Esc&lt;/li>
&lt;li>清除历史记录：Ctrl + Shift + Del&lt;/li>
&lt;/ul>
&lt;h2 id="23-地址栏">2.3 地址栏&lt;/h2>
&lt;p>&lt;strong>个人认为常用&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>地址栏搜索：Ctrl + K / Ctrl + E&lt;/li>
&lt;li>跳转到地址栏：Ctrl + l / Alt + D / F6&lt;/li>
&lt;li>站内搜索：关键字 + site：网址&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>快速跳转到地址栏是需要掌握的
站内搜索：关键字 + site：网址，这个如果使用 google 需要翻墙，并不是单纯地字符串匹配。&lt;/p>
&lt;/blockquote>
&lt;p>其他&lt;/p>
&lt;ul>
&lt;li>使用其他搜索引擎进行搜索：Tab&lt;/li>
&lt;li>为网站名称添加 www. 和 .com：Ctrl + Enter&lt;/li>
&lt;/ul>
&lt;h2 id="24-页面">2.4 页面&lt;/h2>
&lt;p>个人认为常用&lt;/p>
&lt;ul>
&lt;li>搜索：Ctrl + F&lt;/li>
&lt;li>刷新：Ctrl + R / F5 / Ctrl + F5&lt;/li>
&lt;li>调整页面字体大小：Ctrl + &amp;lsquo;&amp;rsquo;+&amp;rsquo;&amp;rsquo; / Ctrl + &amp;lsquo;&amp;rsquo;-&amp;rsquo;&amp;rsquo; 或者 Ctrl + 鼠标滚轮上/下&lt;/li>
&lt;/ul>
&lt;p>其他&lt;/p>
&lt;ul>
&lt;li>先下翻页：PgUp&lt;/li>
&lt;li>向上翻页：PgDn&lt;/li>
&lt;li>滑动到最顶部：Home&lt;/li>
&lt;li>滑动到最底部：End&lt;/li>
&lt;li>打印：Ctrl + P&lt;/li>
&lt;li>页面字体恢复默认：Ctrl + 0&lt;/li>
&lt;li>全屏切换：F11&lt;/li>
&lt;/ul>
&lt;h1 id="reference">Reference&lt;/h1>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/43730820#:~:text=Chrome%20%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%A4%A7%E5%85%A8%201%20%E5%90%8C%E4%B8%AA%E7%AA%97%E5%8F%A3%E6%96%B0%E5%BB%BA%E6%A0%87%E7%AD%BE%EF%BC%9ACtrl%20%2B%20T%202%20%E9%87%8D%E6%96%B0%E6%89%93%E5%BC%80%E6%9C%80%E5%90%8E%E5%85%B3%E9%97%AD%E7%9A%84%E6%A0%87%E7%AD%BE%E9%A1%B5%EF%BC%9ACtrl,%2F%20Ctrl%20%2B%20Shift%20%2B%20...%20%E6%9B%B4%E5%A4%9A%E7%BB%93%E6%9E%9C...%20">https://zhuanlan.zhihu.com/p/43730820#:~:text=Chrome%20%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%A4%A7%E5%85%A8%201%20%E5%90%8C%E4%B8%AA%E7%AA%97%E5%8F%A3%E6%96%B0%E5%BB%BA%E6%A0%87%E7%AD%BE%EF%BC%9ACtrl%20%2B%20T%202%20%E9%87%8D%E6%96%B0%E6%89%93%E5%BC%80%E6%9C%80%E5%90%8E%E5%85%B3%E9%97%AD%E7%9A%84%E6%A0%87%E7%AD%BE%E9%A1%B5%EF%BC%9ACtrl,%2F%20Ctrl%20%2B%20Shift%20%2B%20...%20%E6%9B%B4%E5%A4%9A%E7%BB%93%E6%9E%9C...%20&lt;/a>&lt;/p></description></item><item><title>（转载）MLSys 个方向综述</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_%E8%BD%AC%E8%BD%BDmlsys-%E4%B8%AA%E6%96%B9%E5%90%91%E7%BB%BC%E8%BF%B0/</link><pubDate>Mon, 09 May 2022 22:17:43 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_%E8%BD%AC%E8%BD%BDmlsys-%E4%B8%AA%E6%96%B9%E5%90%91%E7%BB%BC%E8%BF%B0/</guid><description>&lt;p>最近在试着寻找ML + sys可做的方向，发现涉及到的坑太多了，有点眼花缭乱的感觉&amp;hellip;&amp;hellip;不如写点东西总结一哈，帮自己理一下思路。&lt;/p>
&lt;p>个人感觉MLsys不能算是一种方向，而是一种思路&amp;hellip;&amp;hellip;比如对于system研究者来说，可以把ML作为我们开发的系统要适配的一种benchmark，就像transaction对于数据库、某种文件场景对于File System的意义一样。这样一想可做的空间就宽广多了。就算ML哪天又进入寒冬，之前所学的技术也仍然是可持续的。传统的system研究者也应该适应这个潮流，不能简单的把MLsys一律归为大水漫灌..&lt;/p>
&lt;p>有很多topic我也是初次接触，还不是很熟悉。如有错误还请批评指点~&lt;/p>
&lt;h1 id="1-分布式机器学习distributed-dnn-training">1. 分布式机器学习（Distributed DNN Training）&lt;/h1>
&lt;p>这个又可以分为两个方面：from ML / system perspective。安利一下刘铁岩老师的《分布式机器学习》这本书（[ch_]表示引用这本书中的一些章节），还有UCB cs294 19fall的这一节。&lt;/p>
&lt;h2 id="11-ml">1.1 ML&lt;/h2>
&lt;p>从ML的角度做，主要是发明或改进分布式训练算法[ch4] [ch5]，保证在分布式加速的同时，仍然能达到原来的学习效果（loss/accuracy）。因此很多工作也被投在像ICML、NIPS这种专业ML会议上。主要用到的方法包括优化（optimization）和统计学习理论（statistical learning theory）。&lt;/p>
&lt;p>还有一类工作涉及到如何把单机算法改造成分布式[ch9]，比如同步/异步SGD等。这里主要涉及到的问题是如何降低分布式环境下的通信开销，提高加速比。&lt;/p>
&lt;p>这方面了解不多就少写点了&amp;hellip; 可以参考这里。&lt;/p>
&lt;h2 id="12-system">1.2 System&lt;/h2>
&lt;p>还有一个就是从System的角度做。从分布式计算的角度来看，可以把相关工作分为以下几类：&lt;/p>
&lt;p>对于计算量太大的场景（计算并行），可以多线程/多节点并行计算，多节点共享公共的存储空间。常用的一个算法就是同步随机梯度下降（synchronous stochastic gradient descent），含义大致相当于K个（K是节点数）mini-batch SGD [ch6.2]
对于训练数据太多，单机放不下的场景（数据并行，也是最主要的场景），需要将数据划分到多个节点上训练。每个节点先用本地的数据先训练出一个子模型，同时和其他节点保持通信（比如更新参数）以保证最终可以有效整合来自各个节点的训练结果，并得到全局的ML模型。 [ch6.3]
对于模型太大的场景，需要把模型（例如NN中的不同层）划分到不同节点上进行训练。此时不同节点之间可能需要频繁的sync。这个叫做模型并行。 [ch6.4]
Pipeline Parallelism：这是去年（SOSP19 PipeDream）才出现的概念，参考这里的第90、95页 以及这里的简介。Pipeline Parallelism相当于把数据并行和模型并行结合起来，把数据划分成多个chunk，也把训练模型的过程分成了Forward Pass和Backward Pass两个stage。然后用流水线的思想进行计算。
另外，分布式ML本质上还是分布式系统嘛，所以像传统分布式系统里的一些topic（比如一致性、fault tolerance、通信、load balance等等）也可以放到这个背景下进行研究。&lt;/p>
&lt;p>最近挖的比较多的坑大致涉及以下几个点：&lt;/p>
&lt;h3 id="111-分布式ml系统设计">1.1.1 分布式ML系统设计&lt;/h3>
&lt;p>[ch7.3] 最著名的就是几大分布式DL模型：Parameter Server / AllReduce等。&lt;/p>
&lt;p>个人感觉这里面一个可以挖的坑是Decentralized Training。地里一位大佬也在做这个方向。&lt;/p>
&lt;h3 id="112-edge-computing">1.1.2 Edge Computing&lt;/h3>
&lt;p>很多ML模型是需要在手机上运行的（比如毁图秀秀）。针对这一场景，一个是要对手机这种低功耗设备对ML model进行裁剪加速（后面会提到），还有一个要做的就是运行在多个device上的分布式ML。&lt;/p>
&lt;p>这里有个最近非常火的概念：Federated Learning。其实本质还是炒数据并行的冷饭&amp;hellip;不过应用场景比较不一样。FL更多是为了Privacy的考虑，而分布式加速训练在这里倒是个次要目标。FL还涉及到了模型聚合[ch8]，也就是如何把多个device本地训练出的模型合并到一起。&lt;/p>
&lt;h3 id="113-大量计算资源的scheduling--device-placement">1.1.3 大量计算资源的Scheduling / device placement&lt;/h3>
&lt;p>UCB的CS294 19spring对这一节有过介绍。&lt;/p>
&lt;p>这里的计算资源的数量级是很大的&amp;hellip;&amp;hellip;比如工业界会有万台CPU服务器 / 上千台GPU服务器搭建的DL平台。这个小方向要解决的问题就是如何充分利用它们的性能。比如在阿里PAI组的JD里就有这么一条：“设计探索高效的分布式Placement算法，以更系统化的方式来解决大规模深度学习高效训练的问题”。&lt;/p>
&lt;p>这方面比较早的工作大概是这篇paper，说的是如何为TensorFlow计算图里的不同算子分配不同的device，最后用强化学习实现了这个目标。这个工作看起来有点prototype，但提出了一个新的思路。另外还有很多猛如虎的类似Train XX model in y minutes的工作。这种就不仅是placement好就能完成的了，还需要涉及系统拓扑的设计、降低communication开销等等。&lt;/p>
&lt;p>对于集群调度，工业界的一个热点是使用容器平台（例如k8s）来运行分布式机器学习应用。虽然k8s本身就有容器集群调度的功能，但为了让它更好地适应ML的workload，人们开发了一些新的轮子，比如针对TensorFlow（Parameter Server模型）和PyTorch的KubeFlow。还有用k8s来跑AutoML的katib。学术界对这方面的一个研究热点是GPU集群调度，在下面2.2节会介绍。&lt;/p>
&lt;h3 id="114-communication相关">1.1.4 communication相关&lt;/h3>
&lt;p>[ch3.5] [ch7]介绍了一些宏观上的通信模型，但深入进去还有很多可搞的坑。传统搞网络/分布式系统的组比较契合这个小方向。&lt;/p>
&lt;p>例如我校的分布式组原来有一些geo-distributed system的工作，现在也可以往ML上装。&lt;/p>
&lt;h3 id="115-其他sys-for-ml可做的坑">1.1.5 其他sys for ML可做的坑&lt;/h3>
&lt;p>工业界的一个ML pipeline不仅仅是训练，还涉及到很多其他的坑。这些是目前被挖的还比较少的：&lt;/p>
&lt;p>存储 / Data Management：&lt;/p>
&lt;ol>
&lt;li>训练数据的规模是很大的。如何为ML设计一个专用的文件系统（类似大数据界的HDFS）或者数据库来加速读数据呢？ 类似的工作有管理ML model的ModelDB.&lt;/li>
&lt;li>在ML framework中，以及Parameter Server中，需要用一个KV storage system来存储参数。可不可以针对ML的场景优化这个KV存储系统呢？ 关于这个可以参考neopenx大神的blog。&lt;/li>
&lt;/ol>
&lt;h1 id="2-深度学习模型压缩加速-">2. 深度学习模型压缩/加速 ⭐&lt;/h1>
&lt;p>这方面和architecture结合比较紧密。CS229有这一节，也可以参考NIPS19上的这个talk。&lt;/p>
&lt;p>对DL model进行压缩主要考虑两个角度：减少计算量（例如conv层的计算量） / 内存占用（NN的参数数量）。不仅要考虑ML上的metric，也要考虑system层面的performance（例如latency / throughput / 功耗。有时候这些比ML模型的accuracy还重要）。具体的方式大概有以下几种：&lt;/p>
&lt;ol>
&lt;li>Architectural Compression
Layer Design -&amp;gt; Typically using factorization techniques to reduce storage and computation
Pruning（剪枝） -&amp;gt; Eliminating weights, layers, or channels to reduce storage and computation from large pre-trained models. 减少卷积核大小 / 通道数等等&lt;/li>
&lt;li>Weight Compression
Low Bit Precision Arithmetic -&amp;gt; Weights and activations are stored and computed using low bit precision
Quantized（量化） Weight Encoding -&amp;gt; Weights are quantized and stored using dictionary encodings.
很多相关的工作是在ML的角度来压缩模型的（也就是Arch Compression，特别是针对CNN和RNN。比如很著名的MobileNet）。这里我们先(kan)略(bu)过(dong)，来看从System的角度是如何加速的。&lt;/li>
&lt;/ol>
&lt;h2 id="21-通过quantized量化降低计算精度要求-">2.1 通过Quantized（量化）降低计算精度要求 ⭐⭐&lt;/h2>
&lt;p>量化的含义是将卷积层（the weights and / or activations of a CNN）通常要用到的32位浮点数用更低位的数来表示，如int32, int16, int8等等，来降低资源占用（float32无论是计算还是存储都是很吃资源的..）。量化之后无疑会损失一部分精度，但神经网络对噪声并不是特别敏感，因此控制好量化的程度之后对ML任务的影响可以很小。&lt;/p>
&lt;p>一种常用的量化方法是train in floating point and then quantize the resulting weights，训练时还是用float32（因为要涉及到反向传播和梯度下降，全是int就很难搞了..），但在inference的阶段就可以加速啦。一个直观的方法是事先找好一般网络参数的min / max值，然后将训练好的网络参数乘一个scala factor来映射到[MIN_INT, MAX_INT]区间内的整数存起来。在inference时先按int来计算，最后结果再转换回float32。这一过程中其实加速了大量的卷积计算。比如这篇paper就实现了float32到int8的量化。&lt;/p>
&lt;p>混合精度计算：上面讲的方法是用在inference阶段的，其实在模型训练时也可以用类似的方法来加速，只不过再用int就不大行了。一种比较新的方法是用float16（也就是俗称的半精度），fp16占用空间是单精度(fp32)的一半，双精度(double，也就是fp64)的1/4。&lt;/p>
&lt;p>量化的具体实现方法可以参考这里。NVIDIA专门推出了针对inference阶段量化加速的工具包TensorRT&lt;/p>
&lt;h2 id="22-新硬件--dl-acclerator-">2.2 新硬件 / DL Acclerator ⭐⭐&lt;/h2>
&lt;p>在纯硬件方面针对DL workload的工作也有很多，这里来看几个parallel相关的技术。最近Data-Level Parallelism不仅在深度学习中，在其他一些领域（比如数据库）也有了越来越多的应用。&lt;/p>
&lt;p>CPU：尽管GPU已经成了深度学习计算的标配，有时候仍然是需要CPU运算的。例如要在手机等辣鸡设备上进行inference。&lt;/p>
&lt;p>SIMD：SIMD的含义是同一条指令在多个数据流上操作，和在向量处理器中一样。在具体实现中（例如SSE指令集）是把一个128位SSE寄存器（这是新增加的SIMD专用寄存器，和早期借用FPU寄存器的MMX不同。在SSE指令集中是增加了8个这种寄存器）划分成4个块，同时存放4个float32单精度浮点数，4个块可以同时进行运算（有多个运算单元，作用于不同的地址），这样就提高了并行度。后来的SSE2 / SSE3 / SSE4 / AVX指令集在此基础上又增加对float64 / 更多运算的支持，以及扩展了SIMD专用寄存器的位数，但本质上还是一样的。　　另外，SIMD带来的并行和超标量处理器的并行性（一个周期issue多个指令，用于instruction level parallelism）不是一个概念。非超标量处理器也可以SIMD，而超标量处理器可以更并行issue多个SIMD操作。&lt;/p>
&lt;p>VLIW：和一次issue多条指令，然后靠硬件进行ILP调度（也叫动态多发射。需要硬件实现乱序执行、分支预测等操作）的超标量处理器不同，VLIW（Very Large Instruction Width，采用这种技术的处理器也叫做静态多发射处理器）的含义是一次只issue一条可以完成多个操作的复杂长指令（也叫发射包，其实从软件的角度看是多条指令的集合）。因此一条指令的位宽可以很大。VLIW是通过编译器来进行指令级并行调度的（比如一个常用的方法是循环展开，通过识别出可并行的重叠跨循环体指令块来实现ILP）。VLIW的本意是希望在编译阶段就识别出程序中的依赖关系（静态调度），得到可以并行执行的发射包，硬件只需要根据调度好的发射包直接执行即可，这样就简化了硬件实现，从而实现更大宽度发射包的并行执行。intel Itanium的IA64指令集就使用了这个技术，但它在当年并没有取得成功。一个重要的原因是它只适合计算密集、算法固定可控的workload。传统的通用应用程序可能很难具备这个属性（有很多run-time才能确定的值，另外cache访问也是不确定的），但深度学习任务具备这些性质。&lt;/p>
&lt;p>GPU：GPU的本质可以看做SIMT（Single Instruction Multiple Threads）。&lt;/p>
&lt;ul>
&lt;li>GPU集群：DL框架一般都支持GPU和分布式训练，已经可以在GPU集群环境下运行了，但实际上还存在一些问题导致分布式场景下资源的使用率提不上去：1). CPU和GPU之间memcpy开销太大、2). 参数通信开销太大、3). 显存不够用、4). GPU很难虚拟化(多任务共享)、5).需要针对ML workload的更好的集群调度策略。 对于1和3其实也可以用前面提到的神经网络压缩、模型并行等方法解决； 对于2一个解决方案是尽量让计算和通信在时间上重叠起来，参考ATC17的Poseidon； MSR对于5做了很多工作，一方面是对大规模GPU集群上的真实日志数据进行分析，得出了一些经验（发表在ATC19）。另一方面是设计一些更好的scheduling策略，例如OSDI2018的Gandiva（针对DL workload自身的特点来提高GPU集群使用率）和NSDI2019的Tiresias； 对于4目前还没啥很好的解决方案，但可以通过一些软调度方案来模拟。&lt;/li>
&lt;li>这学期8205课上会有GPGPU的topic，到时候再补充 ⭐⭐⭐&lt;/li>
&lt;/ul>
&lt;p>系统结构：这个和纯计算关系不是很大，可能暂时和ML加速也没啥关系（事实上目前在计算机网络研究中用的还多一些）&amp;hellip;&amp;hellip;但对于优化整体性能会有帮助&lt;/p>
&lt;ul>
&lt;li>NUMA：当单个CPU性能已经到瓶颈时，多处理器就成了比较好的解决方案。为了方便编程，需要保证能为应用程序提供跨越所有处理器的单一物理地址空间，这种也叫做共享内存处理器（Shared Memory Processor）。SMP又可以分为两种类型：1) 任何处理器访问任何地址的仿存时间都是相同的，叫做统一存储访问（Uniform Memory Access）。 2) 对于每个核心，访问某些字会比访问其他字快一些，整个内存空间被分割并分配给不同处理器 / 内存控制器，这叫做非统一存储访问（NonUniform Memory Access，NUMA）。NUMA虽然看起来复杂，但可以支持更大的规模（更多的核心），并且访问附近的存储器时具有较低的延迟。 在过去内存控制器还在北桥的时代，多处理器用的是UMA（所有处理器都通过FSB总线连接北桥，再访问内存）。后来随着核心越来越多，为提高访存速度，内存处理器被做到了CPU内，每个CPU有（或者很少的几个核心共享）一个内存控制器，然后直连一部分内存空间，这些核心就被归为一个NUMA node。而跨NUMA node之间的内存访问需要走QPI总线。可以参考这里的图解。 在一些涉及many core的工作中会经常用到NUMA的概念&lt;/li>
&lt;li>RDMA：在网络环境中会用到。RDMA全称是Remote Direct Memory Access，用于实现不需要OS参与的远程内存访问（因为message passing through kernel会浪费本来很大的内存和网络带宽）。具体的技术细节可以参考这里。不过最近（Eurosys2019）已经有了应用RDMA来加速分布式机器学习的工作。&lt;/li>
&lt;/ul>
&lt;p>专用硬件：CPU性能太菜，GPU又太庞大，于是人们开发了AI专用芯片&lt;/p>
&lt;ul>
&lt;li>FPGA：全称是Field Programmable Gate Array，是可以多次烧写的。因为本质上属于软件所以可以快速开发 / 迭代。&lt;/li>
&lt;li>ASIC：全称是application-specific integrated circuits，出厂后电路就不可以改变了（需要流片）。但是性能比FPGA高。Google的TPU就属于一种ASIC。&lt;/li>
&lt;/ul>
&lt;h2 id="23-矩阵算子优化">2.3 矩阵算子优化&lt;/h2>
&lt;p>神经网络中的很多运算本质上就是对矩阵运算，因此可以用一些矩阵乘法优化方案来加速。比如cublas就是封装好的针对矩阵和向量运算的加速库，而对于神经网络加速则会使用cudnn&lt;/p>
&lt;p>算子优化是个非常贴近hardware的工作，对多种设备都人工调优这些算子其实是比较难的&amp;hellip;如果能简化一部分工作就最好啦。于是就有了下面会提到的深度学习编译器。&lt;/p>
&lt;blockquote>
&lt;p>这个工作可能偏向于工业界&lt;/p>
&lt;/blockquote>
&lt;h2 id="24-automl">2.4 AutoML&lt;/h2>
&lt;p>这个严格来说可能不算MLsys了&amp;hellip;但它的思路在很多MLsys问题中也会被用到&lt;/p>
&lt;p>AutoML最早只能调很有限的几种参数，用的方法也比较暴力（启发式搜索）。后来能调的东西越来越多，方法也更加猛如虎&amp;hellip;一个里程碑是NAS，标志着神经网络结构也可以Auto了。&lt;/p>
&lt;p>常用的调参方法大致可以分为这几种：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>随机搜索，或者说叫启发式搜索。包括 GridSearch 和 RandomSearch。这种方法的改进空间主要体现在使用不同的采样方法生成配置，但本质上仍然是随机试验不同的配置，没有根据跑出来的结果来反馈指导采样过程，效率比较低。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multi-armed Bandit。这种方法综合考虑了“探索”和“利用”两个问题，既可以配置更多资源（也就是采样机会）给搜索空间中效果更优的一部分，也会考虑尝试尽量多的可能性。Bandit 结合贝叶斯优化，就构成了传统的 AutoML 的核心。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>深度强化学习。强化学习在 AutoML 中最著名的应用就是 NAS，用于自动生成神经网络结构。另外它在 深度学习参数调优 中也有应用。它的优点是从“从数据中学习”转变为“从动作中学习”（比如某个参数从小调到大），既可以从性能好的样本中学习，也可以从性能坏的样本中学习。但强化学习的坑也比较多，体现在训练可能比较困难，有时结果比较难复现。
之所以把AutoML也列出来，是因为这些方法在下面提到的ML for system问题中会很有用。比如之前做过的AutoTiKV就应用了一种贝叶斯优化方法来调节数据库参数。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>cs294中给出了几个可提高的方向：&lt;/p>
&lt;p>Accelerate data collection and preparation&lt;/p>
&lt;ul>
&lt;li>Automatic data discovery&lt;/li>
&lt;li>Distributed data processing, esp. for image and video data&lt;/li>
&lt;li>Data cleaning and schema driven auto-featurization&lt;/li>
&lt;/ul>
&lt;p>Accelerate model selection and hyper-parameter search&lt;/p>
&lt;ul>
&lt;li>Parallel and distributed execution&lt;/li>
&lt;li>Data and feature caching across training runs&lt;/li>
&lt;/ul>
&lt;p>Provenance&lt;/p>
&lt;ul>
&lt;li>Track previous model development to inform future decisions&lt;/li>
&lt;li>Connect errors in production with decisions in model development&lt;/li>
&lt;/ul>
&lt;h1 id="3-深度学习框架系统设计">3. 深度学习框架/系统设计&lt;/h1>
&lt;p>和Distributed Training的区别是这里更关注一些工程上的东西（框架设计、API设计等等）。一个Deep Learning Framework大致需要以下几个元素：&lt;/p>
&lt;ul>
&lt;li>支持各种算子(op) 和 tensor (data)&lt;/li>
&lt;li>计算图的定义方式（动态 v.s. 静态）&lt;/li>
&lt;li>Auto Diff&lt;/li>
&lt;li>Optimizer（例如Adam）&lt;/li>
&lt;li>各种加速和优化的库：cudnn, openblas,mkl等&lt;/li>
&lt;/ul>
&lt;h2 id="31-deep-learning-framework">3.1 Deep Learning Framework&lt;/h2>
&lt;p>这一节重点关注这几个方向：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Differentiable Programming：如果用过Keras或者PyTorch就会记得它可以简单得像搭积木一样摞一个NN出来，只需要定义一个一个的层（前向传播逻辑）和损失函数就行了。而NN的训练需要Backward Propagation / Forward Propagation，也就是计算微分，运算时framework可以根据定义好的计算图自动求导算梯度。只要可微分就可以保证这个积木能摞出来，然后使用链式法则就可以自动计算微分（Automatic Differentiation）。如果一个语言或者framework具备了Differentiable Programming的性质，就可以更简单的在它上面开发Deep Learning应用（可以类比python手写NN和Keras的区别）。这篇文章对Auto Diff的实现做了很详细的介绍。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Embedded Domain Specific Languages：DSL的概念我们都知道，比如SQL就是数据库系统中的DSL，但这已经相当于一个全新的语言了。Embedded DSL是在现有语言上（例如Python）针对某个特定任务做的扩展。比如为了让Python做矩阵计算更方便发明了numpy；为了进行机器学习就有了TensorFlow / PyTorch等等。Embedded DSL的作用是完成 Linear Algebra -&amp;gt; Pipelines -&amp;gt; Differentiable Programs 的转化。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据计算图的定义方式，可以分为Declarative Abstraction（Embedded DSL先生成静态计算图，类似编译执行 define-and-run，例如Tensorflow、Caffe）和Imperative（Embedded DSL生成动态计算图并直接输出结果，类似解释执行 define-by-run，例如PyTorch、Tensorflow Eager）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>对于具体的DL框架来说，虽然很多公司都开始自研框架了，但最流行的基本就TensorFlow、PyTorch、mxnet等等那几家了。不过最近又出现了分布式强化学习框架Ray，也具有很好的落地潜能。&lt;/p>
&lt;blockquote>
&lt;p>确实如此，工业界很多公司都在做自己的框架了。&lt;/p>
&lt;/blockquote>
&lt;h2 id="32-inference--model-serving">3.2 Inference / Model Serving&lt;/h2>
&lt;p>之前关注了很多训练ML模型中会遇到的问题。但实际应用场景里，inference（直接使用训练好的模型predict）的次数会比training多很多，因此inference的性能也很重要。&lt;/p>
&lt;p>Inference可以再分为以下两种：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Offline: Pre-Materialize Predictions：所有可能的query都是已知的，就事先predict好存起来。一般没有这么玩的&amp;hellip;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Online: Compute Predictions on the fly：根据用户的输入实时predict。这才是最常见的场景&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>一个典型的ML inference pipeline大致涉及到以下工序：&lt;/p>
&lt;ul>
&lt;li>input data&lt;/li>
&lt;li>-&amp;gt; Preprocessing(比如图片要resize)&lt;/li>
&lt;li>-&amp;gt; model prediction(有时候会同时用很多model，还要ensemble起来)&lt;/li>
&lt;li>-&amp;gt; 输出结果，有时候还要处理一下&lt;/li>
&lt;/ul>
&lt;p>这个pipeline的衡量指标包括Latency、Throughput等（和传统的system问题一样呀）。cs294里列出了几个最近的工作，可以参考这里的paper解读。个人感觉这里可做的坑不多&amp;hellip;.大多是修修补补&amp;hellip;&lt;/p>
&lt;h2 id="33深度学习编译器">3.3深度学习编译器&lt;/h2>
&lt;p>这里值得提一下TVM。这篇文章对TVM进行了非常详细的介绍。&lt;/p>
&lt;p>简单的说TVM是在把训练好的ML model部署在不同设备上时用的，重点关注的是Inference而不是Training（也就是推理引擎）。在这一过程中，模型本身可能用了不同的framework来写（比如tensorflow / PyTorch / MXNet，本质区别在于使用的算子类型可能不一样），而要部署到的设备也可能有不同的硬件架构（比如x86 / ARM / GPU / FPGA）。inference的过程也就是将framework X写出来的model放在硬件Y上运行的过程，这一过程和编译器是非常相似的（将语言X写的程序编译到硬件Y上运行），这也就是深度学习编译器的含义。&lt;/p>
&lt;p>为了设计一个高效的深度学习编译器，TVM借鉴了传统编译器LLVM的设计思想：抽象出编译器前端[ 高级语言C/java -&amp;gt; IR ]，编译器中端[ 优化IR，这种是不同编译器平台共享的 ]，编译器后端[ IR -&amp;gt; 目标硬件上的binary ]等概念，引入IR (Intermediate Representation。深度学习问题中可以将计算图作为IR，称为Graph IR)。这样不同硬件/framework都对标同一套IR，就避免了需要对每种硬件和framework排列组合适配的问题。TVM主要解决的是后端的问题[在目标硬件上高效运行IR]。而前端的问题[生成和优化IR]就交给深度学习框架们完成（针对这一步，在TVM stack中提供了NNVM，作用是represent workloads from different frameworks into standardized computation graphs）。&lt;/p>
&lt;p>TVM是和硬件深度集成的，也就是需要针对每种硬件平台实现相关的AI算子（类似NVIDIA GPU上的cuDNN）。然而人工调优这些算子的实现是很费精力的（特别是要针对不同形状的业务模型），这里面也有一些knob需要调整。为了让这个过程也能ML化，于是后来有了AutoTVM。&lt;/p>
&lt;p>cs294 sp19还提出了几个可能的future work：&lt;/p>
&lt;ul>
&lt;li>Compilers are great at Ahead of Time scheduling, what about Just-In-Time scheduling?&lt;/li>
&lt;li>Any way we can share GPU in predictable way and maximize utilization for DNN inference?&lt;/li>
&lt;li>Can we optimize for “fitness” of the kernel when it’s executed along with other kernels instead of its latency?&lt;/li>
&lt;/ul>
&lt;h1 id="4-用ml优化传统的system问题">4. 用ML优化传统的system问题&lt;/h1>
&lt;p>这里面的花样就更多了&amp;hellip;在上学期Jon的ML system课上有过较详细的接触。大部分是用ML去优化一个传统system问题中，一些需要人工经验调整、或者说可以从历史情况learn到一些东西的模块。比如数据库参数、操作系统页表、数据库索引等等。一个模块可以被ML化的前提是它必须是empirical的，参考它在页表（OS的工作集原理）、数据库（DBA是个很吃经验的活&amp;hellip;）中的应用。如果人工都看不出来啥规律就别指望它能ML了&amp;hellip;&lt;/p>
&lt;p>一般认为用ML优化system的思想是起源于Jeff Dean在NIPS2017的workshop。这方面的工作很多发表在纯system的顶级会议以及下属的AI for xxx workshop上，另外一些AI会议的workshop也会收录一些这方面的工作，比如nips 2018的MLsys workshop。从2017年开始已经有很多坑被做过了，但个人感觉还是有一些搞头的。感觉可以从下面两个角度再来搞：&lt;/p>
&lt;p>同样的scenario，使用更合适的ML算法。注意这里是更合适，而不是更高大上猛如虎。
比如这篇ML+Database的paper，使用了LSTM来预测未来的workload pattern，还要用GPU训练，但生产环境上要求数据库服务器也安个显卡是不现实的。工程上的一个解决方案是搞个集中式的训练集群（类似OtterTune），在DBaaS的情况下这种方法倒是行得通，但在对外发布的数据库产品中就不行了。
这里感觉可以参考早期AutoML的一些工作，因为它们本质是很类似的（都是调参嘛&amp;hellip;）。传统方法有启发式搜索/贝叶斯优化。最近也有很多人用强化学习去搞，但还是存在太吃资源的问题&amp;hellip;
这方面对ML知识的要求高一点。
寻找system界更多可以ML化的场景。这个更适合专业的system researcher来做，对ML倒是略有了解即可。
有一类思路是把ML深度集成到系统设计中，比如Andy在2019年的15-721课上提到过Self-Driving Database的概念，和之前用ML优化数据库的工作不同的是，Self-Driving DB更关注如何把ML和DB深度集成，而不是搞一个又一个外挂的模块了。
一个类似的工作是在OS领域：https://engineering.purdue.edu/WukLab/LearnedOS-OSR19.pdf 。
另外还有个工作是在Key-Value Storage Engine的领域：https://arxiv.org/pdf/1907.05443.pdf。它提出了Design Continuum的概念：存储系统中的很多数据结构本质上是很像的（arise from the very same set of fundamental design principles），例如B+tree, LSM-tree, LSH-table等，但它们却有不同的应用场景（比如KV Store中用LSM就比B+ Tree更合适），很难有一个十全十美的设计。这说明它们有相互替换的空间。这样我们可以将不同数据结构的选择也作为存储系统的一个knob，根据具体workload和硬件的情况来自动选择一个合适的底层数据结构（find a close to optimal data structure design for a key-value store given a target workload and hardware environment）。
一个更宏观一些的思路是做system and algorithm co-design，让任意计算机系统都能和ml深度集成。虽然具体的target system不一样，但其中有很多模块都是类似的（例如training、inference、system monitor等等）。针对这一目标MSR提出了AutoSys，对这些通用模块进行了整合。&lt;/p>
&lt;blockquote>
&lt;p>这部分就如同用 ML 方法去选择调度算法一样，比较 trick。&lt;/p>
&lt;/blockquote>
&lt;h1 id="5-其他">5. 其他&lt;/h1>
&lt;p>方向不是很契合就先不看了&amp;hellip;等用到了再填坑&lt;/p>
&lt;p>ML pipeline / lifecycle：https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf
Privacy：https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec10/10_adversarial_ml.pdf
图神经网络训练系统：https://www.msra.cn/zh-cn/news/features/2019-review-machine-learning-system [ATC19 NeuGraph]&lt;/p>
&lt;p>需要的技能树
这是从一些公司ML System Research Scientist岗位的招聘要求中整理出来的，更侧重system一些。&lt;/p>
&lt;p>System：&lt;/p>
&lt;p>工程基础：C/C++、OO programming。阅读源码是个很好的学习方式
OS
分布式系统
编译原理。特别是编译器优化技术、LLVM、memory optimization。Parser之类不喜欢也可以不看
Computer Architecture。另外还需要了解：1.GPU架构，例如显存分配机制、CPU与GPU交互。 2.CPU、存储系统相关的新技术。 3.有条件可以了解下深度学习专用硬件。
常见的并行计算框架，例如MPI/OpenMP/CUDA
ML framework的底层原理，扒源码
工业界的一些新东西：例如k8s、KubeFlow、ElasticDL
ML：&lt;/p>
&lt;p>机器学习基础
常见的分布式机器学习算法、DL模型压缩、模型加速方法（根据具体方向而定）
数理基础不要太菜…不要被人吐槽像没学过高中数学…&lt;/p>
&lt;h1 id="reference">Reference&lt;/h1>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/104444471">https://zhuanlan.zhihu.com/p/104444471&lt;/a>&lt;/p></description></item><item><title>cmake 目录结构和使用</title><link>https://huweim.github.io/post/%E5%B7%A5%E5%85%B7_cmake-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E5%92%8C%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 09 May 2022 22:17:43 +0800</pubDate><guid>https://huweim.github.io/post/%E5%B7%A5%E5%85%B7_cmake-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E5%92%8C%E4%BD%BF%E7%94%A8/</guid><description>&lt;h1 id="0-前言">0. 前言&lt;/h1>
&lt;p>cuTLASS 使用到了 cmake，之前没有接触过，先学习一下他的目录结构和编译过程。&lt;/p>
&lt;h2 id="01-cmake-简介">0.1 Cmake 简介&lt;/h2>
&lt;p>对于 C++ 程序，手动编写 Makefile 非常麻烦。cmake 用于自动编写 Makefile。通过读取 CMakeLists.txt 文件，可以自动生成 make 文件。cmake 中 macro 和 function 的使用，使得 cmake 更像是一个脚本语言。&lt;/p>
&lt;h1 id="1-cmake-目录">1. cmake 目录&lt;/h1>
&lt;h2 id="11-目录结构">1.1 目录结构&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>|-- bin &lt;span style="color:#75715e">#存放可执行文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|-- build &lt;span style="color:#75715e">#目录用于存放编译生成的文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|-- CMakeLists.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|-- include &lt;span style="color:#75715e">#统一存放头文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| |-- hello.h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| |-- gpgpu.h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|-- lib
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|-- README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|-- src
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| |-- CMakeLists.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| |-- main.cpp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| |-- model1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| | |-- CMakeLists.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| | |-- gpgpu.cpp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| | |-- model.cpp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| |-- model2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| | |-- CMakeLists.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| | |-- hello.cpp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| | |-- model.cpp
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>对于大一点的项目可能还会需要 util 目录，library 目录夹或者 tool 目录&lt;/p>
&lt;p>&lt;strong>src&lt;/strong>: 这个 example 包含了 2 models，main.cpp 依赖于 2 基础 models，另外，注意到他们都包含 &lt;code>CMakeLists.txt&lt;/code> 文件&lt;/p>
&lt;h2 id="12-理解-cmakeliststxt">1.2 理解 CMakeLists.txt&lt;/h2>
&lt;p>注意 cmake 文件中不区分大小写&lt;/p>
&lt;h3 id="121-baseline">1.2.1 Baseline&lt;/h3>
&lt;p>最基础的包含以下一些信息，&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 规定该CMakeLists.txt适用的cmake最小版本，这里是 3.12，自己手动安装了 3.20 版本&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cmake_minimum_required&lt;span style="color:#f92672">(&lt;/span>VERSION 3.12.4 FATAL_ERROR&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 项目名称，也就是 cutlass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>project&lt;span style="color:#f92672">(&lt;/span>CUTLASS VERSION 2.9.0 LANGUAGES CXX&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 定义生成的可执行文件(程序)的名称，假设为 gemm&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 这里没找到 cutlass 对应的，cutlass 中有一个 function(cutlass_add_executable_tests NAME TARGET)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>add_executable &lt;span style="color:#f92672">(&lt;/span>gemm gemm.cxx&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 指定头文件搜索路径，根目录下 include&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>include_directories &lt;span style="color:#f92672">(&lt;/span>include&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="2-cmake-编译过程">2. cmake 编译过程&lt;/h1>
&lt;h2 id="21-build-编译并运行">2.1 build, 编译并运行&lt;/h2>
&lt;p>build 目录用于存放编译生成的文件，一般的编译过程：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ mkdir build &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cd build
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">75&lt;/span> &lt;span style="color:#75715e"># compile for NVIDIA Turing GPU architecture&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="22-问题可执行程序在哪个目录生成">2.2 问题，可执行程序在哪个目录生成？&lt;/h2>
&lt;p>看起来似乎是和 Makefile 文件同一目录，而 cutlass 中，执行 make 操作之后，会对程序进行编译，然后直接运行。由于在 sim 上运行需要把 gpgpusim.config 放到程序运行的目录下，所以我们需要知道是在哪个目录运行的。&lt;/p>
&lt;p>cmake 设置 library and executable 文件的存放路径：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>set&lt;span style="color:#f92672">(&lt;/span>LIBRARY_OUTPUT_PATH path&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>set&lt;span style="color:#f92672">(&lt;/span>EXECUTABLE_OUTPUT_PATH path&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>如果子目录中的某个CMakeLists.txt中设置了 set(&amp;hellip;)，就以当前文件中设置的路径为主，否则以父目录中设置的路径为主&lt;/p>
&lt;/blockquote>
&lt;p>如果都没设置呢？从简单的程序来看，在 &lt;code>build&lt;/code> 目录下 &lt;code>cmake .. XXXX&lt;/code>，在 &lt;code>build&lt;/code> 目录下生成 Makefile，执行 &lt;code>make &amp;amp; make install&lt;/code>，可执行程序就在当前（&lt;code>build&lt;/code>） 目录下。&lt;/p></description></item><item><title>Hugo_让你的博客被Google收录</title><link>https://huweim.github.io/post/blog_hugo_%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E8%A2%ABgoogle%E6%94%B6%E5%BD%95/</link><pubDate>Mon, 09 May 2022 22:17:43 +0800</pubDate><guid>https://huweim.github.io/post/blog_hugo_%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E8%A2%ABgoogle%E6%94%B6%E5%BD%95/</guid><description>&lt;h1 id="1-step">1. Step&lt;/h1>
&lt;p>网站在没有提交搜索引擎收录之前，直接搜索你网站的内容是搜不到的，因为搜索引擎不会去检索你的Github仓库。下面的解决方法&lt;/p>
&lt;h2 id="11-check-是否被谷歌收录">1.1 Check 是否被谷歌收录&lt;/h2>
&lt;p>打开谷歌搜索，在搜索框中输入&lt;/p>
&lt;p>site:https://huweim.github.io/&lt;/p>
&lt;p>如果提示说：找不到和您查询的“site:https://huweim.github.io/ ” 相符的内容或信息，说明未被收录。&lt;/p>
&lt;p>如果搜索结果的第一条就是你的博客站点，说明已被收录，不用再继续看下面的内容了。&lt;/p>
&lt;h2 id="12-提交谷歌搜索">1.2 提交谷歌搜索&lt;/h2>
&lt;p>进入Google Web Master Search Console，登录之后提交你的博客网址&lt;/p>
&lt;p>提交后需要通过 DNS 记录验证域名所有权，Github DNS 服务器配置教程：https://docs.github.com/cn/enterprise-server@3.2/admin/configuration/configuring-network-settings/configuring-dns-nameservers&lt;/p>
&lt;p>❌ 1.2.1 ssh 链接到 GitHub Enterprise Server instance，不使用域名验证方法，使用上传 html 文件的方法&lt;/p>
&lt;h2 id="13-上传-html-文件">1.3 上传 html 文件&lt;/h2>
&lt;h3 id="131-放在-content">1.3.1 放在 content&lt;/h3>
&lt;p>将下载的 html 文件放置在 &lt;code>content&lt;/code> 目录下，开启服务后，&lt;code>http://localhost:1313/googlead4e3c06724927ae/&lt;/code> 能够索引到即可，然后 push to Github. ❌&lt;/p>
&lt;h3 id="132-放在-themelayouts">1.3.2 放在 theme/layouts&lt;/h3>
&lt;p>放在 content 目录下不行，检索不到后缀 .html，比如 &lt;code>http://localhost:1313/googlead4e3c06724927ae.html/&lt;/code> 就索引不到了。折腾了半个小时，发现应该放在 &lt;code>themes\jane\layouts&lt;/code> 目录下，因为发现 &lt;code>http://localhost:1313/index.html&lt;/code> 才能够有效地索引，那么对于 &lt;code>googlead4e3c06724927ae.html&lt;/code> 也是一样的道理。&lt;/p>
&lt;p>不过仍然找不到，不是说复制到&lt;code>themes\jane\layouts&lt;/code> 目录下就能索引到，比如我复制了一个 &lt;code>404.html&lt;/code>，改名为 &lt;code>40411.html&lt;/code>，是无法直接找到的。&lt;/p>
&lt;h3 id="133-static">1.3.3 static&lt;/h3>
&lt;p>知识：static这个文件夹有一个特性就是可以将里面的文件复制到public文件夹里面。我们可以将我们自定义的页面放到这个目录下，因为它不是hugo生成的，所以不会被覆盖。&lt;/p>
&lt;p>首先在根目录下的 &lt;code>static&lt;/code> 下创建一个目录，目录名是你的页面名（googlead4e3c06724927ae.html）。然后在这个目录下创建 &lt;code>index.html&lt;/code>，把 &lt;code>googlead4e3c06724927ae.html&lt;/code> 中的内容复制到 &lt;code>index.html&lt;/code>，目的就达到了。这样，访问 &lt;code>http://localhost:1313/googlead4e3c06724927ae.html/&lt;/code> 页面时加载的就是 &lt;code>googlead4e3c06724927ae.html&lt;/code> 的内容。&lt;/p>
&lt;p>2022-05-09 11:13:15，这一方法验证成功。&lt;/p>
&lt;p>&lt;img src="./Img/Verify.png" alt="">&lt;/p>
&lt;h2 id="14-等待">1.4 等待&lt;/h2>
&lt;p>提交博客之后，需要等待一段时间才能在Google上搜到，因为Google需要时间来处理我们的请求、抓取相应网页并将其编入索引。&lt;/p>
&lt;h1 id="reference">Reference&lt;/h1>
&lt;p>&lt;a href="https://huiyu-li.github.io/2019/11/27/Tools/2019-11-27-%E8%AE%A9%E8%B0%B7%E6%AD%8C%E6%90%9C%E7%B4%A2%E5%88%B0%E8%87%AA%E5%B7%B1%E5%86%8DGitHub%E4%B8%8A%E7%9A%84%E5%8D%9A%E5%AE%A2/">https://huiyu-li.github.io/2019/11/27/Tools/2019-11-27-%E8%AE%A9%E8%B0%B7%E6%AD%8C%E6%90%9C%E7%B4%A2%E5%88%B0%E8%87%AA%E5%B7%B1%E5%86%8DGitHub%E4%B8%8A%E7%9A%84%E5%8D%9A%E5%AE%A2/&lt;/a>&lt;/p></description></item><item><title>编译运行 CUTLASS 和 cuBLAS</title><link>https://huweim.github.io/post/%E5%AE%9E%E9%AA%8C_%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8Ccutlass%E5%92%8Ccublas/</link><pubDate>Mon, 09 May 2022 22:17:43 +0800</pubDate><guid>https://huweim.github.io/post/%E5%AE%9E%E9%AA%8C_%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8Ccutlass%E5%92%8Ccublas/</guid><description>&lt;h1 id="0-前言">0. 前言&lt;/h1>
&lt;p>内容包括根据官方文档运行 CUTLASS 的实例，过程中遇到的一些问题，在 GPGPU-Sim 上运行 CUTLASS，阅读官方 doc 的笔记。&lt;/p>
&lt;p>包括根据官方文档运行 cuBLAS 的实例，过程中遇到的问题。&lt;/p>
&lt;h1 id="1-环境">1. 环境&lt;/h1>
&lt;h2 id="11-prerequisites">1.1 Prerequisites&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ git clone https://github.com/NVIDIA/cutlass
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>CUTLASS requires:&lt;/p>
&lt;ul>
&lt;li>NVIDIA CUDA Toolkit (9.2 or later required, 11.1 recommended)&lt;/li>
&lt;li>CMake 3.12+&lt;/li>
&lt;li>host compiler supporting C++11 or greater (g++ 7.3.0 or Microsoft Visual Studio 2015 recommended)&lt;/li>
&lt;li>Python 3.6+&lt;/li>
&lt;/ul>
&lt;p>CUTLASS may be optionally compiled and linked with&lt;/p>
&lt;ul>
&lt;li>cuBLAS&lt;/li>
&lt;li>cuDNN v7.6 or later&lt;/li>
&lt;/ul>
&lt;h3 id="111-cmake">1.1.1 cmake&lt;/h3>
&lt;p>官方给出了建议的环境，cmake 没有安装，apt-get install 安装的是 3.12 版本，不符合要求。手动安装一下 3.20 cmake，&lt;a href="https://gist.github.com/bmegli/4049b7394f9cfa016c24ed67e5041930">教程&lt;/a>&lt;/p>
&lt;p>注意有个 BUG，&lt;code> Could NOT find OpenSSL,&lt;/code>，&lt;code>apt-get install libssl-dev&lt;/code> 即可&lt;/p>
&lt;h3 id="112-gcc">1.1.2 gcc&lt;/h3>
&lt;h2 id="12-build">1.2 Build&lt;/h2>
&lt;p>之后就可以 build 了。这里用 Turing 架构&lt;/p>
&lt;p>Construct a build directory and run CMake.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ export CUDACXX&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>CUDA_INSTALL_PATH&lt;span style="color:#e6db74">}&lt;/span>/bin/nvcc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ mkdir build &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cd build
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">75&lt;/span> &lt;span style="color:#75715e"># compiles for NVIDIA Turing GPU architecture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 在 ~/cutlass/build/tools/library/generated/ 目录下生成 conv2d and gemm 的所有抽象组合&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">75&lt;/span> -DCUTLASS_LIBRARY_KERNELS&lt;span style="color:#f92672">=&lt;/span>all
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 仅需要 subset of gemm kernels with FP32 accumulation and FP16 input, in Ampere and Turing&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;75;80&amp;#39;&lt;/span> -DCUTLASS_LIBRARY_KERNELS&lt;span style="color:#f92672">=&lt;/span>cutlass_tensorop_s*gemm_f16_*_nt_align8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 我想这个 * 应该表示正则表达式&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ make cutlass_profiler -j16
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>需求&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">75&lt;/span> -DCUTLASS_LIBRARY_KERNELS&lt;span style="color:#f92672">=&lt;/span>cutlass_tensorop_i88*gemm_s*_256x128_*x2_tn_align*
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">75&lt;/span> -DCUTLASS_LIBRARY_KERNELS&lt;span style="color:#f92672">=&lt;/span>cutlass_tensorop_i8832gemm_s4_256x128_128x2_tn_align32
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">75&lt;/span> -DCUTLASS_LIBRARY_KERNELS&lt;span style="color:#f92672">=&lt;/span>cutlass_tensorop_i8816gemm_s8_256x128_64x2_tn_align16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ make cutlass_profiler -j16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ nsys profile --stats&lt;span style="color:#f92672">=&lt;/span>true ./turing_tensorop_gemm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="121-cmake-在做什么">1.2.1 cmake 在做什么&lt;/h3>
&lt;p>&lt;code>cmake .. -DCUTLASS_NVCC_ARCHS=75 -DCUTLASS_LIBRARY_KERNELS=all &lt;/code>，在 &lt;code>~/cutlass/build/tools/library/generated/&lt;/code> 目录下生成相应的 .cu 接口&lt;/p>
&lt;h3 id="122-make-在做什么">1.2.2 make 在做什么&lt;/h3>
&lt;p>&lt;code>Building CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/generated/gemm/cutlass_tensorop_i8816gemm_s8_256x128_64x2_tn_align16.cu.o&lt;/code>&lt;/p>
&lt;p>猜测是根据 cmake 中生成的接口文件，生成 &lt;code>cutlass_profiler&lt;/code> 能够运行/调用的目标文件。&lt;/p>
&lt;p>&lt;code>make cutlass_profiler -j16&lt;/code> 这一步之后才能使用 &lt;code>./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_i8816gemm_s8_256x128_64x2_tn_align16&lt;/code> 来运行。&lt;/p>
&lt;h3 id="123-test-in-real-gpu">1.2.3 Test in real GPU&lt;/h3>
&lt;p>工作站 GPU 是 GTX980，SM_50&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ cmake .. -DCUTLASS_NVCC_ARCHS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">50&lt;/span> -DCUTLASS_LIBRARY_KERNELS&lt;span style="color:#f92672">=&lt;/span>cutlass_simt_sgemm_128x128_8x2_nn_align1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ make cutlass_profiler -j16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./tools/profiler/cutlass_profiler --kernels&lt;span style="color:#f92672">=&lt;/span>cutlass_simt_sgemm_128x128_8x2_nn_align1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="13-build-and-run-the-cutlass-profiler">1.3 Build and run the CUTLASS Profiler&lt;/h2>
&lt;p>From the &lt;code>build/&lt;/code> directory created above, compile the the CUTLASS Profiler. 主要是 build &lt;code>build/tool/profiler&lt;/code> 目录。 ✔️&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ make cutlass_profiler -j12
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then execute the CUTLASS Profiler computing GEMM, execute the following command. ❌&lt;/p>
&lt;p>这一步果然不行，cudaGetDeviceProperties() failed for given device，找不到 device&lt;/p>
&lt;p>2022-05-08 14:41:46，✔️，在工作站上就可以用 gpgpu-sim 运行，很奇怪，明明都是同一个 Docker 环境，只是自己电脑没有 GPU 而已&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ ./tools/profiler/cutlass_profiler --kernels&lt;span style="color:#f92672">=&lt;/span>sgemm --m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4352&lt;/span> --n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --k&lt;span style="color:#f92672">=&lt;/span>4096
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">=============================&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Problem ID: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Provider: CUTLASS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Operation: cutlass_simt_sgemm_128x128_nn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Disposition: Passed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status: Success
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Arguments: --m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4352&lt;/span> --n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --A&lt;span style="color:#f92672">=&lt;/span>f32:column --B&lt;span style="color:#f92672">=&lt;/span>f32:column --C&lt;span style="color:#f92672">=&lt;/span>f32:column --alpha&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --beta&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --split_k_slices&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --batch_count&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --op_class&lt;span style="color:#f92672">=&lt;/span>simt --accum&lt;span style="color:#f92672">=&lt;/span>f32 --cta_m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">128&lt;/span> --cta_n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">128&lt;/span> --cta_k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --stages&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> --warps_m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> --warps_n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> --warps_k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --inst_m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --inst_n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --inst_k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --min_cc&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">50&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max_cc&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Bytes: &lt;span style="color:#ae81ff">52428800&lt;/span> bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> FLOPs: &lt;span style="color:#ae81ff">146064539648&lt;/span> flops
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Runtime: 10.5424 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Memory: 4.63158 GiB/s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Math: 13854.9 GFLOP/s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="14-build-and-run-cutlass-unit-tests">1.4 Build and run CUTLASS Unit Tests&lt;/h2>
&lt;h3 id="141-workspace">1.4.1 Workspace&lt;/h3>
&lt;p>From the &lt;code>build/&lt;/code> directory created above, simply build the target &lt;code>test_unit&lt;/code> to compile and run all unit tests. ❌&lt;/p>
&lt;p>这一步失败，看起来是 gcc 版本的问题。换了 gcc 版本，还是直接崩掉。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ make test_unit -j
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>----------&lt;span style="color:#f92672">]&lt;/span> Global test environment tear-down
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[==========]&lt;/span> &lt;span style="color:#ae81ff">946&lt;/span> tests from &lt;span style="color:#ae81ff">57&lt;/span> test cases ran. &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#ae81ff">10812&lt;/span> ms total&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span> PASSED &lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#ae81ff">946&lt;/span> tests.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>指定一个 unit，会 building 目录 &lt;code>test/unit/gemm/warp/CMakeFiles&lt;/code> 中的内容，仍然是找不到 GPU Device ID&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ make test_unit_gemm_warp -j
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="142-工作站">1.4.2 工作站&lt;/h3>
&lt;p>&lt;strong>工作站：&lt;/strong> 还是找不到 gpgpusim.config，这个应该找到对应的执行目录，把 config 文件复制过去即可。&lt;code>cp ~/gpgpu-sim_distribution/configs/tested-cfgs/SM75_RTX2060/* ~/cutlass/build/test/unit/gemm/warp/CMakeFiles/test_unit_gemm_warp.dir/&lt;/code>&lt;/p>
&lt;p>2022-05-09 15:31:59，猜测是在 &lt;code>/cutlass/build/test/unit/gemm/warp&lt;/code> 目录下执行，把 gpgpusim.config 文件复制过去。✔️&lt;/p>
&lt;p>可以成功运行，新的问题是之前遇到的一个问题，wmma 指令的 align syntax 错误。&lt;/p>
&lt;h2 id="15-profiler-和-test-unit-的执行有什么区别">1.5 Profiler 和 Test Unit 的执行有什么区别？&lt;/h2>
&lt;h2 id="16-gemm-运行参数">1.6 gemm 运行参数&lt;/h2>
&lt;p>cutlass_profiler 支持非常自由的运行参数，并且支持参数的批处理（用 , 间隔）。参数如下，f32 应该就是对应的 data type 设置。&lt;/p>
&lt;p>2022-05-09 20:34:24 找到了官方的 &lt;a href="https://github.com/NVIDIA/cutlass#documentation">Documentation&lt;/a>，可以看 Section 3 中的详细解释。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cutlass_profiler &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --operation&lt;span style="color:#f92672">=&lt;/span>Gemm &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8192&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8192&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8192&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --A&lt;span style="color:#f92672">=&lt;/span>f32:column &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --B&lt;span style="color:#f92672">=&lt;/span>f32:column &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --C&lt;span style="color:#f92672">=&lt;/span>f32:column &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --beta&lt;span style="color:#f92672">=&lt;/span>0,1,2 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --profiling-iterations&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --providers&lt;span style="color:#f92672">=&lt;/span>cutlass &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --output&lt;span style="color:#f92672">=&lt;/span>functional-test.csv
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>尝试修改 data type，是否有 i8? 这个语句执行结束后生成了一堆 .csv 文件，包括 conv2d, conv3d, gemm, rank_k, rank_2k，难道是一次执行了这么多程序？而且没有成功跑起来 ❌&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ ./tools/profiler/cutlass_profiler --kernels&lt;span style="color:#f92672">=&lt;/span>cutlass_tensorop_s*gemm_f16_*_nt_align8 --m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3456&lt;/span> --n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --A&lt;span style="color:#f92672">=&lt;/span>i8:column --B&lt;span style="color:#f92672">=&lt;/span>i8:column --C&lt;span style="color:#f92672">=&lt;/span>i8:column --output&lt;span style="color:#f92672">=&lt;/span>test.csv &amp;gt; ~/output/tensor_op3.log.lrr &amp;amp;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>去掉 output 选项，data type 改成 f32&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ ./tools/profiler/cutlass_profiler --kernels&lt;span style="color:#f92672">=&lt;/span>cutlass_tensorop_s*gemm_f16_*_nt_align8 --m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3456&lt;/span> --n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --A&lt;span style="color:#f92672">=&lt;/span>f32:column --B&lt;span style="color:#f92672">=&lt;/span>f32:column --C&lt;span style="color:#f92672">=&lt;/span>f32:column &amp;gt; ~/output/tensor_op3.log.lrr &amp;amp;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="2-examples">2. examples&lt;/h1>
&lt;p>使用官方 README.md 编译会因为没有 Device 而失败，那么换一个思路，尝试利用 cmake 编译运行 examples 中提供的文件。&lt;/p>
&lt;h2 id="21-cmake-编译流程">2.1 cmake 编译流程&lt;/h2>
&lt;ul>
&lt;li>编写CMakeLists.txt&lt;/li>
&lt;li>通过cmake生成Makefile&lt;/li>
&lt;li>make编译&lt;/li>
&lt;/ul>
&lt;p>cuTLASS 在 &lt;code>example&lt;/code> 目录下提供了 CMakeLists.txt。用法&lt;/p>
&lt;ul>
&lt;li>进入 example 目录，新建 build 文件夹；&lt;code>$ mkdir build; cd build&lt;/code>&lt;/li>
&lt;li>&lt;code>cmake ../&lt;/code>; cmake会在找到上级目录找到CMakeLists.txt，生成makefile和一些其它文件&lt;/li>
&lt;li>在makefile所在目录，调用make命令，会根据makefile对程序进行编译生成。&lt;/li>
&lt;/ul>
&lt;h1 id="3-documentation">3. Documentation&lt;/h1>
&lt;h2 id="32-functionality">3.2 Functionality&lt;/h2>
&lt;p>这个部分介绍了 opcode class, *&lt;strong>data type&lt;/strong>, layout. data type 正是我们所需要的。&lt;/p>
&lt;p>opcode class, including Simt, TensorOp, SpTensorOp&lt;/p>
&lt;h3 id="322-device-level-implicit-gemm-convolution">3.2.2 Device-level Implicit GEMM convolution&lt;/h3>
&lt;p>列出了 Device-level Implicit GEMM convolution 的 opcode class, data type, layout&lt;/p>
&lt;h3 id="323-warp-level-matrix-multiply-with-tensor-cores">3.2.3 Warp-level Matrix Multiply with Tensor Cores&lt;/h3>
&lt;p>TensorOp 16-by-8-by-64. 支持 int4b_t，&lt;/p>
&lt;h3 id="324-warp-level-matrix-multiply-with-cuda-wmma-api">3.2.4 Warp-level Matrix Multiply with CUDA WMMA API&lt;/h3>
&lt;p>WmmaTensorOp,&lt;/p>
&lt;p>Instruction Shape ( 16-by-16-by-16, 8-by-32-by-16)&lt;/p>
&lt;p>Warp Shapes (32x32x16, 32x64x16, 64x32x16; 32x32x16, 32x64x16, 64x32x16)&lt;/p>
&lt;h2 id="33-efficient-gemm-in-cuda">3.3 Efficient GEMM in CUDA&lt;/h2>
&lt;h3 id="331-threadblock-level-gemm">3.3.1 Threadblock-level GEMM&lt;/h3>
&lt;p>Each threadblock computes its portion of the output GEMM by iteratively loading tiles of input matrices and computing an accumulated matrix product.&lt;/p>
&lt;h3 id="332-warp-level-gemm">3.3.2 Warp-level GEMM&lt;/h3>
&lt;p>Multiple warps within a threadblock fetch data from shared memory into registers and perform computations. Warp-level GEMMs may be implemented either by TensorCores issuing mma.sync or wmma instructions or by thread-level matrix computations issued to CUDA cores. For maximum performance, access to shared memory should be bank conflict free. To maximize data reuse within the warp, a large warp-level GEMM tile should be chosen.&lt;/p>
&lt;p>使用到了 wmma 指令，shared memory。&lt;/p>
&lt;h3 id="333-thread-level-gemm">3.3.3 Thread-level GEMM&lt;/h3>
&lt;p>SGEMM, IGEMM, HGEMM, and DGEMM are computed by SIMT math instructions issued by thread-level matrix multiply procedures.&lt;/p>
&lt;p>所以现在跑的是 thread-level GEMM&lt;/p>
&lt;h2 id="34-terminology">3.4 Terminology&lt;/h2>
&lt;p>Layout: functor mapping logical coordinates of a tensor to linear offset (as LongIndex); owns stride vectors, if any.&lt;/p>
&lt;p>Operator: an object performing a computation on matrix or tensor objects. May be further refined by scope within the execution model hierarchy.&lt;/p>
&lt;p>Tile: partitions of a tensor that have constant extents and layout known at compile time&lt;/p>
&lt;h2 id="35-cutlass-profiler-">3.5 CUTLASS Profiler ⭐&lt;/h2>
&lt;p>The CUTLASS Profiler is a command-line driven test and profiling environment for CUTLASS computations defined in the CUTLASS Instance Library. The CUTLASS Profiler is capable of executing each GEMM, Sparse Gemm, Conv2d, and Conv3d kernel.&lt;/p>
&lt;p>&lt;code>cutlass_profiler&lt;/code> 就是一个封装好的脚本，运行各类程序&lt;/p>
&lt;p>进入到目录 &lt;code>build/tools/profiler&lt;/code>，运行 &lt;code>cutlass_profiler --help&lt;/code> 可以查看一些有用的信息，直接 &lt;code>cutlass_profiler --help&lt;/code> 找不到 cutlass_profiler；用 &lt;code>./cutlass_profiler --help&lt;/code> 就开始跑程序了，有点不知道怎么用这个 &amp;ndash;help。&lt;/p>
&lt;p>2022-05-09 20:22:30，还是用 &lt;code>./cutlass_profiler --help&lt;/code> 就可以跑，不过神奇的是这是用 gpgpu-sim 跑得，最后会把需要的信息 print 在屏幕上。&lt;/p>
&lt;h3 id="351-gemm">3.5.1 GEMM&lt;/h3>
&lt;p>The CUTLASS Profiler is capable of executing GEMM and Sparse GEMM problems.&lt;/p>
&lt;h4 id="3511-gemm-arguments-">3.5.1.1 GEMM Arguments ⭐&lt;/h4>
&lt;p>The complete set of arguments available to each operation may be viewed by specifying the operation name in addition to &amp;ndash;help. The argument flags and their aliases usable for GEMM appear as follows.&lt;/p>
&lt;p>可以通过 option &lt;code>--help&lt;/code> 查看完整的 operation，他这里给出的例子是 &lt;code>./tools/profiler/cutlass_profiler --operation=gemm --help&lt;/code>，所以还是得执行这个脚本吧。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>To execute kernels targeting Tensor Core operations, supply the flag &lt;code>--op_class=tensorop&lt;/code> in the command line.&lt;/p>
&lt;p>实际上，op_class 也就是选择 TensorOp or SIMT&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ ./tools/profiler/cutlass_profiler --op_class&lt;span style="color:#f92672">=&lt;/span>tensorop --m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3456&lt;/span> --n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4096&lt;/span> --k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8192&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="3513-自己运行">3.5.1.3 自己运行&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./tools/profiler/cutlass_profiler --operation&lt;span style="color:#f92672">=&lt;/span>Gemm --op_class&lt;span style="color:#f92672">=&lt;/span>tensorop --m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span> --n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span> --k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">128&lt;/span> --inst_m&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8&lt;/span> --inst_n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8&lt;/span> --inst_k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">32&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>如何使用 4-bit 进行计算&lt;/strong>: 对于 TensorOp, Instruction Shape 8-by-8-by-32 对应的是 A-int4b_t, B-int4b_t, C-int32_t，通过参数 &lt;code>--inst_m&lt;/code>, &lt;code>--inst_n&lt;/code>, &lt;code>inst_k&lt;/code> 来决定&lt;/p>
&lt;h3 id="352-conv">3.5.2 Conv&lt;/h3>
&lt;p>和 gemm 也是类似的，重点还是搞懂他们的参数。&lt;/p>
&lt;h2 id="36-gemm-api-components">3.6 GEMM API Components&lt;/h2>
&lt;p>This document focuses on device-level, threadblock-level GEMMs, warp-level GEMMs, thread-level GEMMs, and instruction-level GEMMs.&lt;/p>
&lt;h3 id="361-device-wide-gemm-api">3.6.1 Device-wide GEMM API&lt;/h3>
&lt;p>The device-wide GEMM API is embodied by the following operators&lt;/p>
&lt;ul>
&lt;li>cutlass::gemm::device::Gemm - basic GEMM operation&lt;/li>
&lt;li>cutlass::gemm::device::GemmArray - batched GEMM operation in which input matrices are read from arrays of pointers&lt;/li>
&lt;li>cutlass::gemm::device::GemmBatched - batched GEMM operation in which input matrices are separated by a constant stride&lt;/li>
&lt;li>cutlass::gemm::device::GemmSplitKParallel - GEMM operation that partitions the GEMM K dimension then launches a separate reduction kernel&lt;/li>
&lt;/ul>
&lt;p>都在 &lt;code>cutlass/include/cutlass/gemm/device/&lt;/code> 目录下，basic GEMM 对应 &lt;code>gemm.h&lt;/code> 文件&lt;/p>
&lt;h1 id="4-在-gpgpu-sim-上运行">4. 在 GPGPU-Sim 上运行&lt;/h1>
&lt;h2 id="41-cutlass-sim">4.1 cutlass-sim&lt;/h2>
&lt;p>尝试了 Admodt 提供的 &lt;code>https://github.com/gpgpu-sim/cutlass-gpgpu-sim&lt;/code>，仍然是 syntax error，估计是新版本编译的 PTX 有问题。2022-05-26 15:53:30，确实如此。&lt;/p>
&lt;p>不同 CUDA 版本对应不同的 PTX .version，得到的 PTX 指令是不一样的。这就是为什么 CUDA 11.4 wmma 指令会报错。&lt;/p>
&lt;h2 id="42-step">4.2 Step&lt;/h2>
&lt;ul>
&lt;li>下载并安装 CUDA Toolkit 9.2，使用这个版本编译模拟器。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;p>接下来是编译运行 cuBLAS 的过程&lt;/p>
&lt;h1 id="1-nvidia-samples">1. NVIDIA Samples&lt;/h1>
&lt;p>&lt;a href="https://github.com/NVIDIA/cuda-samples/">https://github.com/NVIDIA/cuda-samples/&lt;/a> 在 library 目录中有提供调用 cublas 的代码，果然官方提供的资源才是最好的。git clone 下来就可以在 A10 上编译运行。重点是理解不同 API 的含义，需要的 parameter，gemm 的 data type, shape 等等，这一点需要多看文档。&lt;/p>
&lt;h2 id="11-如何确定调用了-tensor-core">1.1 如何确定调用了 tensor core?&lt;/h2>
&lt;blockquote>
&lt;p>Tensor cores were first introduced with Volta GPUs (compute capability&amp;gt;=sm_70) and significantly accelerate matrix multiplications. Starting with cuBLAS version 11.0.0, the library will automatically make use of Tensor Core capabilities wherever possible, unless they are explicitly disabled by selecting pedantic compute modes in cuBLAS (see cublasSetMathMode(), cublasMath_t).&lt;/p>
&lt;/blockquote>
&lt;p>文档中说 cublas 会自动调用 tensor core&lt;/p>
&lt;h1 id="2-documentation">2. Documentation&lt;/h1>
&lt;h2 id="21-using-the-cublas-api">2.1 Using the cuBLAS API&lt;/h2>
&lt;blockquote>
&lt;p>cuBLAS库提供了现成的矩阵乘法算子，例如&lt;code>cublasGemmEx&lt;/code>和&lt;code>cublasLtMatmul&lt;/code>。其中后者是轻量级版本，API调用更灵活。&lt;/p>
&lt;/blockquote>
&lt;p>cublasGemmEx&lt;/p>
&lt;h2 id="211-general-description">2.1.1 General Description&lt;/h2>
&lt;blockquote>
&lt;p>应该注意的是，该库将选择启用 Tensor Core 的实现，只要它确定它将提供最佳性能。&lt;/p>
&lt;/blockquote>
&lt;p>cuBLAS 11.0.0 之后支持任何 size 的矩阵，只是对齐的 size 能够更好地发挥 Tensor core 的性能。&lt;/p>
&lt;h2 id="212-cublas-datatypes-reference">2.1.2 cuBLAS Datatypes Reference&lt;/h2>
&lt;p>&lt;code>cublasDataType_t handle&lt;/code>，一个有关cuBLAS库的上下文的句柄，之后需要传递给API函数，即计算乘法的函数&lt;/p>
&lt;p>&lt;code>cublasOperation_t&lt;/code>, N, 非转置；T，转置；C，共轭转置。&lt;/p>
&lt;p>&lt;code>cublasGemmEx&lt;/code> 中的 &lt;code>cublasGemmAlgo_t&lt;/code>，&lt;code>cublasGemmAlgo_t&lt;/code> 最高支持 sm_75，sm_80 已经不支持了，所以在 sm_80 中指定了也是无效的，在 sm_80 中所有枚举都等同于 &lt;code>CUBLAS_GEMM_DEFAULT&lt;/code> 或者 &lt;code>CUBLAS_GEMM_DEFAULT_TENSOR_OP&lt;/code>。在更新的架构中也会 deprecated&lt;/p>
&lt;p>&lt;code>cudaDataType_t&lt;/code>, 直接作为 &lt;code>cublasGemmEx&lt;/code> 的参数，支持 int8 到 double 类型。&lt;/p>
&lt;h3 id="213-cublas-level---3-function-reference">2.1.3 cuBLAS Level - 3 Function Reference&lt;/h3>
&lt;p>&lt;code>cublasSgemm&lt;/code>, &lt;code>cublasDgemm&lt;/code>, &lt;code>cublasCgemm&lt;/code>, &lt;code>cublasZgemm&lt;/code>, &lt;code>cublasHgemm&lt;/code> 应该是比较初始的 API。&lt;/p>
&lt;h3 id="214-blas-like-extension">2.1.4 BLAS-like Extension&lt;/h3>
&lt;blockquote>
&lt;p>&lt;code>cublasGemmEx()&lt;/code>. This function is an extension of cublas&lt;!-- raw HTML omitted -->gemm that allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run. Supported combinations of arguments are listed further down in this section.&lt;/p>
&lt;/blockquote>
&lt;p>自定义 data types，自己在 int8 中就用的这个 API，支持 sm_50 以上的架构。&lt;/p>
&lt;h2 id="22-using-the-cublaslt-api">2.2 Using the cuBLASLt API&lt;/h2>
&lt;p>cuBLASLt, a lightweight library dedicated to GEMM&lt;/p>
&lt;blockquote>
&lt;p>The cuBLASLt in general does not guarantee to support all possible sizes and configurations.&lt;/p>
&lt;/blockquote>
&lt;p>不一定支持任何 size&lt;/p>
&lt;h3 id="221-cublaslt-datatypes-reference">2.2.1 cuBLASLt Datatypes Reference&lt;/h3>
&lt;p>&lt;code>cublasLtMatmulTile_t&lt;/code> 提供多种 tile size&lt;/p>
&lt;h2 id="23-using-the-cublasxt-api">2.3 Using the cuBLASXt API&lt;/h2>
&lt;p>The cuBLASXt API of cuBLAS exposes a multi-GPU capable Host interface&lt;/p>
&lt;p>cuBLASXT 似乎可以调用多个 GPU，比如有 4 A10 in QZ Server，code 限制只用两个 GPU。通过 cuBLASXT 执行 FP32 gemm，TFLOPS 应该不具备参考性了。&lt;/p>
&lt;p>参考：https://github.com/sxzhang1993/Run-cutlass-with-gpgpu-sim&lt;/p></description></item><item><title>关于 Deep Work</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_deep_work/</link><pubDate>Sun, 08 May 2022 20:56:54 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_deep_work/</guid><description>&lt;h1 id="0-前言">0. 前言&lt;/h1>
&lt;p>Why read? 最近同步推进的任务比较多，包括雅思/托福，LeetCode刷题（不能说不找实习就荒废掉了），自己lead的课题（可以先读paper慢开始，但是对本人提升很重要），科研实习的课题（这个必须定时有反馈，留下不错的印象），公司的一些活儿（按时完成且上心），锻炼（荒废了一段时间但是必须），AICS 课程（好在这个和目前的研究有了相关性）。&lt;/p>
&lt;p>因此，需要进行详细而且比较严格的时间管理，才能同时做好这么多事情。&lt;/p>
&lt;h2 id="01-知乎">0.1 知乎&lt;/h2>
&lt;p>知乎一位作者写的自我观察非常好，在我需要去完成论文的那段时间，我也是进入了这样一种状态。这个任务必须去交付（DDL 4.14），而且需要交互（修改之后给娄老师，并且完成娄老师交代的改动），是一种紧张，紧凑的任务，但是不觉得效率很高，写初稿反而磕磕盼盼，但是事后来看，在一段比较短的时间内（4.4-4.13）产出了一篇有结构的初稿并且慢慢完善并投稿。而现在，你需要记住、重拾这种状态，并把他融入到你的日常工作中。&lt;/p>
&lt;blockquote>
&lt;p>我观察到自己真正非常高效地推进困难任务，产出自己想要的结果往往发生在连续的长时间的工作中，并且我的心理状态是这样的：我一定要在这个任务上有重大突破或者完成它，不然饭也先别吃了，于是我往往会在同一个任务上从下午一点工作到六七点，或者从下午五点工作到晚上十点十一点，碰到问题就解决问题，没有头绪就尝试各种可能推进任务的可能性。别看这一下子时间花的挺长，产生的结果和进展常常令我感到惊讶。以我常规的工作方式（如每天固定投入1-2小时），往往拖了一两周都毫无进展的困难任务，我一整个下午就解决了。这样的情况发生了很多次，并且反差如此的强烈，于是我打算将这个现象思考清楚，总结出其中的原因。我很自然地联想到了Cal Newport的《Deep Work》，隐隐感觉和他提出的“深度工作”的这个概念有关。当然在总结他书中的观点之前，我会想梳理一下自己的思考。&lt;/p>
&lt;/blockquote>
&lt;h2 id="02-知乎总结">0.2 知乎总结&lt;/h2>
&lt;blockquote>
&lt;p>最小阻力定律 （The Law of Least Resistance），我们人类天生倾向于选择一条阻力最小的路径，也就是说我们倾向于回避困难任务，选择更轻松、更简单的事情先做。因此在常规工作方式下，你太容易不自觉地回避困难任务了，虽然你可能计划要在某件事上工作1-2小时，但其实在这些时间内，你都很可能有意无意地远离真正能推进任务的困难的点，只是做些表层的简单的事，并且很容易被别的事情分心从而中断在这个任务上的投入（比如，查看回复邮件，肚子饿了想着先回家吃饭）；于是你在这个困难任务上真正的有效投入其实非常有限，也就不奇怪几周时间也没什么进展了。而另一种深度工作方式，由于我在一开始就和自己约定好了：不解决问题，饭都不吃，做好了打持久战的准备，所以退无可退，避无可避，只能直面最困难的点。&lt;/p>
&lt;/blockquote>
&lt;p>毫无疑问，我会倾向于在简单任务（LeetCode简单题，回复邮件，刷知乎，读paper的intruction和abstract，浅层的阅读代码）上花费时间来获取反馈、满足感和完成感。面对困难任务时，总是在解决掉困难任务的一个简单部分\有所进展之后，就想回到床上躺一会儿。这时最近（2022-05-07 14:40:33）工作的普遍现象。&lt;/p>
&lt;blockquote>
&lt;p>尽量不要切换任务，因为有各种成本（意志力成本，任务启动成本）。意志力成本：我们对困难任务多少会有些恐惧，所以启动它本身就要消耗一些意志力，如果你按照常规的工作方式拖着很多天都没解决，那么每次启动这个任务的时候都要一次又一次地消耗意志力成本。任务启动成本：启动任务直到在最困难的点想办法推进任务之前，你需要回顾涉及到的各种背景知识、约束条件和细节，因此你在不同的时间段启动任务时，每次这个任务启动成本也是不能省的。因此一件事干大半天、一整天的收益和产生的价值远远超过你一天在几个任务间切换，每个任务各花1-2小时。当我们把指数效应放到较大的时间尺度下，我们看到的现象是一件事你坚持很多天，很多个月，很多年，越到后期你越能获得指数级的丰厚回报；其实我们也可以把指数效应放到较小的时间尺度下，比如一天，在一天中，你坚持做一件事一直做下去，越到后面你也越能获得指数级的回报，而如果你只在一件事上花了1-2小时，很可能你还没等到收益指数级增长的时候。（而像盖茨这样的大神，在创业初期写程序的时候，可能连续几周几个月连轴转都在高度专注地推进一件事，可以想象到后面所产生的恐怖的指数级的价值。）&lt;/p>
&lt;/blockquote>
&lt;p>没错，启动任务会消耗大量的精神力，而自己老是在启动后自我中断，这无疑大大降低了自己的效率。自己能认识到中断任务的负面反映，知道过于固定的任务时间（比如严格 2h paper reading）是不可取的，这也是我寻求 time management 方法的原因。&lt;/p>
&lt;blockquote>
&lt;p>当然不是所有事情都适合用深度工作的方式，现在看来自认为的困难任务是适合的，而比较简单但工作量巨大的任务却不适合，原因其实就在上面写的两条中了。
更新一条：长时间专注于同一个任务，越到后面越会有指数级的效应和产出，还很可能和cognitive workload theory有关，越到后面，因为在之前的时间里你已经将那些基础性的东西都弄得很熟悉了，所以也就不再占据你的认知工作负荷（cognitive workload），这样到最后你大脑的认知思考能力全部被解放出来集中在最难的点上，不断地取得之前浅度工作（shallow work）所不能取得的突破。&lt;/p>
&lt;p>Cal自己是”深度工作“的践行者，他的一整天都会深度围绕着一项核心的工作，然后将那些无法避免的浅层工作压缩到自己日程的边角料时间段里。这样一天仅仅三到四个小时的无中断的高度专注的工作，一周五天就能创造出很多有价值的东西。Cal自己还是两个孩子的父亲，这样的工作方式也让他有更多的时间留给家人，并且让他有时间阅读惊人数量的书籍。&lt;/p>
&lt;/blockquote>
&lt;h1 id="1-youtube">1. Youtube&lt;/h1>
&lt;p>举了几个例子，JK 罗琳为了避免干扰，找了家酒店写作，她原本没打算一直待在那里。但是第一天工作的效果很好，所以他就继续下去了，直到完成《哈利波特》。&lt;/p>
&lt;p>比尔盖茨 1974 年用 deep work 理论完成 basic 的第一版，在 8 个周的时间内，他一直在写代码，累了就在键盘上趴一会儿，睡一个小时。最后 8 周完成了这个软件。&lt;/p>
&lt;p>MIT 和一些学校的学生、老师发现 deep work 对他们研究工作的帮助，于是创作了这本书。普通人可能没法像他们那样一次在一个工作上投入 8 weeks，但是仍然有进入 deep work 的方法。&lt;/p>
&lt;p>deep work 很难，也很稀少。我们的世界有很多分散注意力的东西，比如合作时需要立刻回邮件，回消息，比如社交媒体的影响，刷虎扑，微博等等。&lt;/p>
&lt;p>一些 deep work 的方法&lt;/p>
&lt;h2 id="11-schedule-distraction">1.1 Schedule Distraction&lt;/h2>
&lt;p>大概就是说统一一个时间来处理分散注意力的事项。这个和我最近在做的其实很像，比如昨天（2022-05-07）在想去看一些信息的时候（研究院）没有立刻去，而是把它记录下来，之一统一处理。这是一个避免分散注意力，保持高效的方法。玩手机，回邮件也应该有意识地去克制。&lt;/p>
&lt;h2 id="12-deep-work-ritual">1.2 Deep Work Ritual&lt;/h2>
&lt;p>简而言之就是，保持节奏，养成习惯。养成习惯这个技巧，在有意识地学习。&lt;/p>
&lt;p>一周 7 天，你可能习惯在晚上进入 deep work，那就 7 天都保持这个节奏，把会议，事项安排到早上，把需要 deep work 的工作安排在晚上。&lt;/p>
&lt;p>研究表明新手大概能保持 1h deep work，但是 deep work master 能够保持大概 4h，要争取做到这个强度。&lt;/p>
&lt;h2 id="13-evening-shundown">1.3 Evening Shundown&lt;/h2>
&lt;p>休息好很重要。没做完的事情，拆分成比较具体的事情，写入第二天的计划中。因为此时一天的工作还保存在大脑的 cache 中，主要是还没有断开连接，列好计划，第二天可以减少很多开销。对于需要 deep work 的工作，需要及时 shutdown，列好计划之后在第二天留出充足的时间来完成，而不是强行低效率地去磨。&lt;/p>
&lt;h1 id="reference">Reference&lt;/h1>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/39759254">https://zhuanlan.zhihu.com/p/39759254&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=gTaJhjQHcf8">https://www.youtube.com/watch?v=gTaJhjQHcf8&lt;/a>&lt;/p></description></item><item><title>如何读论文（李沐）</title><link>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_%E5%A6%82%E4%BD%95%E8%AF%BB%E8%AE%BA%E6%96%87%E6%9D%8E%E6%B2%90/</link><pubDate>Sun, 08 May 2022 20:16:17 +0800</pubDate><guid>https://huweim.github.io/post/%E6%80%BB%E7%BB%93_%E5%A6%82%E4%BD%95%E8%AF%BB%E8%AE%BA%E6%96%87%E6%9D%8E%E6%B2%90/</guid><description>&lt;h1 id="0-前言">0. 前言&lt;/h1>
&lt;p>最近（2022-05-08 16:40:17）看到了李沐的这个系列，之前没看过。其实读研以来陆陆续续也看过很多阅读文章，写文章的技巧，教程等等，自己也有一些感触和经验。而最近做完 GPU 微架构相关的一部分课题之后，恰好会接触一个比较新的领域。去读这种新的领域的文章，慢慢开拓到熟悉，也是一种体验。正好这个时间点，就结合自己之前读论文的经验，学到的技巧，结合李沐大神在视频中传授的方法，以及他阅读和看待文章的视角，来做一个总结和记录。&lt;/p>
&lt;h1 id="1-方法">1. 方法&lt;/h1>
&lt;p>第一遍，title, abs, conclusion, conclusion 一般和 abs 结构是一样的，可能会有更加详细的数据，性能提升 XX% 等等; 浏览 method 和实验部分的图表，大概花十几分钟看看这篇文章在讲什么，是不是值得继续读下去。&lt;/p>
&lt;p>标题中的作者，学校，也很重要，你对领域内比较熟悉的话，对这些作者肯定也是有所了解的。&lt;/p>
&lt;p>第二遍，精读，知道每个 part 在讲什么，但是不需要知道所有的细节，看懂重要的 figure description. 判断是不是自己领域内的，&lt;strong>圈出没看过的参考文献&lt;/strong>，如果这篇比较难读，可以从参考文献中的经典入手。&lt;/p>
&lt;p>第三遍，带着自己的思考，提出的是什么问题，你能想到这个问题吗？用的什么解决方法，你来做的话会用什么方法？如果用同样的方法，你能做出什么优化？你能拓展出新的东西吗，他的做法可参考吗，你能够复现吗，他有什么地方讲得有问题吗？等等等等，带着一系列问题，给自己设置一系列问题。这一遍看完，合上文章之后，也知道文章在讲什么，也可以给别人讲述出技术细节（所以有人讨论，有一个输出的机会也是很重要的）。&lt;/p>
&lt;h1 id="2-第二遍精读关于-alexnet">2. 第二遍精读，关于 AlexNet&lt;/h1>
&lt;p>看视频，从李沐阅读 paper 的视角，去思考一位领域内大牛看到文章中的一些句子，脑海中的想法是怎样的。&lt;/p>
&lt;h2 id="21-intro">2.1 Intro&lt;/h2>
&lt;p>看到第一段，李沐不光介绍了段落表达的意思，还做了额外的科普，过拟合是 ML 中的一大派别，我想他应该能继续讲出哪些人在做这方面的工作。推测出，从现在的眼光来看，正则好像又没那么重要。&lt;/p>
&lt;p>第一段主要是讲故事，比如自己，需要介绍 GPU 广泛应用于XXX，以及 warp 调度等等。&lt;/p>
&lt;p>之后，分析文章写法的问题。在介绍完数据集之后，直接就开始提，用 CNN 解决问题。写法的问题在于，当时的主流并非 CNN，其实应该介绍一些主流算法，然后过渡到 CNN。（同意这个观点）&lt;/p>
&lt;p>最后一段开始讲 paper 的贡献，讲自己的工作，做了什么，做了 5 层卷积，3 层全连接，等等。他们用了一些新的技术（挖坑，可以继续往下做），阐述了他们不光卖点很好，也有很多新的东西。&lt;/p>
&lt;blockquote>
&lt;p>从这个角度管中窥豹，一篇文章有好的结果本身已经是一个卖点了，但是他们的方法可能过于复杂，难以复现，别的研究者没法跟进，那么文章可能能中但是不太会有人跟；另一方面，有好的结果，同时有创新的东西，大家都可以 follow，那么这个工作很可能是一篇开山之作，挖坑之作，会有比较高的引用率。&lt;/p>
&lt;/blockquote>
&lt;h2 id="22-the-dataset">2.2 The DataSet&lt;/h2>
&lt;p>AlexNet 这篇文章标题带了 ImageNet，因此大概介绍了一下数据集。自己写的时候，也是大概交代一下 GPU 的 baseline 架构背景，和简单的领域内知识。这个部分对同行来说不是很重要，完善文章结构，没有涉及到 paper 的 idea。&lt;/p>
&lt;p>最后一段还说明了对图片做的一些处理。&lt;/p>
&lt;p>什么是 End to end: 原始的图片\文本直接进去，不做任何特征提取，神经网络能帮你做出来。end to end 是他当时的亮点，但是当时由于历史局限性，可能没有发现。&lt;/p>
&lt;h2 id="23-the-architecture">2.3 The Architecture&lt;/h2>
&lt;p>Section 3.1 首先介绍了 ReLU。有一些技术细节，其实刚接触的时候是不知道具体意思的，没有关系，可以圈一下，记录一下。比如 3.1 的 saturating non-linearity, non-saturating non-linearity.&lt;/p>
&lt;p>Section 3.2 介绍了 Training on Multiple GPU, 如何划分 GPU 的。这个部分，站在 ML 研究者角度，第二遍可以略过，除非需要复现，因为可能看不懂，而且不是 ML 方法上的东西；对于 系统/体系结构 研究者，可以关注一下。&lt;/p>
&lt;p>Section 3.3 讲了 LRN 的一些细节，第二遍读的时候也可以忽略掉&lt;/p>
&lt;p>Section 3.4 讲了 Overlap Pooling，实际上一般 pooling 是没有 overlap，如果不知道 pooling 是什么可以先跳过，去看了 pooling 之后再细读。这个地方可以大概知道用的 pooling 方法不太常规。&lt;/p>
&lt;p>文章类似于一个技术报告的风格，没有 highlight, 宣传做了什么。&lt;/p>
&lt;p>李沐老师对于 Figure 2 的讲解通俗易懂。&lt;/p>
&lt;p>为什么模型图切到两块 GPU？可能是 Alex 刚好是这么做了，并且写了大量的代码去支撑这个工作，所以将其作为一个贡献放在里面。&lt;/p>
&lt;h2 id="24-reducing-overfitting">2.4 Reducing Overfitting&lt;/h2>
&lt;p>介绍如何降低过拟合。&lt;/p>
&lt;p>Section 4.2 Dropout, 很多模型融合到一起太贵了&lt;/p>
&lt;blockquote>
&lt;p>这两部分主要是关于技术细节，所以结合自己经历的感触可能不是特别多。&lt;/p>
&lt;/blockquote>
&lt;h2 id="25-details-of-learning">2.5 Details of Learning&lt;/h2>
&lt;p>介绍模型是如何训练的。逐渐出现了一些看起来玄学的部分，比如一些参数的设置。&lt;/p>
&lt;p>当时需要训练 5-6 天，在两个 GPU 上。&lt;/p>
&lt;h2 id="26-result">2.6 Result&lt;/h2>
&lt;p>进入到了实验部分，对于不复现的人来说，或者不是同领域的人，实验相对没有那么重要。&lt;/p>
&lt;h1 id="3-如何找到研究想法">3. 如何找到研究想法&lt;/h1>
&lt;h2 id="31-打补丁">3.1 打补丁&lt;/h2>
&lt;p>在原作者想法的基础上，做一些加减法。比如他用了新的 loss，用了两个（我能不能用 3 个？做数据增强，我能不能做？），用故事把你的补丁串联起来。&lt;/p>
&lt;p>如果一篇文章已经是打补丁的论文，可能难以找到机会。如果脑洞比较大的工作，可以做更多尝试。比如 iPAWs 已经是 16 年的文章，follow 的人有但不是特别多，回头来看可能不是一个特别好的选择。&lt;/p></description></item><item><title>Matplotlib</title><link>https://huweim.github.io/post/%E7%BC%96%E7%A8%8B_matplotlib/</link><pubDate>Tue, 19 Apr 2022 20:29:07 +0800</pubDate><guid>https://huweim.github.io/post/%E7%BC%96%E7%A8%8B_matplotlib/</guid><description>&lt;h4 id="plthist">plt.hist()&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>data &lt;span style="color:#f92672">=&lt;/span> [&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>,&lt;span style="color:#ae81ff">5&lt;/span>,&lt;span style="color:#ae81ff">6&lt;/span>,&lt;span style="color:#ae81ff">5&lt;/span>,&lt;span style="color:#ae81ff">8&lt;/span>,&lt;span style="color:#ae81ff">7&lt;/span>,&lt;span style="color:#ae81ff">64&lt;/span>,&lt;span style="color:#ae81ff">66&lt;/span>,&lt;span style="color:#ae81ff">5&lt;/span>,&lt;span style="color:#ae81ff">8&lt;/span>,&lt;span style="color:#ae81ff">8&lt;/span>,&lt;span style="color:#ae81ff">8&lt;/span>,&lt;span style="color:#ae81ff">8&lt;/span>,&lt;span style="color:#ae81ff">8&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>hist(data, bins&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>, label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;weight&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="参数">参数&lt;/h5>
&lt;!-- raw HTML omitted -->
&lt;ul>
&lt;li>bins，直方图的 bucket，bins = 3 表示将区间分为 3 buckets。数值范围是 0-66，那么大概分为 0-22，23-45，45-67 三个 bucket，然后根据 &lt;code>data&lt;/code> 中的情况，往这几个 bucket 投票/计数。&lt;/li>
&lt;li>range，x 轴的显示范围&lt;/li>
&lt;/ul>
&lt;h4 id="pltplot">plt.plot()&lt;/h4>
&lt;h5 id="ylabel">y.label()&lt;/h5>
&lt;p>给出y轴的名称&lt;/p>
&lt;p>论文中时常需要设置 y 轴参数&lt;/p>
&lt;p>plt.legend() 显示 label&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>legend(loc&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;upper right&amp;#39;&lt;/span>, ncol&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>, fontsize&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">30&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ncol 设置行列如何排布，有时候需要排成一排，有时候需要排成一列&lt;/p>
&lt;h5 id="pltxticks-pltyticks">plt.xticks(), plt.yticks()&lt;/h5>
&lt;p>关于刻度的一些设置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>xticks(locs, [labels], &lt;span style="color:#f92672">**&lt;/span>kwargs) &lt;span style="color:#75715e"># Set locations and labels&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>yticks(np&lt;span style="color:#f92672">.&lt;/span>arange(&lt;span style="color:#ae81ff">0.5&lt;/span>, &lt;span style="color:#ae81ff">1.3&lt;/span>, step&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.05&lt;/span>),family&lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;Arial&amp;#39;&lt;/span>, weight&lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;normal&amp;#39;&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">20&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>np.arange(0.5, 1.3, step=0.05) 设置刻度的范围和步长&lt;/p>
&lt;p>不显示 y 轴刻度&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>yticks([])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="pltylim05-13">plt.ylim((0.5, 1.3))&lt;/h5>
&lt;p>设置坐标轴范围，为 0.5-1.3&lt;/p>
&lt;h5 id="plttitle">plt.title()&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>title(filename, {&lt;span style="color:#e6db74">&amp;#39;family&amp;#39;&lt;/span> : &lt;span style="color:#e6db74">&amp;#39;Arial&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;weight&amp;#39;&lt;/span> : &lt;span style="color:#e6db74">&amp;#39;normal&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;size&amp;#39;&lt;/span> : &lt;span style="color:#ae81ff">20&lt;/span>})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="pltsubplot">plt.subplot()&lt;/h5>
&lt;p>这个是画图的时候比较常用的&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 分成2x2，占用第一个，即第一行第一列的子图&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>subplot(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filename &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;cifar&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> show_graph_cifar(filename)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 分成2x2，占用第一个，即第一行第一列的子图&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>subplot(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filename &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;resnet&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> show_graph_resnet(filename)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 分成2x1，占用第二个，即第二行&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>subplot(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filename &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;alex&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> show_graph_alex(filename)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>效果&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h5 id="pltlegend">plt.legend()&lt;/h5>
&lt;p>参数 &lt;code>ncol&lt;/code> 设置列数，可以把他们排成一行&lt;/p>
&lt;h5 id="pltgrid">plt.grid&lt;/h5>
&lt;p>如何把网格放在最底层？zorder 似乎没有效果。虽然警告了 &lt;code>zorder unknown word&lt;/code>，但是设置之后还是 work 了。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>grid(visible&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, which&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;major&amp;#39;&lt;/span>, axis&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;y&amp;#39;&lt;/span>, linestyle&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;--&amp;#39;&lt;/span>, zorder&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>zorder 设置值比 bar 更小，就可以放在最底层了&lt;/p>
&lt;h5 id="pltbar-绘制直方图和叠加直方图">plt.bar() 绘制直方图和叠加直方图&lt;/h5>
&lt;p>叠加直方图这次没做，不过不是很难，大概是给 bar 指定一个起始位置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>bar(x_length &lt;span style="color:#f92672">+&lt;/span> bar_width,y_8,width&lt;span style="color:#f92672">=&lt;/span>bar_width,label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;8 entry&amp;#34;&lt;/span>, color&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;darkorange&amp;#39;&lt;/span>, edgecolor&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;black&amp;#39;&lt;/span>, zorder&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>加上边框 edgecolor 会好看很多。&lt;/p>
&lt;h5 id="pltsubplots_adjustbottom035">plt.subplots_adjust(bottom=0.35)&lt;/h5>
&lt;p>提前设置好 top, bottom, 各种空白等参数，让打出来的图片可以直接使用&lt;/p>
&lt;h5 id="color-参数">color 参数&lt;/h5>
&lt;!-- raw HTML omitted --></description></item><item><title>CSAPP阅读笔记</title><link>https://huweim.github.io/post/%E6%95%99%E6%9D%90_csapp%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link><pubDate>Tue, 19 Apr 2022 18:34:28 +0800</pubDate><guid>https://huweim.github.io/post/%E6%95%99%E6%9D%90_csapp%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid><description>&lt;h1 id="0-前言">0. 前言&lt;/h1>
&lt;p>最近刚把 adaptive 调度的一部分工作写成了文章，在 SOCC DDL（4.14） 投了出去。之后会准备oneflow面试，如果可以拿下这份远程实习，对工程能力和并行计算优化方面会有更好的优化。因此，希望可以好好准备一下oneflow二面。准备的方式就是浏览一遍CSAPP，一方面是复习体系结构中的一些知识，另一方面再构建一下对整个计算机结构，操作系统，C语言的了解和认识。&lt;/p>
&lt;h2 id="01-知识点挖坑">0.1 知识点挖坑&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>浮点数的表示&lt;/p>
&lt;/li>
&lt;li>
&lt;p>流水线的设计（第四章）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="12">1.2&lt;/h2>
&lt;p>预处理：把 # 开头的一些代码替换成对应的内容，比如 &lt;code>#include&amp;lt;stdio.gh&amp;gt;&lt;/code>, &lt;code>#define&lt;/code>，或者是之前见过的，别人写代码时用到的宏定义来代替 for 循环等操作。hello.c -&amp;gt; hello.i&lt;/p>
&lt;p>编译器：得到汇编语言，也就是各种指令。生成 hello.s (s 代表 assembly？)&lt;/p>
&lt;p>汇编阶段：translate assembly to machine code.&lt;/p>
&lt;p>链接：合并，比如 hello 程序调用了 printf 函数，which is in &lt;code>printf.o&lt;/code>，链接器（ld）负责做这种合并&lt;/p>
&lt;p>大型项目往往会出现和链接器相关的错误。比如你定义了2个名字相同的全局变量，静态库和动态库区别？第七章会介绍&lt;/p>
&lt;h3 id="141-系统的硬件组成">1.4.1 系统的硬件组成&lt;/h3>
&lt;p>这里有描述架构和微架构的区别&lt;/p>
&lt;blockquote>
&lt;p>指令集架构描述的是每条机器代码指令的效果；微体系结构（微架构）描述的是处理器实际上是如何实现的。&lt;/p>
&lt;/blockquote>
&lt;h3 id="141-运行-hello-程序">1.4.1 运行 hello 程序&lt;/h3>
&lt;p>DMA，data 不通过处理器，直接从磁盘到达主存&lt;/p>
&lt;h2 id="15-cache-至关重要">1.5 Cache 至关重要&lt;/h2>
&lt;p>cache，SRAM&lt;/p>
&lt;h2 id="17-os管理硬件">1.7 OS管理硬件&lt;/h2>
&lt;p>两个基本功能：防止硬件被失控的应用程序滥用；向application提供简单一致的机制来控制复杂而大不相同的低级硬件设备&lt;/p>
&lt;p>PS: 硬件设备的架构、配置各不相同，而OS正是这样的一个接口。&lt;/p>
&lt;h3 id="171-进程process">1.7.1 进程（process）&lt;/h3>
&lt;p>OS 跟踪 进程 运行需要的所有状态信息，这种状态称为&lt;strong>上下文&lt;/strong>，包括 PC，register file。&lt;/p>
&lt;p>上下文切换：OS 决定把控制权转移到另一个进程，进行上下文切换&lt;/p>
&lt;p>PS：从微架构来理解，其实就是执行的下一条指令PC以及数据（来自register file）换掉了，换成了OS层面的另一个进程。&lt;/p>
&lt;h3 id="172-线程">1.7.2 线程&lt;/h3>
&lt;p>这个是OS中的线程概念，一个进程由多个线程的执行单元组成&lt;/p>
&lt;h3 id="173-虚拟内存-">1.7.3 虚拟内存 ⭐&lt;/h3>
&lt;p>抽象概念，为每个进程提供独占DRAM的假象。&lt;/p>
&lt;p>&amp;ldquo;&lt;img src="D:%5CShanghaiTech%5C2022-Spring%5CCSAPP%5CNote%5CImg%5CVirtual_Memory.png" alt=""> align=left&amp;rdquo;&lt;/p>
&lt;p>从下到上（低地址向高地址）：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>代码和数据： 所有进程代码都是从同一固定地址开始。之后是存放全局变量的位置。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>堆：malloc, free 这种标准库函数来进行申请，程序员来管理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>共享库：存放C标准库，数学库。第七章动态链接部分会介绍。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>栈：编译器管理，实现函数调用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>内核虚拟内存：application 无法直接调用，必须调用kernel来执行&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="191-amdahls-law">1.9.1 Amdahl&amp;rsquo;s Law&lt;/h3>
&lt;p>HPC，并行加速&lt;/p>
&lt;h2 id="21-信息存储">2.1 信息存储&lt;/h2>
&lt;p>浮点数：以2为基数的科学计数法。&lt;/p>
&lt;h3 id="212-字数据大小">2.1.2 字数据大小&lt;/h3>
&lt;p>字长（word size）决定虚拟地址空间的最大大小。对于字长 $w$，虚拟地址的范围为 0~$2^w-1$。近几年64位字长的机器比较多，32位字长使得虚拟地址空间为4GB。大多数64位机器可以运行32位机器编译的程序，向后兼容性。&lt;/p>
&lt;h3 id="213-寻址和字节顺序">2.1.3 寻址和字节顺序&lt;/h3>
&lt;p>这里面比较重要的知识是大端小端。&lt;/p>
&lt;p>知识前提：多字节对象（比如一个int）被存储为连续的字节序列。假设一个 int 型变量 x，取地址为 0x100，int 32bit，4 字节，实际上他的 4 个字节存储在 0x100, 0x101, 0x102, 0x103，这个是还未对齐到 cache line 的，每个字节的地址。&lt;/p>
&lt;p>假设 x = 0x01234567&lt;/p>
&lt;p>小端就是最低有效字节（67）在 0x100 地址处&lt;/p>
&lt;p>&amp;ldquo;&lt;img src="D:%5CShanghaiTech%5C2022-Spring%5CCSAPP%5CNote%5CImg%5CByte_address.png" alt=""> align=left&amp;rdquo;&lt;/p>
&lt;p>反汇编（disassembler）：确定可执行文件所表示的指令序列。&lt;/p>
&lt;p>typedef 给数据命名，主要是改善代码可读性。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">typedef&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#f92672">*&lt;/span>int_pointer;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>int_pointer ip;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>不过这个*为什么是和 int_pointer 挨在一起的？&lt;/p>
&lt;h3 id="217-位运算">2.1.7 位运算&lt;/h3>
&lt;p>这个自己编程时用的比较少&lt;/p>
&lt;p>与运算：$&amp;amp;$&lt;/p>
&lt;p>或运算：$|$&lt;/p>
&lt;p>NOT，取反：$~$ &lt;code>~0x41&lt;/code>&lt;/p>
&lt;h2 id="22">2.2&lt;/h2>
&lt;p>介绍一些补码，无符号数，有符号数&lt;/p>
&lt;p>2.3 阐述整数运算&lt;/p>
&lt;h2 id="24-浮点数">2.4 浮点数&lt;/h2>
&lt;h3 id="242-ieee浮点表示">2.4.2 IEEE浮点表示&lt;/h3>
&lt;p>尾数（significand）：一个二进制小数&lt;/p>
&lt;p>阶码（exponent）：浮点数加权。&lt;/p>
&lt;h1 id="3-程序的机器级表示">3. 程序的机器级表示&lt;/h1>
&lt;h3 id="321-机器级代码">3.2.1 机器级代码&lt;/h3>
&lt;p>大概讲一讲汇编代码，指令的编码格式，执行流程，关于控制的条件码&lt;/p>
&lt;h2 id="37-过程">3.7 过程&lt;/h2>
&lt;p>运行时栈&lt;/p>
&lt;h3 id="374-栈上的局部存储">3.7.4 栈上的局部存储&lt;/h3>
&lt;p>有时，寄存器不足够存放所有的本地数据，对于GPU会把这部分数据放入local memory&lt;/p>
&lt;h3 id="3101-理解指针">3.10.1 理解指针&lt;/h3>
&lt;h1 id="4-处理器体系结构">4. 处理器体系结构&lt;/h1>
&lt;h2 id="41-y86-65-指令集体系结构">4.1 Y86-65 指令集体系结构&lt;/h2>
&lt;p>介绍指令集架构，指令，RISC，CISC&lt;/p>
&lt;h2 id="42-逻辑设计和硬件控制语言-hcl">4.2 逻辑设计和硬件控制语言 HCL&lt;/h2>
&lt;p>最常用的是 Verilog，描述电路的语言。&lt;/p>
&lt;p>设计逻辑门，组合逻辑电路，布尔表达式，存储器，时钟（CLK）&lt;/p>
&lt;p>很多的逻辑门组合成一个网，就能构建计算块(computational block)，称为组合电路(combinational circuits)。构建这些网有两条限制：&lt;/p>
&lt;ul>
&lt;li>两个或多个逻辑门的输出不能连接在一起。否则它们可能会使线上的信号矛盾，可能会导致一个不合法的电压或电路故障&lt;/li>
&lt;li>这个网必须是无环的。也就是在网中不能有路径经过一系列的门而形成一个回路，这样的回路会导致该网络计算的函数有歧义。&lt;/li>
&lt;/ul>
&lt;h2 id="43-y86-65-的顺序实现">4.3 Y86-65 的顺序实现&lt;/h2>
&lt;p>介绍了几个 pipeline stage, fetch, decode, excute, memory, write back, PC update&lt;/p>
&lt;h2 id="44-流水线的通用原理">4.4 流水线的通用原理&lt;/h2>
&lt;p>介绍流水线如何缩短执行所需的 cycle&lt;/p>
&lt;p>流水线会有一些&lt;strong>局限性&lt;/strong>：不同逻辑单元的时钟周期并非完全一致的。流水线过深会造成收益下降。&lt;/p>
&lt;h2 id="45-y86-64-的流水线实现">4.5 Y86-64 的流水线实现&lt;/h2>
&lt;p>设计分支预测，data hazard, control hazard, structure hazard。主要是关于几个 hazard，以及处理方式，比如 stall, forward（转发）。&lt;/p>
&lt;p>什么是结构危害？&lt;/p>
&lt;p>当硬件不能在重叠执行中同时支持所有可能的指令组合时，资源冲突就会引起&lt;strong>结构性危险&lt;/strong>。在现代处理器中，结构性危害主要发生在不常用的特殊功能单元中（例如浮点除法或其他复杂的长期运行指令）。这个没有 data hazard and control hazard 常见。&lt;/p>
&lt;p>还涉及异常处理&lt;/p>
&lt;h3 id="458-流水线控制逻辑">4.5.8 流水线控制逻辑&lt;/h3>
&lt;p>发现特殊控制条件，流水线控制机制，控制逻辑实现&lt;/p>
&lt;h3 id="459-性能分析">4.5.9 性能分析&lt;/h3>
&lt;p>其实就是 IPC 和 CPI&lt;/p>
&lt;h2 id="5-优化程序性能">5. 优化程序性能&lt;/h2>
&lt;p>编写高效的程序：适当的算法和数据结构；编写编译器能够有效优化以转换成高效可执行代码的源码，这一点就要求比较了解和编译器相关的知识；利用并行性。&lt;/p>
&lt;h2 id="51-优化编译器的能力和局限性">5.1 优化编译器的能力和局限性&lt;/h2>
&lt;p>&lt;code>-O2 -O3&lt;/code> 级别的优化可以进一步提高程序的性能，但是也可能增加程序的规模。&lt;/p>
&lt;p>阻碍优化的因素：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>阐述了两个指针可能指向同一个内存位置的情况，这个被称为&lt;strong>内存别名使用&lt;/strong>(memory aliasing)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>函数调用，函数可能会有副作用（改变全局程序状态的一部分）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>大部分编译器不会判断函数是否偶副作用。可以用 inline function 替换来优化函数调用&lt;/p>
&lt;p>gcc不是最好的编译器，但对于大部分人已经足够用。&lt;/p>
&lt;h2 id="52-表示程序性能">5.2 表示程序性能&lt;/h2>
&lt;blockquote>
&lt;p>我们引入度量标准每元素的周期数(Cycles Per Element, CPE)作为一种表示性能并指导我们改进代码的方法。处理器活动的顺序是由时钟控制的，时钟提供了某个频率的规律信号，通常用千兆赫兹(GHz)，即十亿周期每秒来表示。CPE 越小越好。&lt;/p>
&lt;/blockquote>
&lt;h2 id="54-消除循环的低效率">5.4 消除循环的低效率&lt;/h2>
&lt;p>最常见的就是把函数调用放到 loop 之外，比如我之前编程时很喜欢写&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c++" data-lang="c++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span>(&lt;span style="color:#66d9ef">int&lt;/span> i&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>; i &lt;span style="color:#f92672">&amp;lt;&lt;/span> vec.size(); i&lt;span style="color:#f92672">++&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这样会大量调用 size() function，可以做如下优化&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c++" data-lang="c++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">int&lt;/span> len &lt;span style="color:#f92672">=&lt;/span> vec.size();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span>(&lt;span style="color:#66d9ef">int&lt;/span> i&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>; i &lt;span style="color:#f92672">&amp;lt;&lt;/span> len; i&lt;span style="color:#f92672">++&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>以上的代码移动(code motion)是一种优化。这类优化包括识别要执行多次(例如在循环里)但是计算结果不会改变的计算。因而可以将计算移动到代码前面不会被多次求值的部分。&lt;/p>
&lt;h2 id="55-减少过程调用">5.5 减少过程调用&lt;/h2>
&lt;blockquote>
&lt;p>过程调用会代码相当大的开销，而且妨碍大多数形式的程序优化。我们可以直接访问数组，而不是利用函数调用并加上边界检查。&lt;/p>
&lt;/blockquote>
&lt;p>也就是说去掉不必要的一些边界检查。&lt;/p>
&lt;h2 id="56-消除不必要的内存引用">5.6 消除不必要的内存引用&lt;/h2>
&lt;p>比如无需每次都把结果写入 dest，可以使用一个临时变量，消除不必要的存储器引用&lt;/p>
&lt;h2 id="57-理解现代处理器">5.7 理解现代处理器&lt;/h2>
&lt;p>提到了 superscaler，OoO，分支预测。大概是介绍这类技术&lt;/p>
&lt;h2 id="58-循环展开">5.8 循环展开&lt;/h2>
&lt;blockquote>
&lt;p>循环展开是一种程序变换，通过增加每次迭代计算的元素的数量，减少循环的迭代次数。其思想是在一次迭代中访问数组并做乘法，这样得到的程序需要更少的迭代，从而降低循环的开销。&lt;/p>
&lt;/blockquote>
&lt;p>PS：这个应该是和最基础的并行思想想结合。&lt;/p>
&lt;h2 id="59-提高并行性">5.9 提高并行性&lt;/h2>
&lt;p>多个积累变量；重新结合变换；这就涉及到 SIMD&lt;/p>
&lt;h2 id="511-一些限制因素">5.11 一些限制因素&lt;/h2>
&lt;p>寄存器溢出；分支预测和预测错误处罚；Amdahl定律&lt;/p>
&lt;h2 id="512-理解内存性能">5.12 理解内存性能&lt;/h2>
&lt;p>这个就是研究 load/store 的影响，也就是 cache&lt;/p>
&lt;h2 id="513-应用性能提高技术">5.13 应用：性能提高技术&lt;/h2>
&lt;p>高级设计：为遇到的问题选择适当的算法和数据结构。&lt;/p>
&lt;p>基本编码原则：消除连续的函数调用；消除不必要的内存引用；&lt;/p>
&lt;p>低级优化：展开循环；SIMD；&lt;/p>
&lt;h2 id="514-确认和消除性能瓶颈">5.14 确认和消除性能瓶颈&lt;/h2>
&lt;p>也就是常说的 profiling 技术，code profiler。&lt;/p>
&lt;p>最基础的就是插入工具代码，确定程序的各个部分需要多少时间。&lt;/p>
&lt;h1 id="6-存储器层次结构memory-system">6. 存储器层次结构（memory system）&lt;/h1>
&lt;h2 id="61-存储技术">6.1 存储技术&lt;/h2>
&lt;p>RAM，Random-Access-Memory，分为 SRAM，DRAM。&lt;/p>
&lt;p>SRAM：每个 bit 存在一个双稳态（bitstable）存储器单元。只有两个稳定的配置（configuration）或者状态（state）&lt;/p>
&lt;p>DRAM：DRAM相比SRAM，对干扰非常敏感。内存系统必须周期性地读出，然后重写来刷新每一位。&lt;/p>
&lt;p>NVM（Non-Volatile Memory）：闪存（flash memory），新型的基于闪存的磁盘驱动器，SSD（Solid State Disk）&lt;/p>
&lt;p>Cache 的一些基本信息。&lt;/p>
&lt;h1 id="7-链接">7. 链接&lt;/h1>
&lt;p>链接：将各种代码和数据和数据片段收集并组合成为一个单一文件的过程，这个文件可以被&lt;strong>加载&lt;/strong>（复制）到内存并执行。链接可以执行与 compile time, load time, run time.&lt;/p>
&lt;p>PS：链接在大型工程项目编译时应该很重要，可以避免对一些源代码文件的重复编译。&lt;/p>
&lt;p>大多数编译系统提供编译驱动程序（compiler driver），为用户根据需求调用语言预处理器、编译器、汇编器和链接器。&lt;/p>
&lt;h2 id="72-静态链接">7.2 静态链接&lt;/h2>
&lt;p>为了创建可执行文件，链接器必须完成两个主要任务：&lt;/p>
&lt;ul>
&lt;li>符号解析（symbol resolution），将一个符号引用和一个符号定义结合起来&lt;/li>
&lt;li>重定位（relocation），编译器和汇编器生成从地址 0 开始的代码和数据节。链接器通过把每个符号定义域一个存储器位置联系起来，然后修改所有对这些符号的引用，使得它们指向这个存储器位置，从而重定位这些节。&lt;/li>
&lt;/ul>
&lt;p>目标文件纯粹是字节块的集合。这些块中，有些包含程序代码，有些则包含程序数据，而其他的则包含指导链接器和加载器的数据结构。链接器将这些块连接起来，确定被连接块的运行时位置，并且修改代码和数据块中的各种位置。链接器对目标机器了解甚少。产生目标文件的编译器和汇编器已经完成了大部分工作。&lt;/p>
&lt;h2 id="73-目标文件">7.3 目标文件&lt;/h2>
&lt;p>目标文件有三种形式：&lt;/p>
&lt;ul>
&lt;li>可重定位目标文件，包含二进制文件和代码，其形式在编译时和其他可重定位目标文件合并起来，创建一个可执行目标文件&lt;/li>
&lt;li>可执行目标文件，其形式可被拷贝到存储器并执行&lt;/li>
&lt;li>共享目标文件，一种特殊类型的可重定位目标文件，可以在加载或者运行时被动态地加载到存储器并链接。&lt;/li>
&lt;/ul>
&lt;p>编译器和汇编器生成可重定位目标文件（包括共享目标文件），链接器生成可执行目标文件&lt;/p>
&lt;h2 id="710-动态共享链接库">7.10 动态共享链接库&lt;/h2>
&lt;p>共享库(shared library)是致力于解决静态库缺陷的一个现代创新产物。共享库是一个目标模块，在运行时，可以加载到任意的存储器地址，并和一个在存储器中的程序链接起来。这个过程称为动态链接(dynamic linking)，是由一个叫做动态链接器的程序来执行的。&lt;/p>
&lt;p>共享库也称为共享目标(shared object)，在 Unix 系统中通常用 .so 后缀来表示。微软的操作系统大量地利用了共享库，它们称为 DLL。&lt;/p>
&lt;p>动态链接库和静态链接库的区别：&lt;/p>
&lt;p>关于静态库&lt;/p>
&lt;ul>
&lt;li>
&lt;p>静态库对函数库的链接是放在编译时期完成的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>程序在运行时与函数库再无瓜葛，移植方便。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>浪费空间和资源，因为所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>为什么需要动态库？&lt;/strong>&lt;/p>
&lt;p>空间浪费是静态库的一个问题。&lt;/p>
&lt;p>另一个问题是静态库对程序的更新、部署和发布页会带来麻烦。如果静态库liba.lib更新了，所以使用它的应用程序都需要重新编译、发布给用户（对于玩家来说，可能是一个很小的改动，却导致整个程序重新下载，全量更新）。&lt;/p>
&lt;p>&lt;strong>动态库特点&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不同的应用程序如果调用相同的库，那么在内存里只需要有一份该共享库的实例，规避了空间浪费问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>动态库在程序运行是才被载入，也解决了静态库对程序的更新、部署和发布页会带来麻烦。用户只需要更新动态库即可，增量更新。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="8-异常控制流">8. 异常控制流&lt;/h1>
&lt;p>这部分后面有需要再看&lt;/p>
&lt;blockquote>
&lt;p>处理器按照一定的序列的地址执行对应的指令，从这一个地址过渡到下一个地址成为控制转移，这样的控制转移序序列称为处理器的控制流（flow control）&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>系统必须能够对系统状态的变化做出反应，这些系统状态不是被内部程序变量捕获的，而且也不一定要和程序的执行相关。比如，一个硬件定时器定期产生信号，这个事件必须得到处理。当子进程终止时，创造这些子进程的父进程必须得到通知。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>线代系统通过使控制流发生突变来对这些情况做出反应。一般而言，我们把这些突变称为异常控制流(Exceptional Control Flow, ECF)。异常控制流发生在计算机系统的各个层次。比如，在硬件层，硬件检测到的事件会触发控制突然转移到异常处理程序。在操作系统层，内核通过上下文转换将控制从一个用户进程转移到另一个用户进程。在应用层，一个进程可以发送信号到另一个进程，而接受者会将控制突然转移到它的一个信号处理程序。一个程序可以通过回避通常的栈规则，并执行到其他函数中任意位置的非本地跳转来对错误做出反应。&lt;/p>
&lt;/blockquote>
&lt;h1 id="9-虚拟内存">9. 虚拟内存&lt;/h1>
&lt;p>虚拟内存这一块工作，以及请求到下级缓存之后的知识完全不solid，这也间接导致了商汤，平头哥的面试出现重大问题。在和 Xia 的交流中，这是一个潜在的研究方向。既是补短板，也是打基础。&lt;/p>
&lt;h1 id="91-物理和虚拟寻址">9.1 物理和虚拟寻址&lt;/h1>
&lt;p>物理寻址：理解为类似直接映射的方式。&lt;/p>
&lt;p>虚拟寻址：请求中包含的是一个 VA（virtual memory），经过 MMU 进行 address translation 后得到 PA（physical address），此时去 Main memory 寻址。&lt;/p>
&lt;h1 id="93-虚拟内存作为缓存的工具">9.3 虚拟内存作为缓存的工具&lt;/h1>
&lt;p>VM 系统将虚拟内存分割为大小固定的块，称为虚页（virtual page, VP）。类似地，物理内存被分割为物理页（physical page, PP）。&lt;/p>
&lt;p>在任意时刻，虚拟页面的集合部分都分为三个不相交的子集：&lt;/p>
&lt;ul>
&lt;li>未分配的：VM 系统还未分配(或者创建)的页。未分配的块没有任何数据和它们相关联，因此也就不占用任何磁盘空间。&lt;/li>
&lt;li>缓存的：当前缓存在物理存储器中的已分配页。&lt;/li>
&lt;li>未缓存的：没有缓存在物理存储器中的已分配页。&lt;/li>
&lt;/ul>
&lt;h3 id="931-dram高速缓存的组织结构">9.3.1 DRAM高速缓存的组织结构&lt;/h3>
&lt;p>在存储层次结构中，DRAM缓存的位置对于他的组织结构有很大的影响。DRAM 缓存的组织结构完全是由巨大的不命中开销（large overhead）驱动的。&lt;/p>
&lt;p>因为大的不命中处罚和访问第一字节的开销，虚拟页往往很大，典型地是4KB-2MB。&lt;/p>
&lt;p>由于大的不命中处罚，DRAM 缓存是全相连的，也就是说，任何虚拟页都可以放置在任何的物理页中。不命中时的替换策略也很重要，因为替换错了虚拟页的出发也非常高。因此，与硬件对 SRAM 缓存相比，操作系统对 DRAM 缓存使用了更复杂精密的替换算法。最后，因为对磁盘的访问时间很长，DRAM 缓存总是使用写回(write back)，而不是直写。&lt;/p>
&lt;p>PS：也就是说虚拟页采用的是写回的替换策略。&lt;/p>
&lt;h3 id="932-页表">9.3.2 页表&lt;/h3>
&lt;p>同任何缓存一样，虚拟存储器系统必须有某种方法来判定一个虚拟页是否存放在 DRAM 中的某个地方。如果是，系统还必须确定这个虚拟页存放在哪个物理页中。如果不命中，系统必须判断这个虚拟页存放在磁盘的哪个位置，在物理存储器中选择一个牺牲页，并将虚拟页从磁盘拷贝到 DRAM 中，替换这个牺牲页。&lt;/p>
&lt;p>PS：这个和 cache 的逻辑其实是类似的。页是访问的最小单位&lt;/p>
&lt;p>这些功能是由许多软硬件联合提供的，包括操作系统软、MMU(存储器管理单元)中的地址翻译硬件和一个存放在物理存储器中叫做页表(page table)的数据结构，页表将虚拟页映射到物理页。每次地址翻译硬件将一个虚拟地址转换为物理地址时都会读取页表。操作系统负责维护页表的内容，以及在磁盘与 DRAM 之间来回传送页。&lt;/p>
&lt;p>下图展示了一个页表的基本组织结构。&lt;strong>页表&lt;/strong>就是一个页表条目(Page Table Entry, PTE)的数组。虚拟地址空间中的每个页在页表中一个固定偏移量处都有一个 PTE。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="933-页命中">9.3.3 页命中&lt;/h3>
&lt;h3 id="934-缺页">9.3.4 缺页&lt;/h3>
&lt;p>在虚拟存储器的习惯说法中，DRAM不命中称为缺页（page fault）。缺页异常调用内核中缺页异常处理程序，该程序会选择一个牺牲页。在磁盘和存储器之间传送页的活动叫做交换(swapping)或者页面调度(paging)。&lt;/p>
&lt;p>PS：那就和 cache miss, cache replacement 类似。DRAM 提供不了，需要找 disk 去要数据&lt;/p>
&lt;h3 id="936-局部性再次搭救">9.3.6 局部性再次搭救&lt;/h3>
&lt;p>尽管在整个运行过程中程序引用的不同页面的总数可能超出物理存储器总的大小，但是局部性原则保证了在任意时刻，程序往往在一个较小的活动页面(active page)集合上工作，这个集合叫做工作集(working set)或者常驻集(resident set)。&lt;/p>
&lt;p>如果工作集的大小超出了物理存储器的大小，那么程序将产生一种不幸的状态，叫做颠簸（thrashing），这时页面将不断的换进换出。&lt;/p>
&lt;p>PS：这些概念和 cache 都是类似的。&lt;/p>
&lt;h2 id="96-address-translation">9.6 Address Translation&lt;/h2>
&lt;p>虚拟地址有两段信息，&lt;strong>虚拟页号&lt;/strong> 以及 &lt;strong>虚拟页偏移量&lt;/strong>，通过虚拟页号查询&lt;strong>页表&lt;/strong>得到物理页号，物理页偏移和虚拟页偏移对应。&lt;/p>
&lt;h3 id="962-利用tlb加速地址翻译">9.6.2 利用TLB加速地址翻译&lt;/h3>
&lt;p>TLB 是什么？TLB是一个小的、虚拟地址的缓存，其中每一行都保存着一个由单个PTE组成的块。TLB通常有高度的相连性。&lt;/p>
&lt;p>PS：也就是快速地查找 虚拟页表 到 物理页表 的映射。&lt;/p>
&lt;p>下图的描述很清晰，仔细去看 1,2,3 过程。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="963-多级页表">9.6.3 多级页表&lt;/h3>
&lt;p>PS：这个是平头哥面试过程中没有回答出来的部分。对于页表概念的空白，一部分原因应该是对OS的掌握不够。&lt;/p>
&lt;p>为什么需要用到多级页表？得先了解一级页表的缺陷，一级页表的地址必须指向确定的物理页，否则就会出现错误，所以如果用一级页表的话，就必须把全部的页表都加载进去。&lt;/p>
&lt;p>假设虚拟地址空间为32位（即4GB），每个页面映射4KB以及每条页表项占4B：&lt;/p>
&lt;ul>
&lt;li>一级页表：进程需要1M个页表entry（4GB / 4KB = 1M, 2^20个页表项），即页表（每个进程都有一个页表）占用4MB（1M * 4B = 4MB）的内存空间。&lt;/li>
&lt;li>二级页表：一级页表映射4MB（2^22）、二级页表映射4KB，则需要1K个一级页表项（4GB / 4MB = 1K, 2^10个一级页表项）、每个一级页表项对应1K个二级页表项（4MB / 4KB = 1K），这样页表占用4.004MB（1K * 4B + 1K * 1K * 4B = 4.004MB）的内存空间。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>多级页表的内存空间占用反而变大了。但是二级页表可以不存在，二级页表可以存在但不存在主存。&lt;/p>
&lt;p>做个简单的计算，假设只有20%的一级页表项被用到了，那么页表占用的内存空间就只有0.804MB（1K * 4B + 0.2 * 1K * 1K * 4B = 0.804MB），对比单级页表的4M是不是一个巨大的节约？&lt;/p>
&lt;p>那么为什么不分级的页表就做不到这样节约内存呢？我们从页表的性质来看，保存在主存中的页表承担的职责是将虚拟地址翻译成物理地址；假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有1M个页表项来映射，而二级页表则最少只需要1K个页表项（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。&lt;/p>
&lt;/blockquote>
&lt;p>PS：讲一下个人理解。每个process有4GB虚拟地址空间，因为程序的虚拟地址可能是4GB中的任意一个字节。一级页表覆盖了整个4GB的虚拟地址空间，但是如果某个一级页表没有被用到，那么就无需创建这个页表项对应的二级页表，可以在需要时再创建二级页表。&lt;/p>
&lt;h2 id="99-动态内存分配">9.9 动态内存分配&lt;/h2>
&lt;p>这部分和OS以及编程中的内存管理相关。&lt;/p>
&lt;p>显式分配：比如malloc, free, new, delete&lt;/p>
&lt;p>隐式分配：分配器检测一个已分配的块何时不再被程序所使用，那么就释放这个块。&lt;/p>
&lt;p>垃圾回收（GC, Garbage Collection）：自动释放未使用的已分配的块。一些高级语言会依赖于GC来释放已分配的块，比如 Lisp, ML, JAVA&lt;/p>
&lt;h1 id="10-系统级io">10. 系统级IO&lt;/h1>
&lt;p>将一些输入输出，打开/读取文件，重定向&lt;/p>
&lt;h1 id="11-网络编程">11. 网络编程&lt;/h1>
&lt;p>IP, Socket, Web 服务器, HTTP。&lt;/p>
&lt;p>PS：这部分自己不是很熟&lt;/p>
&lt;h1 id="12-并发编程">12. 并发编程&lt;/h1>
&lt;p>竞争，死锁&lt;/p></description></item></channel></rss>